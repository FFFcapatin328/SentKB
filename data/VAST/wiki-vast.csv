topic,explanation
affirmative actions,"The term affirmative action refers to a policy aimed at increasing workplace or educational opportunities for underrepresented parts of society. These programs are commonly implemented by businesses and governments by taking individuals' race, sex, religion, or national origin into account.

Affirmative action focuses on demographics with historically low representation in leadership, professional, and academic roles and is often considered a means of countering discrimination against particular groups."
affirmative action,"The term affirmative action refers to a policy aimed at increasing workplace or educational opportunities for underrepresented parts of society. These programs are commonly implemented by businesses and governments by taking individuals' race, sex, religion, or national origin into account.

Affirmative action focuses on demographics with historically low representation in leadership, professional, and academic roles and is often considered a means of countering discrimination against particular groups."
tfa,"Teach For America (TFA) is a nonprofit organization whose stated mission is to ""enlist, develop, and mobilize as many as possible of our nation's most promising future leaders to grow and strengthen the movement for educational equity and excellence"".[2]

The organization aims to accomplish this by recruiting and selecting college graduates from top universities around the United States to serve as teachers. The selected members, known as ""corps members,"" commit to teaching for at least two years in a public or public charter K–12 school in one of the 52 low-income communities that the organization serves."
trained teachers,"Trained Teacher means a teacher who has satisfactorily completed a prescribed course of training at a higher education institution, or such other course or courses which the Director-General determines as satisfying requirements for classification as a teacher."
capitalism,"Capitalism is an economic system based on the private ownership of the means of production and their operation for profit.[1][2][3][4] Central characteristics of capitalism include capital accumulation, competitive markets, a price system, private property and the recognition of property rights, voluntary exchange and wage labor.[5][6] In a capitalist market economy, decision-making and investments are determined by owners of wealth, property, ability to manouver capital or production ability in capital and financial markets—whereas prices and the distribution of goods and services are mainly determined by competition in goods and services markets.[7]

Economists, historians, political economists and sociologists have adopted different perspectives in their analyses of capitalism and have recognized various forms of it in practice. These include laissez-faire or free-market capitalism, state capitalism and welfare capitalism. Different forms of capitalism feature varying degrees of free markets, public ownership,[8] obstacles to free competition and state-sanctioned social policies. The degree of competition in markets and the role of intervention and regulation as well as the scope of state ownership vary across different models of capitalism.[9][10] The extent to which different markets are free and the rules defining private property are matters of politics and policy. Most of the existing capitalist economies are mixed economies that combine elements of free markets with state intervention and in some cases economic planning.[11]

Market economies have existed under many forms of government and in many different times, places and cultures. Modern capitalist societies developed in Western Europe in a process that led to the Industrial Revolution. Capitalist systems with varying degrees of direct government intervention have since become dominant in the Western world and continue to spread. Economic growth is a characteristic tendency of capitalist economies.[12]"
opidates,"Opiates are the natural or synthetic drugs that have a morphine-like pharmacological action. Medically, opiates are used primarily for relief of pain. Opiates include morphine and drugs structurally similar to morphine (eg, codeine, hydrocodone, hydromorphone, oxycodone)."
hedonism,"Hedonism refers to a family of theories, all of which have in common that pleasure plays a central role in them. Psychological or motivational hedonism claims that human behavior is determined by desires to increase pleasure and to decrease pain.[1][2] Normative or ethical hedonism, on the other hand, is not about how we actually act but how we ought to act: we should pursue pleasure and avoid pain.[2] Axiological hedonism, which is sometimes treated as a part of ethical hedonism, is the thesis that only pleasure has intrinsic value.[1][3][4] Applied to well-being or what is good for someone, it is the thesis that pleasure and suffering are the only components of well-being.[5] These technical definitions of hedonism within philosophy, which are usually seen as respectable schools of thought, have to be distinguished from how the term is used in everyday language, sometimes referred to as ""folk hedonism"". In this sense, it has a negative connotation, linked to the egoistic pursuit of short-term gratification by indulging in sensory pleasures without regard for the consequences.[2][6]"
mother teresa,"Mother Mary Teresa Bojaxhiu[6] (born Anjezë Gonxhe Bojaxhiu, Albanian: [aˈɲɛzə ˈɡɔndʒɛ bɔjaˈdʒiu]; 26 August 1910 – 5 September 1997), honoured in the Catholic Church as Saint Teresa of Calcutta,[7] was an Albanian-Indian[4] Roman Catholic nun and missionary.[8] She was born in Skopje (now the capital of North Macedonia), then part of the Kosovo Vilayet of the Ottoman Empire. After living in Skopje for eighteen years, she moved to Ireland and then to India, where she lived for most of her life.

In 1950, Teresa founded the Missionaries of Charity, a Roman Catholic religious congregation that had over 4,500 nuns and was active in 133 countries in 2012. The congregation manages homes for people who are dying of HIV/AIDS, leprosy and tuberculosis. It also runs soup kitchens, dispensaries, mobile clinics, children's and family counselling programmes, as well as orphanages and schools. Members take vows of chastity, poverty, and obedience, and also profess a fourth vow – to give ""wholehearted free service to the poorest of the poor.""[9]

Teresa received a number of honours, including the 1962 Ramon Magsaysay Peace Prize and the 1979 Nobel Peace Prize. She was canonised on 4 September 2016, and the anniversary of her death (5 September) is her feast day. A controversial figure during her life and after her death, Teresa was admired by many for her charitable work. She was praised and criticized on various counts, such as for her views on abortion and contraception, and was criticized for poor conditions in her houses for the dying. Her authorized biography was written by Navin Chawla and published in 1992, and she has been the subject of films and other books. On 6 September 2017, Teresa and St. Francis Xavier were named co-patrons of the Roman Catholic Archdiocese of Calcutta."
regligion and prosecution,"Religious persecution is the systematic mistreatment of an individual or a group of individuals as a response to their religious beliefs or affiliations or their lack thereof. The tendency of societies or groups within societies to alienate or repress different subcultures is a recurrent theme in human history. Moreover, because a person's religion often determines his or her morality, world view, self-image, attitudes towards others, and overall personal identity to a significant extent, religious differences can be significant cultural, personal, and social factors.

Religious persecution may be triggered by religious bigotry (i.e. when members of a dominant group denigrate religions other than their own) or it may be triggered by the state when it views a particular religious group as a threat to its interests or security. At a societal level, the dehumanisation of a particular religious group may readily lead to violence or other forms of persecution. Indeed, in many countries, religious persecution has resulted in so much violence that it is considered a human rights problem."
nations,"A nation is a community of people formed on the basis of a combination of shared features such as language, history, ethnicity, culture and/or territory. A nation is thus the collective identity of a group of people understood as defined by those features. A nation is generally more overtly political than an ethnic group;[1][2] it has been described as ""a fully mobilized or institutionalized ethnic group"".[3] Some nations are equated with ethnic groups (see ethnic nationalism and nation state) and some are equated with an affiliation with a social and political constitution (see civic nationalism and multiculturalism).[3] A nation has also been defined as a cultural-political community that has become conscious of its autonomy, unity and particular interests.[4]

Benedict Anderson characterised a nation as an ""imagined community"",[5] and Paul James sees it as an ""abstract community"".[6] A nation is an imagined community in the sense that the material conditions exist for imagining extended and shared connections and that it is objectively impersonal, even if each individual in the nation experiences themselves as subjectively part of an embodied unity with others. For the most part, members of a nation remain strangers to each other and will likely never meet.[7] Hence the phrase, ""a nation of strangers"" used by such writers as American journalist Vance Packard.[8]"
salaries should be raised,salary
farms,"A farm (also called an agricultural holding) is an area of land that is devoted primarily to agricultural processes with the primary objective of producing food and other crops; it is the basic facility in food production.[1] The name is used for specialized units such as arable farms, vegetable farms, fruit farms, dairy, pig and poultry farms, and land used for the production of natural fiber, biofuel and other commodities. It includes ranches, feedlots, orchards, plantations and estates, smallholdings and hobby farms, and includes the farmhouse and agricultural buildings as well as the land. In modern times the term has been extended so as to include such industrial operations as wind farms and fish farms, both of which can operate on land or sea.

There are about 570 million farms in the world, with most of which are small and family-operated. Small farms with a land area of less than 2 hectares operate about 12% of the world's agricultural land, and family farms comprise about 75% of the world's agricultural land.[2]

Modern farms in developed countries are highly mechanized. In the United States, livestock may be raised on rangeland and finished in feedlots and the mechanization of crop production has brought about a great decrease in the number of agricultural workers needed. In Europe, traditional family farms are giving way to larger production units. In Australia, some farms are very large because the land is unable to support a high stocking density of livestock because of climatic conditions. In less developed countries, small farms are the norm, and the majority of rural residents are subsistence farmers, feeding their families and selling any surplus products in the local market."
farm,"A farm (also called an agricultural holding) is an area of land that is devoted primarily to agricultural processes with the primary objective of producing food and other crops; it is the basic facility in food production.[1] The name is used for specialized units such as arable farms, vegetable farms, fruit farms, dairy, pig and poultry farms, and land used for the production of natural fiber, biofuel and other commodities. It includes ranches, feedlots, orchards, plantations and estates, smallholdings and hobby farms, and includes the farmhouse and agricultural buildings as well as the land. In modern times the term has been extended so as to include such industrial operations as wind farms and fish farms, both of which can operate on land or sea.

There are about 570 million farms in the world, with most of which are small and family-operated. Small farms with a land area of less than 2 hectares operate about 12% of the world's agricultural land, and family farms comprise about 75% of the world's agricultural land.[2]

Modern farms in developed countries are highly mechanized. In the United States, livestock may be raised on rangeland and finished in feedlots and the mechanization of crop production has brought about a great decrease in the number of agricultural workers needed. In Europe, traditional family farms are giving way to larger production units. In Australia, some farms are very large because the land is unable to support a high stocking density of livestock because of climatic conditions. In less developed countries, small farms are the norm, and the majority of rural residents are subsistence farmers, feeding their families and selling any surplus products in the local market."
ignorance,"Ignorance is a lack of knowledge and information. The word ""ignorant"" is an adjective that describes a person in the state of being unaware, or even cognitive dissonance and other cognitive relation, and can describe individuals who are unaware of important information or facts. Ignorance can appear in three different types: factual ignorance (absence of knowledge of some fact), object ignorance (unacquaintance with some object), and technical ignorance (absence of knowledge of how to do something).[1]"
vacation,"A vacation (American English), or holiday (British English), is a leave of absence from a regular job. People often take a vacation during specific holiday observances, or for specific festivals or celebrations. Vacations are often spent with friends or family.[1] Vacations may include a specific trip or journey, usually for the purpose of recreation or tourism.

A person may take a longer break from work, such as a sabbatical, gap year, or career break.

The concept of taking a vacation is a recent invention, and has developed through the last two centuries. Historically, the idea of travel for recreation was a luxury that only wealthy people could afford (see Grand Tour). In the Puritan culture of early America, taking a break from work for reasons other than weekly observance of the Sabbath was frowned upon. However, the modern concept of vacation was led by a later religious movement encouraging spiritual retreat and recreation. The notion of breaking from work periodically took root among the middle and working class.[2]"
midwife,"A midwife is a health professional who cares for mothers and newborns around childbirth, a specialization known as midwifery.

The education and training for a midwife concentrates extensively on the care of women throughout their lifespan; concentrating on being experts in what is normal and identifying conditions that need further evaluation. In most countries, midwives are recognized as skilled healthcare providers. Midwives are trained to recognize variations from the normal progress of labor and understand how to deal with deviations from normal. They may intervene in high risk situations such as breech births, twin births, and births where the baby is in a posterior position, using non-invasive techniques. For complications related to pregnancy and birth that are beyond the midwife's scope of practice, including surgical and instrumental deliveries, they refer their patients to physicians or surgeons.[3][4] In many parts of the world, these professions work in tandem to provide care to childbearing women. In others, only the midwife is available to provide care, and in yet other countries, many women elect to utilize obstetricians primarily over midwives.

Many developing countries are investing money and training for midwives, sometimes by upskilling those people already practicing as traditional birth attendants. Some primary care services are currently lacking, due to a shortage of funding for these resources."
wal-mart,"Walmart Inc.is an American multinational retail corporation that operates a chain of hypermarkets (also called supercenters), discount department stores, and grocery stores from the United States, headquartered in Bentonville, Arkansas.[9] The company was founded by Sam Walton in nearby Rogers, Arkansas in 1962 and incorporated under Delaware General Corporation Law on October 31, 1969. It also owns and operates Sam's Club retail warehouses.[10][11] As of July 31, 2021, Walmart has 10,524 stores and clubs in 24 countries, operating under 48 different names.[2][3][12] The company operates under the name Walmart in the United States and Canada, as Walmart de México y Centroamérica in Mexico and Central America, and as Flipkart Wholesale in India. It has wholly owned operations in Chile, Canada, and South Africa. Since August 2018, Walmart holds only a minority stake in Walmart Brasil, which was renamed Grupo Big in August 2019, with 20 percent of the company's shares, and private equity firm Advent International holding 80 percent ownership of the company.

Walmart is the world's largest company by revenue, with US$548.743 billion, according to the Fortune Global 500 list in 2020. It is also the largest private employer in the world with 2.2 million employees. It is a publicly traded family-owned business, as the company is controlled by the Walton family. Sam Walton's heirs own over 50 percent of Walmart through both their holding company Walton Enterprises and their individual holdings.[13] Walmart was the largest United States grocery retailer in 2019, and 65 percent of Walmart's US$510.329 billion sales came from U.S. operations.[14][15]

Walmart was listed on the New York Stock Exchange in 1972. By 1988, it was the most profitable retailer in the U.S.,[16] and it had become the largest in terms of revenue by October 1989.[17] The company was originally geographically limited to the South and lower Midwest, but it had stores from coast to coast by the early 1990s. Sam's Club opened in New Jersey in November 1989, and the first California outlet opened in Lancaster, in July 1990. A Walmart in York, Pennsylvania, opened in October 1990, the first main store in the Northeast.[18]

Walmart's investments outside the U.S. have seen mixed results. Its operations and subsidiaries in Canada,[19] the United Kingdom,[20] Central America, South America, and China are successful, but its ventures failed in Germany, Japan, and South Korea.[21][22][23]"
walmart,"Walmart Inc. is an American multinational retail corporation that operates a chain of hypermarkets (also called supercenters), discount department stores, and grocery stores from the United States, headquartered in Bentonville, Arkansas.[9] The company was founded by Sam Walton in nearby Rogers, Arkansas in 1962 and incorporated under Delaware General Corporation Law on October 31, 1969. It also owns and operates Sam's Club retail warehouses.[10][11] As of July 31, 2021, Walmart has 10,524 stores and clubs in 24 countries, operating under 48 different names.[2][3][12] The company operates under the name Walmart in the United States and Canada, as Walmart de México y Centroamérica in Mexico and Central America, and as Flipkart Wholesale in India. It has wholly owned operations in Chile, Canada, and South Africa. Since August 2018, Walmart holds only a minority stake in Walmart Brasil, which was renamed Grupo Big in August 2019, with 20 percent of the company's shares, and private equity firm Advent International holding 80 percent ownership of the company.

Walmart is the world's largest company by revenue, with US$548.743 billion, according to the Fortune Global 500 list in 2020. It is also the largest private employer in the world with 2.2 million employees. It is a publicly traded family-owned business, as the company is controlled by the Walton family. Sam Walton's heirs own over 50 percent of Walmart through both their holding company Walton Enterprises and their individual holdings.[13] Walmart was the largest United States grocery retailer in 2019, and 65 percent of Walmart's US$510.329 billion sales came from U.S. operations.[14][15]

Walmart was listed on the New York Stock Exchange in 1972. By 1988, it was the most profitable retailer in the U.S.,[16] and it had become the largest in terms of revenue by October 1989.[17] The company was originally geographically limited to the South and lower Midwest, but it had stores from coast to coast by the early 1990s. Sam's Club opened in New Jersey in November 1989, and the first California outlet opened in Lancaster, in July 1990. A Walmart in York, Pennsylvania, opened in October 1990, the first main store in the Northeast.[18]

Walmart's investments outside the U.S. have seen mixed results. Its operations and subsidiaries in Canada,[19] the United Kingdom,[20] Central America, South America, and China are successful, but its ventures failed in Germany, Japan, and South Korea.[21][22][23]"
faux lotteries,faux lotteries
pot,pot
matter in an election,"An election is a formal group decision-making process by which a population chooses an individual or multiple individuals to hold public office.

Elections have been the usual mechanism by which modern representative democracy has operated since the 17th century. Elections may fill offices in the legislature, sometimes in the executive and judiciary, and for regional and local government. This process is also used in many other private and business organisations, from clubs to voluntary associations and corporations.

The universal use of elections as a tool for selecting representatives in modern representative democracies is in contrast with the practice in the democratic archetype, ancient Athens, where the Elections were not used were considered an oligarchic institution and most political offices were filled using sortition, also known as allotment, by which officeholders were chosen by lot.

Electoral reform describes the process of introducing fair electoral systems where they are not in place, or improving the fairness or effectiveness of existing systems. Psephology is the study of results and other statistics relating to elections (especially with a view to predicting future results). Election is the fact of electing, or being elected.

To elect means ""to select or make a decision"", and so sometimes other forms of ballot such as referendums are referred to as elections, especially in the United States."
science,"Science (from Latin scientia 'knowledge')[1] is a systematic enterprise that builds and organizes knowledge in the form of testable explanations and predictions about the world.[2][3][4]

The earliest roots of science can be traced to Ancient Egypt and Mesopotamia in around 3000 to 1200 BCE.[5][6] Their contributions to mathematics, astronomy, and medicine entered and shaped Greek natural philosophy of classical antiquity, whereby formal attempts were made to provide explanations of events in the physical world based on natural causes.[5][6] After the fall of the Western Roman Empire, knowledge of Greek conceptions of the world deteriorated in Western Europe during the early centuries (400 to 1000 CE) of the Middle Ages,[7] but was preserved in the Muslim world during the Islamic Golden Age.[8] The recovery and assimilation of Greek works and Islamic inquiries into Western Europe from the 10th to 13th century revived ""natural philosophy"",[7][9] which was later transformed by the Scientific Revolution that began in the 16th century[10] as new ideas and discoveries departed from previous Greek conceptions and traditions.[11][12][13][14] The scientific method soon played a greater role in knowledge creation and it was not until the 19th century that many of the institutional and professional features of science began to take shape;[15][16][17] along with the changing of ""natural philosophy"" to ""natural science.""[18]

Modern science is typically divided into three major branches[19] that consist of the natural sciences (e.g., biology, chemistry, and physics), which study nature in the broadest sense; the social sciences (e.g., economics, psychology, and sociology), which study individuals and societies;[20][21] and the formal sciences (e.g., logic, mathematics, and theoretical computer science), which deal with symbols governed by rules.[22][23] There is disagreement,[24][25][26] however, on whether the formal sciences actually constitute a science as they do not rely on empirical evidence.[27][25] Disciplines that use existing scientific knowledge for practical purposes, such as engineering and medicine, are described as applied sciences.[28][29][30][31][32]

New knowledge in science is advanced by research from scientists who are motivated by curiosity about the world and a desire to solve problems.[33][34] Contemporary scientific research is highly collaborative and is usually done by teams in academic and research institutions,[35] government agencies, and companies.[36][37] The practical impact of their work has led to the emergence of science policies that seek to influence the scientific enterprise by prioritizing the development of commercial products, armaments, health care, public infrastructure, and environmental protection."
environmental implications,"An environmental impact is defined as any change to the environment, whether adverse or beneficial, resulting from a facility’s activities, products, or services.[2] In other words it is the effect that people's actions have on the environment. For example, when volatile organic compounds are released into the environment, the effect or impact is pollution in the form of smog, in this case being negative. It can go the other way, as a person picking up litter can have a beneficial impact on the local environment."
arctic,"The Arctic is a polar region located at the northernmost part of Earth. The Arctic consists of the Arctic Ocean, adjacent seas, and parts of Alaska (United States), Canada, Finland, Greenland (Denmark), Iceland, Norway, Russia, and Sweden. Land within the Arctic region has seasonally varying snow and ice cover, with predominantly treeless permafrost (permanently frozen underground ice) containing tundra. Arctic seas contain seasonal sea ice in many places.

The Arctic region is a unique area among Earth's ecosystems. The cultures in the region and the Arctic indigenous peoples have adapted to its cold and extreme conditions. Life in the Arctic includes zooplankton and phytoplankton, fish and marine mammals, birds, land animals, plants and human societies.[3] Arctic land is bordered by the subarctic."
graffiti,"Graffiti (both singular and plural; the singular graffito is rarely used except in archeology) is writing or drawings made on a wall or other surface, usually without permission and within public view.[1][2] Graffiti ranges from simple written words to elaborate wall paintings, and has existed since ancient times, with examples dating back to ancient Egypt, ancient Greece, and the Roman Empire.[3]

Graffiti is a controversial subject. In most countries, marking or painting property without permission is considered by property owners and civic authorities as defacement and vandalism, which is a punishable crime, citing the use of graffiti by street gangs to mark territory or to serve as an indicator of gang-related activities.[4] Graffiti has become visualized as a growing urban ""problem"" for many cities in industrialized nations, spreading from the New York City subway system in the early 1970s to the rest of the United States and Europe and other world regions.[5]"
grafitti,"Graffiti (both singular and plural; the singular graffito is rarely used except in archeology) is writing or drawings made on a wall or other surface, usually without permission and within public view.[1][2] Graffiti ranges from simple written words to elaborate wall paintings, and has existed since ancient times, with examples dating back to ancient Egypt, ancient Greece, and the Roman Empire.[3]

Graffiti is a controversial subject. In most countries, marking or painting property without permission is considered by property owners and civic authorities as defacement and vandalism, which is a punishable crime, citing the use of graffiti by street gangs to mark territory or to serve as an indicator of gang-related activities.[4] Graffiti has become visualized as a growing urban ""problem"" for many cities in industrialized nations, spreading from the New York City subway system in the early 1970s to the rest of the United States and Europe and other world regions.[5]"
american ignorance,"Ignorance is a lack of knowledge and information. The word ""ignorant"" is an adjective that describes a person in the state of being unaware, or even cognitive dissonance and other cognitive relation, and can describe individuals who are unaware of important information or facts. Ignorance can appear in three different types: factual ignorance (absence of knowledge of some fact), object ignorance (unacquaintance with some object), and technical ignorance (absence of knowledge of how to do something).[1]"
farm mechanization,"Mechanised agriculture is the process of using agricultural machinery to mechanise the work of agriculture, greatly increasing farm worker productivity. In modern times, powered machinery has replaced many farm jobs formerly carried out by manual labour or by working animals such as oxen, horses and mules.

The entire history of agriculture contains many examples of the use of tools, such as the hoe and the plough. The ongoing integration of machines since the Industrial Revolution however has allowed farming to become much less labour-intensive.

Current mechanised agriculture includes the use of tractors, trucks, combine harvesters, countless types of farm implements, aeroplanes and helicopters (for aerial application), and other vehicles. Precision agriculture even uses computers in conjunction with satellite imagery and satellite navigation (GPS guidance) to increase yields.

Mechanisation was one of the large factors responsible for urbanisation and industrial economies. Besides improving production efficiency, mechanisation encourages large scale production and sometimes can improve the quality of farm produce. On the other hand, it can displace unskilled farm labour and can cause environmental degradation (such as pollution, deforestation, and soil erosion), especially if it is applied shortsightedly rather than holistically."
agricultural mechanization,"Mechanised agriculture is the process of using agricultural machinery to mechanise the work of agriculture, greatly increasing farm worker productivity. In modern times, powered machinery has replaced many farm jobs formerly carried out by manual labour or by working animals such as oxen, horses and mules.

The entire history of agriculture contains many examples of the use of tools, such as the hoe and the plough. The ongoing integration of machines since the Industrial Revolution however has allowed farming to become much less labour-intensive.

Current mechanised agriculture includes the use of tractors, trucks, combine harvesters, countless types of farm implements, aeroplanes and helicopters (for aerial application), and other vehicles. Precision agriculture even uses computers in conjunction with satellite imagery and satellite navigation (GPS guidance) to increase yields.

Mechanisation was one of the large factors responsible for urbanisation and industrial economies. Besides improving production efficiency, mechanisation encourages large scale production and sometimes can improve the quality of farm produce. On the other hand, it can displace unskilled farm labour and can cause environmental degradation (such as pollution, deforestation, and soil erosion), especially if it is applied shortsightedly rather than holistically."
farming methods,"Agriculture is the practice of cultivating plants and livestock.[1] Agriculture was the key development in the rise of sedentary human civilization, whereby farming of domesticated species created food surpluses that enabled people to live in cities. The history of agriculture began thousands of years ago. After gathering wild grains beginning at least 105,000 years ago, nascent farmers began to plant them around 11,500 years ago. Pigs, sheep, and cattle were domesticated over 10,000 years ago. Plants were independently cultivated in at least 11 regions of the world. Industrial agriculture based on large-scale monoculture in the twentieth century came to dominate agricultural output, though about 2 billion people still depended on subsistence agriculture. The major agricultural products can be broadly grouped into foods, fibers, fuels and raw materials (such as rubber). Food classes include cereals (grains), vegetables, fruits, oils, meat, milk, eggs and fungi. Over one-third of the world's workers are employed in agriculture, second only to the service sector, although in recent decades, the global trend of a decreasing number of agricultural workers continues, especially in developing countries where smallholding is being overtaken by industrial agriculture and mechanization that brings a enormous crop yield increase.

Modern agronomy, plant breeding, agrochemicals such as pesticides and fertilizers, and technological developments have sharply increased crop yields, but causing ecological and environmental damage. Selective breeding and modern practices in animal husbandry have similarly increased the output of meat, but have raised concerns about animal welfare and environmental damage. Environmental issues include contributions to global warming, depletion of aquifers, deforestation, antibiotic resistance, and growth hormones in industrial meat production. Agriculture is both a cause of and sensitive to environmental degradation, such as biodiversity loss, desertification, soil degradation and global warming, all of which can cause decreases in crop yield. Genetically modified organisms are widely used, although some are banned in certain countries."
fees,"A fee is the price one pays as remuneration for rights or services. Fees usually allow for overhead, wages, costs, and markup. Traditionally, professionals in the United Kingdom (and previously the Republic of Ireland) receive a fee in contradistinction to a payment, salary, or wage, and often use guineas rather than pounds as units of account. Under the feudal system, a Knight's fee was what was given to a knight for his service, usually the usage of land. A contingent fee is an attorney's fee which is reduced or not charged at all if the court case is lost by the attorney.

A service fee, service charge, or surcharge is a fee added to a customer's bill. The purpose of a service charge often depends on the nature of the product and corresponding service provided. Examples of why this fee is charged are: travel time expenses, truck rental fees, liability and workers' compensation insurance fees, and planning fees. UPS and FedEx have recently begun surcharges for fuel.

Restaurants and banquet halls charging service charges in lieu of tips must distribute them to their wait staff in some US states (e.g., Massachusetts, New York, Montana), but in the state of Kentucky may keep them. A fee may be a flat fee or a variable one, or part of a two-part tariff. A membership fee is charged as part of a subscription business model."
deaf,"Hearing loss is a partial or total inability to hear.[5] Hearing loss may be present at birth or acquired at any time afterwards.[6][7] Hearing loss may occur in one or both ears.[2] In children, hearing problems can affect the ability to acquire spoken language, and in adults it can create difficulties with social interaction and at work.[8] Hearing loss can be temporary or permanent. Hearing loss related to age usually affects both ears and is due to cochlear hair cell loss.[9] In some people, particularly older people, hearing loss can result in loneliness.[2] Deaf people usually have little to no hearing.[6]

Hearing loss may be caused by a number of factors, including: genetics, ageing, exposure to noise, some infections, birth complications, trauma to the ear, and certain medications or toxins.[2] A common condition that results in hearing loss is chronic ear infections.[2] Certain infections during pregnancy, such as cytomegalovirus, syphilis and rubella, may also cause hearing loss in the child.[2][10] Hearing loss is diagnosed when hearing testing finds that a person is unable to hear 25 decibels in at least one ear.[2] Testing for poor hearing is recommended for all newborns.[8] Hearing loss can be categorized as mild (25 to 40 dB), moderate (41 to 55 dB), moderate-severe (56 to 70 dB), severe (71 to 90 dB), or profound (greater than 90 dB).[2] There are three main types of hearing loss: conductive hearing loss, sensorineural hearing loss, and mixed hearing loss.[3]

About half of hearing loss globally is preventable through public health measures.[2] Such practices include immunization, proper care around pregnancy, avoiding loud noise, and avoiding certain medications.[2] The World Health Organization recommends that young people limit exposure to loud sounds and the use of personal audio players to an hour a day in an effort to limit exposure to noise.[11] Early identification and support are particularly important in children.[2] For many, hearing aids, sign language, cochlear implants and subtitles are useful.[2] Lip reading is another useful skill some develop.[2] Access to hearing aids, however, is limited in many areas of the world.[2]

As of 2013 hearing loss affects about 1.1 billion people to some degree.[12] It causes disability in about 466 million people (5% of the global population), and moderate to severe disability in 124 million people.[2][13][14] Of those with moderate to severe disability 108 million live in low and middle income countries.[13] Of those with hearing loss, it began during childhood for 65 million.[15] Those who use sign language and are members of Deaf culture may see themselves as having a difference rather than a disability.[16] Many members of Deaf culture oppose attempts to cure deafness[17][18][19] and some within this community view cochlear implants with concern as they have the potential to eliminate their culture.[20] The terms hearing impairment or hearing loss are often viewed negatively as emphasizing what people cannot do, although the terms are still regularly used when referring to deafness in medical contexts.[16][21]"
nuclear wepaons,"A nuclear weapon (also known as an atom bomb, atomic bomb, nuclear bomb or nuclear warhead, and colloquially as an A-bomb or nuke) is an explosive device that derives its destructive force from nuclear reactions, either fission (fission bomb) or from a combination of fission and fusion reactions (thermonuclear bomb). Both bomb types release large quantities of energy from relatively small amounts of matter.

The first test of a fission (""atomic"") bomb released an amount of energy approximately equal to 20,000 tons of TNT (84 TJ).[1] The first thermonuclear (""hydrogen"") bomb test released energy approximately equal to 10 million tons of TNT (42 PJ). Nuclear bombs have had yields between 10 tons TNT (the W54) and 50 megatons for the Tsar Bomba (see TNT equivalent). A thermonuclear weapon weighing little more than 2,400 pounds (1,100 kg) can release energy equal to more than 1.2 million tons of TNT (5.0 PJ).[2]

A nuclear device no larger than a conventional bomb can devastate an entire city by blast, fire, and radiation. Since they are weapons of mass destruction, the proliferation of nuclear weapons is a focus of international relations policy. Nuclear weapons have been deployed twice in war, by the United States against the Japanese cities of Hiroshima and Nagasaki in 1945 during World War II."
a gamble,"Gambling (also known as betting) is the wagering something of value (""the stakes"") on an event with an uncertain outcome with the intent of winning something else of value. Gambling thus requires three elements to be present: consideration (an amount wagered), risk (chance), and a prize.[1] The outcome of the wager is often immediate, such as a single roll of dice, a spin of a roulette wheel, or a horse crossing the finish line, but longer time frames are also common, allowing wagers on the outcome of a future sports contest or even an entire sports season.

The term ""gaming""[2] in this context typically refers to instances in which the activity has been specifically permitted by law. The two words are not mutually exclusive; i.e., a ""gaming"" company offers (legal) ""gambling"" activities to the public[3] and may be regulated by one of many gaming control boards, for example, the Nevada Gaming Control Board. However, this distinction is not universally observed in the English-speaking world. For instance, in the United Kingdom, the regulator of gambling activities is called the Gambling Commission (not the Gaming Commission).[4] The word gaming is used more frequently since the rise of computer and video games to describe activities that do not necessarily involve wagering, especially online gaming, with the new usage still not having displaced the old usage as the primary definition in common dictionaries. ""Gaming"" has also been used to circumvent laws against ""gambling"". The media and others have used one term or the other to frame conversations around the subjects, resulting in a shift of perceptions among their audiences.[5]

Gambling is also a major international commercial activity, with the legal gambling market totaling an estimated $335 billion in 2009.[6] In other forms, gambling can be conducted with materials that have a value, but are not real money. For example, players of marbles games might wager marbles, and likewise games of Pogs or Magic: The Gathering can be played with the collectible game pieces (respectively, small discs and trading cards) as stakes, resulting in a meta-game regarding the value of a player's collection of pieces."
casino's,"A casino is a facility for certain types of gambling. Casinos are often built near or combined with hotels, resorts, restaurants, retail shopping, cruise ships, and other tourist attractions. Some casinos are also known for hosting live entertainment, such as stand-up comedy, concerts, and sports."
selling genes,"This gene encodes a cell surface adhesion molecule that belongs to a family of adhesion/homing receptors. The encoded protein contains a C-type lectin-like domain, a calcium-binding epidermal growth factor-like domain, and two short complement-like repeats. The gene product is required for binding and subsequent rolling of leucocytes on endothelial cells, facilitating their migration into secondary lymphoid organs and inflammation sites. Single-nucleotide polymorphisms in this gene have been associated with various diseases including immunoglobulin A nephropathy. Alternatively spliced transcript variants have been found for this gene."
gamble,"Gambling (also known as betting) is the wagering something of value (""the stakes"") on an event with an uncertain outcome with the intent of winning something else of value. Gambling thus requires three elements to be present: consideration (an amount wagered), risk (chance), and a prize.[1] The outcome of the wager is often immediate, such as a single roll of dice, a spin of a roulette wheel, or a horse crossing the finish line, but longer time frames are also common, allowing wagers on the outcome of a future sports contest or even an entire sports season.

The term ""gaming""[2] in this context typically refers to instances in which the activity has been specifically permitted by law. The two words are not mutually exclusive; i.e., a ""gaming"" company offers (legal) ""gambling"" activities to the public[3] and may be regulated by one of many gaming control boards, for example, the Nevada Gaming Control Board. However, this distinction is not universally observed in the English-speaking world. For instance, in the United Kingdom, the regulator of gambling activities is called the Gambling Commission (not the Gaming Commission).[4] The word gaming is used more frequently since the rise of computer and video games to describe activities that do not necessarily involve wagering, especially online gaming, with the new usage still not having displaced the old usage as the primary definition in common dictionaries. ""Gaming"" has also been used to circumvent laws against ""gambling"". The media and others have used one term or the other to frame conversations around the subjects, resulting in a shift of perceptions among their audiences.[5]

Gambling is also a major international commercial activity, with the legal gambling market totaling an estimated $335 billion in 2009.[6] In other forms, gambling can be conducted with materials that have a value, but are not real money. For example, players of marbles games might wager marbles, and likewise games of Pogs or Magic: The Gathering can be played with the collectible game pieces (respectively, small discs and trading cards) as stakes, resulting in a meta-game regarding the value of a player's collection of pieces."
poison gas,"The use of toxic chemicals as weapons dates back thousands of years, but the first large scale use of chemical weapons was during World War I.[1][2] They were primarily used to demoralize, injure, and kill entrenched defenders, against whom the indiscriminate and generally very slow-moving or static nature of gas clouds would be most effective. The types of weapons employed ranged from disabling chemicals, such as tear gas, to lethal agents like phosgene, chlorine, and mustard gas. This chemical warfare was a major component of the first global war and first total war of the 20th century. The killing capacity of gas was limited, with about ninety thousand fatalities from a total of 1.3 million casualties caused by gas attacks. Gas was unlike most other weapons of the period because it was possible to develop countermeasures, such as gas masks. In the later stages of the war, as the use of gas increased, its overall effectiveness diminished. The widespread use of these agents of chemical warfare, and wartime advances in the composition of high explosives, gave rise to an occasionally expressed view of World War I as ""the chemist's war"" and also the era where weapons of mass destruction were created.[3][4]

The use of poison gas by all major belligerents throughout World War I constituted war crimes as its use violated the 1899 Hague Declaration Concerning Asphyxiating Gases and the 1907 Hague Convention on Land Warfare, which prohibited the use of ""poison or poisoned weapons"" in warfare.[5][6] Widespread horror and public revulsion at the use of gas and its consequences led to far less use of chemical weapons by combatants during World War II."
ev charger,"A charging station, also called an EV charger or electric vehicle supply equipment (EVSE), is a piece of equipment that supplies electrical power for charging plug-in electric vehicles (including hybrids, neighborhood electric vehicles, trucks, buses, and others).

Although batteries can only be charged with DC power, most electric vehicles have an onboard AC-to-DC converter that allows them to be plugged into a standard household AC electrical receptacle. Inexpensive low-power public charging stations will also provide AC power, known as ""AC charging stations"". To facilitate higher power charging, which requires much larger AC-to-DC converters, the converter is built into the charging station instead of the vehicle, and the station supplies already-converted DC power directly to the vehicle, bypassing the vehicle's onboard converter. These are known as ""DC charging stations"". Most fully electric car models can accept both AC and DC power.

Charging stations provide connectors that conform to a variety of standards. DC charging stations are commonly equipped with multiple connectors to be able to supply a wide variety of vehicles.

Public charging stations are typically found street-side or at retail shopping centers, government facilities, and other parking areas."
subsidation,"A subsidy or government incentive is a form of financial aid or support extended to an economic sector (business, or individual) generally with the aim of promoting economic and social policy.[1] Although commonly extended from the government, the term subsidy can relate to any type of support – for example from NGOs or as implicit subsidies. Subsidies come in various forms including: direct (cash grants, interest-free loans) and indirect (tax breaks, insurance, low-interest loans, accelerated depreciation, rent rebates).[2][3]

Furthermore, they can be broad or narrow, legal or illegal, ethical or unethical. The most common forms of subsidies are those to the producer or the consumer. Producer/production subsidies ensure producers are better off by either supplying market price support, direct support, or payments to factors of production.[3] Consumer/consumption subsidies commonly reduce the price of goods and services to the consumer. For example, in the US at one time it was cheaper to buy gasoline than bottled water.[1]"
immunization,"Vaccination is the administration of a vaccine to help the immune system develop protection from a disease. Vaccines contain a microorganism or virus in a weakened, live or killed state, or proteins or toxins from the organism. In stimulating the body's adaptive immunity, they help prevent sickness from an infectious disease. When a sufficiently large percentage of a population has been vaccinated, herd immunity results. Herd immunity protects those who may be immunocompromised and cannot get a vaccine because even a weakened version would harm them.[1] The effectiveness of vaccination has been widely studied and verified.[2][3][4] Vaccination is the most effective method of preventing infectious diseases;[5][6][7][8] widespread immunity due to vaccination is largely responsible for the worldwide eradication of smallpox and the elimination of diseases such as polio and tetanus from much of the world. However, some diseases, such as measles outbreaks in America, have seen rising cases due to relatively low vaccination rates in the 2010s – attributed, in part, to vaccine hesitancy.[9]

The first disease people tried to prevent by inoculation was most likely smallpox, with the first recorded use of variolation occurring in the 16th century in China.[10] It was also the first disease for which a vaccine was produced.[11][12] Although at least six people had used the same principles years earlier, the smallpox vaccine was invented in 1796 by English physician Edward Jenner. He was the first to publish evidence that it was effective and to provide advice on its production.[13] Louis Pasteur furthered the concept through his work in microbiology. The immunization was called vaccination because it was derived from a virus affecting cows (Latin: vacca 'cow').[11][13] Smallpox was a contagious and deadly disease, causing the deaths of 20–60% of infected adults and over 80% of infected children.[14] When smallpox was finally eradicated in 1979, it had already killed an estimated 300–500 million people in the 20th century.[15][16][17]

Vaccination and immunization have a similar meaning in everyday language. This is distinct from inoculation, which uses unweakened live pathogens. Vaccination efforts have been met with some reluctance on scientific, ethical, political, medical safety, and religious grounds, although no major religions oppose vaccination, and some consider it an obligation due to the potential to save lives.[18] In the United States, people may receive compensation for alleged injuries under the National Vaccine Injury Compensation Program. Early success brought widespread acceptance, and mass vaccination campaigns have greatly reduced the incidence of many diseases in numerous geographic regions."
racism,"Racism is the belief that groups of humans possess different behavioral traits corresponding to inherited attributes and can be divided based on the superiority of one race over another.[1][2][3] It may also mean prejudice, discrimination, or antagonism directed against other people because they are of a different race or ethnicity.[2] Modern variants of racism are often based in social perceptions of biological differences between peoples. These views can take the form of social actions, practices or beliefs, or political systems in which different races are ranked as inherently superior or inferior to each other, based on presumed shared inheritable traits, abilities, or qualities.[2][4] There have been attempts to legitimise racist beliefs through scientific means, which have been overwhelmingly shown to be unfounded.

In terms of political systems (e.g., apartheid) that support the expression of prejudice or aversion in discriminatory practices or laws, racist ideology may include associated social aspects such as nativism, xenophobia, otherness, segregation, hierarchical ranking, and supremacism.

While the concepts of race and ethnicity are considered to be separate in contemporary social science, the two terms have a long history of equivalence in popular usage and older social science literature. ""Ethnicity"" is often used in a sense close to one traditionally attributed to ""race"": the division of human groups based on qualities assumed to be essential or innate to the group (e.g. shared ancestry or shared behavior). Therefore, racism and racial discrimination are often used to describe discrimination on an ethnic or cultural basis, independent of whether these differences are described as racial. According to a United Nations convention on racial discrimination, there is no distinction between the terms ""racial"" and ""ethnic"" discrimination. The UN Convention further concludes that superiority based on racial differentiation is scientifically false, morally condemnable, socially unjust and dangerous. The convention also declared that there is no justification for racial discrimination, anywhere, in theory or in practice.[5]

Racism is a relatively modern concept, arising in the European age of imperialism, the subsequent growth of capitalism, and especially the Atlantic slave trade,[1][6][better source needed] of which it was a major driving force.[7] It was also a major force behind racial segregation especially in the United States in the nineteenth and early twentieth centuries and South Africa under apartheid; 19th and 20th century racism in Western culture is particularly well documented and constitutes a reference point in studies and discourses about racism.[8] Racism has played a role in genocides such as the Holocaust, the Armenian genocide, and the genocide of Serbs, as well as colonial projects including the European colonization of the Americas, Africa, and Asia as well as the Soviet deportations of indigenous minorities.[9] Indigenous peoples have been—and are—often subject to racist attitudes."
learn,learn
hyrdocarbons,"A hydrocarbon is an organic chemical compound composed exclusively of hydrogen and carbon atoms. Hydrocarbons are naturally-occurring compounds and form the basis of crude oil, natural gas, coal, and other important energy sources.

Hydrocarbons are highly combustible and produce carbon dioxide, water, and heat when they are burned. Therefore, hydrocarbons are highly effective as a source of fuel."
pregnancy when older,"Geriatric pregnancy is a rarely used term for having a baby when you’re 35 or older. Rest assured, most healthy women who get pregnant after age 35 and even into their 40s have healthy babies. That doesn't mean you shouldn't think about smart ways to make sure you and your baby stay as healthy as possible during your pregnancy.
Geriatric Pregnancy Risks
Problems can arise no matter how old you are when you get pregnant. But some become more likely when you hit 35, including:
High blood pressure, which can lead to preeclampsia (dangerously high blood pressure and organ damage)
Gestational diabetes
Miscarriage or stillbirth
Labor problems that require you to have a C-section
Premature birth
Low birthweight
Chromosome disorders in the baby, like Down syndrome
Geriatric Pregnancy Benefits
On the other hand, there’s proof you might be doing yourself and your baby a favor by putting off childbearing until you’re older. Studies have shown:
Older moms tend to be better educated and have higher incomes, so they may have more resources than younger moms.
Older moms are more likely to live longer.
Children of older moms may end up healthier, more well-adjusted, and better educated."
education problems due to disabilites,"Physical Inaccessibility: Students with disabilities continue to encounter physical barriers to educational services, such as a lack of ramps and/or elevators in multi-level school buildings, heavy doors, inaccessible washrooms, and/or inaccessible transportation to and from school."
autism,"Autism is a neurodevelopmental disorder characterized by difficulties with social interaction and communication, and by restricted and repetitive behavior.[3] Parents often notice signs during the first three years of their child's life.[1][3] These signs often develop gradually, though some autistic children experience regression in their communication and social skills after reaching developmental milestones at a normal pace.[13]

Autism is associated with a combination of genetic and environmental factors.[4] Risk factors during pregnancy include certain infections, such as rubella, toxins including valproic acid, alcohol, cocaine, pesticides, lead, and air pollution, fetal growth restriction, and autoimmune diseases.[14][15][16] Controversies surround other proposed environmental causes; for example, the vaccine hypothesis, which has been disproven.[17][18] Autism affects information processing in the brain and how nerve cells and their synapses connect and organize; how this occurs is not well understood.[19] The Diagnostic and Statistical Manual of Mental Disorders (DSM-5) combines forms of the condition, including Asperger syndrome and pervasive developmental disorder not otherwise specified (PDD-NOS) into the diagnosis of autism spectrum disorder (ASD).[3][20]

Several interventions have been shown to reduce symptoms and improve the ability of autistic people to function and participate independently in the community.[21] Behavioral, psychological, education, and/or skill-building interventions may be used to assist autistic people to learn life skills necessary for living independently, as well as other social, communication, and language skills. Therapy also aims to reduce challenging behaviors and build upon strengths.[22] Some autistic adults are unable to live independently.[23] An autistic culture has developed, with some individuals seeking a cure and others believing autism should be accepted as a difference to be accommodated instead of cured.[24]

Globally, autism is estimated to affect 24.8 million people as of 2015.[12] In the 2000s, the number of autistic people worldwide was estimated at 1–2 per 1,000 people.[25] In the developed countries, about 1.5% of children are diagnosed with ASD as of 2017,[26] from 0.7% in 2000 in the United States.[27] It is diagnosed four-to-five times more often in males than females.[27] The number of people diagnosed has increased considerably since the 1990s, which may be partly due to increased recognition of the condition."
drug abuse,"Substance abuse, also known as drug abuse, is the use of a drug in amounts or by methods which are harmful to the individual or others. It is a form of substance-related disorder. Differing definitions of drug abuse are used in public health, medical and criminal justice contexts. In some cases, criminal or anti-social behaviour occurs when the person is under the influence of a drug, and long-term personality changes in individuals may also occur.[4] In addition to possible physical, social, and psychological harm, the use of some drugs may also lead to criminal penalties, although these vary widely depending on the local jurisdiction.[5]

Drugs most often associated with this term include: alcohol, amphetamines, barbiturates, benzodiazepines, cannabis, cocaine, hallucinogens, methaqualone, and opioids. The exact cause of substance abuse is not clear, but there are two predominant theories: either a genetic disposition which is learned from others, or a habit which, if addiction develops, manifests itself as a chronic debilitating disease.[6]

In 2010 about 5% of people (230 million) used an illicit substance.[1] Of these, 27 million have high-risk drug use—otherwise known as recurrent drug use—causing harm to their health, causing psychological problems, and/or causing social problems that put them at risk of those dangers.[1][2] In 2015, substance use disorders resulted in 307,400 deaths, up from 165,000 deaths in 1990.[3][7] Of these, the highest numbers are from alcohol use disorders at 137,500, opioid use disorders at 122,100 deaths, amphetamine use disorders at 12,200 deaths, and cocaine use disorders at 11,100"
public educaiton,"State schools (in England, Wales, and New Zealand) or public schools (Scottish English and North American English)[note 1] are generally primary or secondary schools that educate all children without charge. They are funded in whole or in part by taxation.[citation needed] State funded schools exist in virtually every country of the world, though there are significant variations in their structure and educational programmes. State education generally encompasses primary and secondary education (4 years old to 18 years old)."
jobs finding,"Job hunting, job seeking, or job searching is the act of looking for employment, due to unemployment, underemployment, discontent with a current position, or a desire for a better position.

As of 2010, less than 10% of U.S. jobs are filled through online ads.[1]"
genetics,"Genetics is a branch of biology concerned with the study of genes, genetic variation, and heredity in organisms.[1][2][3]

Though heredity had been observed for millennia, Gregor Mendel, Moravian scientist and Augustinian friar working in the 19th century in Brno, was the first to study genetics scientifically. Mendel studied ""trait inheritance"", patterns in the way traits are handed down from parents to offspring over time. He observed that organisms (pea plants) inherit traits by way of discrete ""units of inheritance"". This term, still used today, is a somewhat ambiguous definition of what is referred to as a gene.

Trait inheritance and molecular inheritance mechanisms of genes are still primary principles of genetics in the 21st century, but modern genetics has expanded beyond inheritance to studying the function and behavior of genes. Gene structure and function, variation, and distribution are studied within the context of the cell, the organism (e.g. dominance), and within the context of a population. Genetics has given rise to a number of subfields, including molecular genetics, epigenetics and population genetics. Organisms studied within the broad field span the domains of life (archaea, bacteria, and eukarya).

Genetic processes work in combination with an organism's environment and experiences to influence development and behavior, often referred to as nature versus nurture. The intracellular or extracellular environment of a living cell or organism may switch gene transcription on or off. A classic example is two seeds of genetically identical corn, one placed in a temperate climate and one in an arid climate (lacking sufficient waterfall or rain). While the average height of the two corn stalks may be genetically determined to be equal, the one in the arid climate only grows to half the height of the one in the temperate climate due to lack of water and nutrients in its environment."
modification,"Ethical considerations regarding gene editing are largely controversial in society. The scientific community generally advocates on the side of caution when it comes to whether or not to utilize genome-edited organisms in everyday life. As aforementioned, genetic modifications are studied by researchers under controlled conditions after they are inserted into an organism, which allows for enhancement of the scientific understanding of the effects of certain gene modifications and certain organism responses, and can then be translated into more generalized research for further understanding. Scientists and policymakers are in agreement that public deliberations should decide the legality of germ line genome editing.[10]

The ethical issues of genetic modifications are highly debated, and there are many arguments that can be considered legitimate on both sides. A major argument over the accepted ethics for genetic modifications includes whether the genetic modifications are safe for humans and our environment to endure. While some individuals infer that if genetically modified organisms are tested under controlled laboratory conditions and ensured to be safe, then they will be safe for all purposes; other individuals feel that if genetically modified organisms are unnatural, then they will automatically cause detriment to our society, or they have uncertain potential for harm"
walmart expanding,"A midwife is a health professional who cares for mothers and newborns around childbirth, a specialization known as midwifery.

The education and training for a midwife concentrates extensively on the care of women throughout their lifespan; concentrating on being experts in what is normal and identifying conditions that need further evaluation. In most countries, midwives are recognized as skilled healthcare providers. Midwives are trained to recognize variations from the normal progress of labor and understand how to deal with deviations from normal. They may intervene in high risk situations such as breech births, twin births, and births where the baby is in a posterior position, using non-invasive techniques. For complications related to pregnancy and birth that are beyond the midwife's scope of practice, including surgical and instrumental deliveries, they refer their patients to physicians or surgeons.[3][4] In many parts of the world, these professions work in tandem to provide care to childbearing women. In others, only the midwife is available to provide care, and in yet other countries, many women elect to utilize obstetricians primarily over midwives.

Many developing countries are investing money and training for midwives, sometimes by upskilling those people already practicing as traditional birth attendants. Some primary care services are currently lacking, due to a shortage of funding for these resources."
beauty products,"During the 20th century, the popularity of cosmetics increased rapidly.[34] Cosmetics are used by girls at increasingly young ages, especially in the United States. Because of the fast-decreasing age of makeup users, many companies, from high-street brands like Rimmel to higher-end products like Estee Lauder, cater to this expanding market by introducing flavored lipsticks and glosses, cosmetics packaged in glittery and sparkly packaging, and marketing and advertising using young models.[35] The social consequences of younger and younger cosmetics use has had much attention in the media over the last few years.

Criticism of cosmetics has come from a wide variety of sources including some feminists,[36] religious groups, animal rights activists, authors, and public interest groups. It has also faced criticism from men, some of whom describe it as a form of deception or fakeup.[37]

Safety[edit source]
In the United States: ""Under the law, cosmetic products and ingredients do not need FDA premarket approval.""[38] The EU and other regulatory agencies around the world have more stringent regulations.[39] The FDA does not have to approve or review cosmetics, or what goes in them, before they are sold to the consumers. The FDA only regulates against some colors that can be used in the cosmetics and hair dyes. The cosmetic companies do not have to report any injuries from the products; they also only have voluntary recalls of products.[4]

There has been a marketing trend towards the sale of cosmetics lacking controversial ingredients, especially those derived from petroleum, sodium lauryl sulfate (SLS), and parabens.[40] Per- and polyfluoroalkyl substances (PFAS) are a class of about 9,000 synthetic organofluorine compounds that have multiple highly toxic fluorine atoms attached to an alkyl chain. PFAS are used by major cosmetics industry companies in a wide range of cosmetics, including such products as lipstick, eye liner, mascara, foundation, concealer, lip balm, blush, nail polish. A 2021 study tested 231 personal care products and found organic fluorine, a hallmark of PFAS, in more than half of the samples. Substantial levels of fluorine were identified in tested brands of products as follows: 82% of the brands of waterproof mascara, 63% of the brands of foundations, and 62% of liquid lipstick. PFAS compounds are readily absorbed through human skin and through tear ducts, and such products on lips are often unwittingly ingested. Manufacturers often fail to label their products as containing PFAS, which makes it difficult for cosmetics consumers to avoid products containing PFAS.[41]

Formaldehyde is no longer used in cosmetics but has been replaced by formaldehyde releasers. Formaldehyde is dangerous to human health.[42][43] In 2011, the US National Toxicology Program described formaldehyde as ""known to be a human carcinogen"".[44][45][46]

The danger of formaldehyde is a major reason for the development of formaldehyde releasers which release formaldehyde slowly at lower levels.[47]

Numerous reports have raised concern over the safety of a few surfactants, including 2-butoxyethanol. In some individuals, SLS may cause a number of skin problems, including dermatitis. Additionally, some individuals have had an emergence of vitiliago after using cosmetics containing the ingredient rhododendrol.[48][49][50][51][52][53]

Parabens can cause skin irritation and contact dermatitis in individuals with paraben allergies, a small percentage of the general population.[54] Animal experiments have shown that parabens have a weak estrogenic activity, acting as xenoestrogens.[55]


Patch test
Perfumes are widely used in consumer products. Studies concluded from patch testing show fragrances contain some ingredients which may cause allergic reactions.[56]

Balsam of Peru was the main recommended marker for perfume allergy before 1977, which is still advised. The presence of Balsam of Peru in a cosmetic will be denoted by the INCI term Myroxylon pereirae.[57][58] In some instances, Balsam of Peru is listed on the ingredient label of a product by one of its various names, but it may not be required to be listed by its name by mandatory labeling conventions (in fragrances, for example, it may simply be covered by an ingredient listing of ""fragrance"").[58][59][60][61]

Some cosmetics companies have made pseudo-scientific claims about their products which are misleading or unsupported by scientific evidence.[62][63]

Animal testing[edit source]
Ambox current red Asia Australia.svg
This section needs to be updated. Please help update this article to reflect recent events or newly available information. (September 2016)
Main article: Testing cosmetics on animals
As of 2019 an estimated 50-100 million animals are tested on each year in locations such as the United States and China.[64] Such tests have involved general toxicity, eye and skin irritants, phototoxicity (toxicity triggered by ultraviolet light), and mutagenicity.[65][66] Due to the ethical concerns around animal testing, some nations have legislated against animal testing for cosmetics. An updated list can be found on the Humane Societies website.[67] According to the Humane Society of the United States, there are nearly 50 non-animal tests that have been validated for use, with many more in development, that may replace animal testing and are potentially more efficacious.[68] In the United States, mice, rats, rabbits, and cats are the most used animals for testing.[69] In 2018, California banned the sale of animal tested cosmetics.[70]

Cosmetics testing is banned in the Netherlands, India, Norway, Israel, New Zealand, Belgium, and the UK, and in 2002, the European Union agreed to phase in a near-total ban on the sale of animal-tested cosmetics throughout the EU from 2009, and to ban all cosmetics-related animal testing.[71] In December 2009, the European Parliament and Council passed the EC Regulation 1223/2009 on cosmetics, a bill to regulate the cosmetic industry in the EU.[72] EC Regulation 1223/2009 took effect on July 11, 2013.[72] In March 2013, the EU banned the import and sale of cosmetics containing ingredients tested on animals.[73] China required animal testing on cosmetic products until 2014, when they waived animal testing requirements for domestically produced products.[74] In 2019, China approved nine non-animal testing methods, and announced that by 2020 laws making animal testing compulsory would be lifted.[75]

In June 2017, legislation was proposed in Australia to end animal testing in the cosmetics industry.[76] In March 2019, the Australian Senate passed a bill banning the use of data from animal testing in the cosmetic industry after July 1, 2020.[77]"
mining for pay,"In addition to the environmental impacts of mining processes, a prominent criticism pertaining to this form of extractive practice and of mining companies are the human rights abuses occurring within mining sites and communities in close proximity of them.[83] Frequently, despite being protected by International Labor rights, miners are not given appropriate equipment to provide them with protection from possible mine collapse or from harmful pollutants and chemicals expelled during the mining process, work in inhumane conditions spending numerous hours working in extreme heat, darkness and 14 hour workdays with no allocated time for breaks.[84]

Child labor[edit source]

Breaker boys: child workers who broke down coal at a mine in South Pittston, Pennsylvania, United States in the early 20th century
Included within the human rights abuses that occur during mining processes are instances of child labor. These instances are a cause for widespread criticism of mines harvesting cobalt, a mineral essential for powering modern technologies such as laptops, smartphones and electric vehicles. Many of these cases of child laborers are found in the Democratic Republic of Congo. Reports have risen of children carrying sacks of cobalt weighing 25 kg from small mines to local traders[85] being paid for their work only in food and accommodation. A number of companies such as Apple, Google, Microsoft and Tesla have been implicated in lawsuits brought forth by families whose children were severely injured or killed during mining activities in Congo.[86] In December 2019, 14 Congolese families filed a lawsuit against Glencore, a mining company which supplies the essential cobalt to these multinational corporations with allegations of negligence that led to the deaths of children or injuries such as broken spines, emotional distress and forced labor.[citation needed]

Indigenous peoples[edit source]
There have also been instances of killings and evictions attributed to conflicts with mining companies. Almost a third of 227 murders in 2020 were of Indigenous peoples rights activists on the frontlines of climate change activism linked to logging, mining, large-scale agribusiness, hydroelectric dams, and other infrastructure, according to Global Witness.[87]

The relationship between indigenous peoples and mining is defined by struggles over access to land. In Australia, the Aboriginal Bininj said mining posed a threat to their living culture and could damage sacred heritage sites.[88][89]

In the Philippines, an anti-mining movement has raised concerns regarding ""the total disregard for [Indigenous communities'] ancestral land rights"".[90] Ifugao peoples' opposition to mining led a governor to proclaim a ban on mining operations in Mountain Province, Philippines.[90]

In Brazil, more than 170 tribes organized a march to oppose controversial attempts to strip back indigenous land rights and open their territories to mining operations.[91] The United Nations Commission on Human Rights has called on Brazil's Supreme Court to uphold Indigenous land rights to prevent exploitation by mining groups and industrial agriculture.[92]"
nissan leaf,"The Nissan Leaf, styled as LEAF, is a compact five-door hatchback battery electric vehicle (BEV) manufactured by Nissan. It was introduced in Japan and the United States in December 2010 and is currently in its second generation, introduced in October 2017.

The Leaf's range on a full charge has been increased gradually from 117 km (73 miles) to 364 km (226 miles) (EPA rated), due to the use of a larger battery pack along with several minor improvements.[2]

Among other awards and recognition, the Leaf has won the 2010 Green Car Vision Award, the 2011 European Car of the Year, the 2011 World Car of the Year, and the 2011–2012 Car of the Year Japan.

Global sales totaled 500,000 Leafs by December 2020.[3] As of September 2021, European sales totaled more than 208,000 units,[4] U.S. sales over 161,000 units,[5] and more than 150,000 in Japan.[6]

The Leaf listed as the world's all-time top selling plug-in electric car through December 2019. The Tesla Model 3 surpassed the Leaf in early 2020 to become the all-time best selling electric car.[7][8]"
farmer,"A farmer[1] is a person engaged in agriculture, raising living organisms for food or raw materials. The term usually applies to people who do some combination of raising field crops, orchards, vineyards, poultry, or other livestock. A farmer might own the farmed land or might work as a laborer on land owned by others, but in most developed economies, a farmer is usually a farm owner, while employees of the farm are known as farm workers, or farmhands. However, in other older definitions a farmer was a person who promotes or improves the growth of plants, land or crops or raises animals (as livestock or fish) by labor and attention.

Over half a billion farmers are smallholders, most of whom are in developing countries, and who economically support almost two billion people.[2][3] Globally, women constitute more than 40% of agricultural employees.[4]"
teacher training,"Teacher education or teacher training refers to the policies, procedures, and provision designed to equip (prospective) teachers with the knowledge, attitudes, behaviors, and skills they require to perform their tasks effectively in the classroom, school, and wider community. The professionals who engage in training the prospective teachers are called teacher educators (or, in some contexts, teacher trainers).

There is a longstanding and ongoing debate about the most appropriate term to describe these activities. The term 'teacher training' (which may give the impression that the activity involves training staff to undertake relatively routine tasks) seems to be losing ground, at least in the U.S., to 'teacher education' (with its connotation of preparing staff for a professional role as a reflective practitioner).The two major components of teacher education are in-service teacher education and pre-service teacher education.[1]"
training teachers,"Teacher education or teacher training refers to the policies, procedures, and provision designed to equip (prospective) teachers with the knowledge, attitudes, behaviors, and skills they require to perform their tasks effectively in the classroom, school, and wider community. The professionals who engage in training the prospective teachers are called teacher educators (or, in some contexts, teacher trainers).

There is a longstanding and ongoing debate about the most appropriate term to describe these activities. The term 'teacher training' (which may give the impression that the activity involves training staff to undertake relatively routine tasks) seems to be losing ground, at least in the U.S., to 'teacher education' (with its connotation of preparing staff for a professional role as a reflective practitioner).The two major components of teacher education are in-service teacher education and pre-service teacher education.[1]"
new employees,"Employment is the relationship between two parties, usually based on a contract where work is paid for, where one party, which may be a corporation, for profit, not-for-profit organization, co-operative or other entity is the employer and the other is the employee.[1] Employees work in return for payment, which may be in the form of an hourly wage, by piecework or an annual salary, depending on the type of work an employee does or which sector they are working in. Employees in some fields or sectors may receive gratuities, bonus payment or stock options. In some types of employment, employees may receive benefits in addition to payment. Benefits can include health insurance, housing, disability insurance or use of a gym. Employment is typically governed by employment laws, organisation or legal contracts."
casino,"A casino is a facility for certain types of gambling. Casinos are often built near or combined with hotels, resorts, restaurants, retail shopping, cruise ships, and other tourist attractions. Some casinos are also known for hosting live entertainment, such as stand-up comedy, concerts, and sports."
religion beliefs,"Religion is a social-cultural system of designated behaviors and practices, morals, beliefs, worldviews, texts, sanctified places, prophecies, ethics, or organizations, that relates humanity to supernatural, transcendental, and spiritual elements;[1] however, there is no scholarly consensus over what precisely constitutes a religion.[2][3]

Different religions may or may not contain various elements ranging from the divine,[4] sacred things,[5] faith,[6] a supernatural being or supernatural beings[7] or ""some sort of ultimacy and transcendence that will provide norms and power for the rest of life"".[8] Religious practices may include rituals, sermons, commemoration or veneration (of deities and/or saints), sacrifices, festivals, feasts, trances, initiations, funerary services, matrimonial services, meditation, prayer, music, art, dance, public service, or other aspects of human culture. Religions have sacred histories and narratives, which may be preserved in sacred scriptures, and symbols and holy places, that aim mostly to give a meaning to life. Religions may contain symbolic stories, which are sometimes said by followers to be true, that may also attempt to explain the origin of life, the universe, and other phenomena. Traditionally, faith, in addition to reason, has been considered a source of religious beliefs.[9]

There are an estimated 10,000 distinct religions worldwide.[10] About 84% of the world's population is affiliated with Christianity, Islam, Hinduism, Buddhism, or some form of folk religion.[11] The religiously unaffiliated demographic includes those who do not identify with any particular religion, atheists, and agnostics. While the religiously unaffiliated have grown globally, many of the religiously unaffiliated still have various religious beliefs.[12]

The study of religion comprises a wide variety of academic disciplines, including theology, comparative religion and social scientific studies. Theories of religion offer various explanations for the origins and workings of religion, including the ontological foundations of religious being and belief.[13]"
mother theresa,"Mother Mary Teresa Bojaxhiu[6] (born Anjezë Gonxhe Bojaxhiu, Albanian: [aˈɲɛzə ˈɡɔndʒɛ bɔjaˈdʒiu]; 26 August 1910 – 5 September 1997), honoured in the Catholic Church as Saint Teresa of Calcutta,[7] was an Albanian-Indian[4] Roman Catholic nun and missionary.[8] She was born in Skopje (now the capital of North Macedonia), then part of the Kosovo Vilayet of the Ottoman Empire. After living in Skopje for eighteen years, she moved to Ireland and then to India, where she lived for most of her life.

In 1950, Teresa founded the Missionaries of Charity, a Roman Catholic religious congregation that had over 4,500 nuns and was active in 133 countries in 2012. The congregation manages homes for people who are dying of HIV/AIDS, leprosy and tuberculosis. It also runs soup kitchens, dispensaries, mobile clinics, children's and family counselling programmes, as well as orphanages and schools. Members take vows of chastity, poverty, and obedience, and also profess a fourth vow – to give ""wholehearted free service to the poorest of the poor.""[9]

Teresa received a number of honours, including the 1962 Ramon Magsaysay Peace Prize and the 1979 Nobel Peace Prize. She was canonised on 4 September 2016, and the anniversary of her death (5 September) is her feast day. A controversial figure during her life and after her death, Teresa was admired by many for her charitable work. She was praised and criticized on various counts, such as for her views on abortion and contraception, and was criticized for poor conditions in her houses for the dying. Her authorized biography was written by Navin Chawla and published in 1992, and she has been the subject of films and other books. On 6 September 2017, Teresa and St. Francis Xavier were named co-patrons of the Roman Catholic Archdiocese of Calcutta."
ethic,"Ethics or moral philosophy is a branch[1] of philosophy that ""involves systematizing, defending, and recommending concepts of right and wrong behavior"".[2] The field of ethics, along with aesthetics, concerns matters of value; these fields comprise the branch of philosophy called axiology.[3]

Ethics seeks to resolve questions of human morality by defining concepts such as good and evil, right and wrong, virtue and vice, justice and crime. As a field of intellectual inquiry, moral philosophy is related to the fields of moral psychology, descriptive ethics, and value theory.

Three major areas of study within ethics recognized today are:[2]

Meta-ethics, concerning the theoretical meaning and reference of moral propositions, and how their truth values (if any) can be determined;
Normative ethics, concerning the practical means of determining a moral course of action;
Applied ethics, concerning what a person is obligated (or permitted) to do in a specific situation or a particular domain of action.[2]"
corportations,corportations
olympics losing money,"The Olympic Games, considered to be the world's foremost international sporting event with over 200 nations participating,[1] has historically had the highest costs and expenses for the hosts, with the estimated cost of the 2016 Summer Games in Rio de Janeiro being at approximately US$11.1 billion.[2]

Sports-related costs for the Summer Games since 1960 is on average US$5.2 billion and for the Winter Games $393.1 million dollars. The highest recorded total cost was the 2014 Sochi Winter Olympics, costing approximately US$55 billion. The 2016 Rio de Janeiro Summer Games experienced the biggest loss recorded at approximately US$2 billion.[3]"
electric,"n 1850, William Gladstone asked the scientist Michael Faraday why electricity was valuable. Faraday answered, “One day sir, you may tax it.”[91]

In the 19th and early 20th century, electricity was not part of the everyday life of many people, even in the industrialised Western world. The popular culture of the time accordingly often depicted it as a mysterious, quasi-magical force that can slay the living, revive the dead or otherwise bend the laws of nature.[92] This attitude began with the 1771 experiments of Luigi Galvani in which the legs of dead frogs were shown to twitch on application of animal electricity. ""Revitalization"" or resuscitation of apparently dead or drowned persons was reported in the medical literature shortly after Galvani's work. These results were known to Mary Shelley when she authored Frankenstein (1819), although she does not name the method of revitalization of the monster. The revitalization of monsters with electricity later became a stock theme in horror films.

As the public familiarity with electricity as the lifeblood of the Second Industrial Revolution grew, its wielders were more often cast in a positive light,[93] such as the workers who ""finger death at their gloves' end as they piece and repiece the living wires"" in Rudyard Kipling's 1907 poem Sons of Martha.[93] Electrically powered vehicles of every sort featured large in adventure stories such as those of Jules Verne and the Tom Swift books.[93] The masters of electricity, whether fictional or real—including scientists such as Thomas Edison, Charles Steinmetz or Nikola Tesla—were popularly conceived of as having wizard-like powers.[93]

With electricity ceasing to be a novelty and becoming a necessity of everyday life in the later half of the 20th century, it required particular attention by popular culture only when it stops flowing,[93] an event that usually signals disaster.[93] The people who keep it flowing, such as the nameless hero of Jimmy Webb’s song ""Wichita Lineman"" (1968),[93] are still often cast as heroic, wizard-like figures.[93]"
us and kerry,"John Forbes Kerry (born December 11, 1943) is an American politician and diplomat, currently serving as the first United States special presidential envoy for climate. He previously served as the 68th United States secretary of state from 2013 to 2017. An attorney and former naval officer, Kerry first drew public attention as a decorated Vietnam veteran turned anti-war activist. He went on to serve as a prosecutor and as Lieutenant Governor of Massachusetts, before serving as United States Senator from Massachusetts from 1985 to 2013. A member of the Democratic Party, he was the Democratic nominee for President of the United States in the 2004 election, which he lost to incumbent President George W. Bush.

Kerry grew up as a military brat in Massachusetts and Washington, D.C. before attending boarding school in Massachusetts and New Hampshire. In 1966, after graduating from Yale University, Kerry enlisted in the United States Naval Reserve, ultimately attaining the rank of lieutenant. From 1968 to 1969, during the Vietnam War, he served an abbreviated four-month tour of duty in South Vietnam. While serving as the commanding officer of a Swift boat, Kerry sustained three wounds in combat with the Viet Cong, for which he earned three Purple Heart Medals. Kerry was awarded the Silver Star Medal and the Bronze Star Medal for valorous conduct in separate military engagements. After completing his active military service, Kerry returned to the United States and became an outspoken opponent of the Vietnam War. He gained national recognition as an anti-war activist, serving as a spokesperson for the Vietnam Veterans Against the War organization. Kerry testified in the Fulbright Hearings before the Senate Committee on Foreign Relations, where he described the United States government's policy in Vietnam as the cause of war crimes.

In 1972, Kerry entered electoral politics as a Democratic candidate for the United States House of Representatives in Massachusetts' 5th congressional district. Kerry won the Democratic nomination but was defeated in the general election by his Republican opponent. He subsequently worked as a radio talk show host in Lowell and as the executive director of an advocacy organization while attending the Boston College School of Law. After obtaining his juris doctor in 1976, Kerry served from 1977 to 1979 as the first assistant district attorney of Middlesex County, where he tried criminal cases and managed the district attorney's office. After a period in private legal practice, Kerry was elected Lieutenant Governor of Massachusetts in 1982. In 1984, Kerry was elected to the United States Senate. As a member of the Senate Committee on Foreign Relations, he led a series of hearings investigating narcotics trafficking in Latin America, which exposed aspects of the Iran–Contra affair. He was reelected to additional terms in 1990, 1996, 2002 and 2008.

Kerry won the Democratic Party presidential nomination in 2004, alongside vice presidential nominee and North Carolina Senator John Edwards. Kerry campaigned as a critic of Republican President George W. Bush's prosecution of the Iraq War and advocated a liberal domestic policy. Kerry lost the Electoral College and the popular vote by narrow margins, winning 251 electors to Bush's 286 and 48.3% of the popular vote to Bush's 50.7%. Kerry remained in the Senate and chaired the Committee on Foreign Relations from 2009 to 2013. In January 2013, he was nominated by President Barack Obama to succeed outgoing Secretary of State Hillary Clinton, and was confirmed by his Senate colleagues on a vote of 94 to 3. As Secretary of State, Kerry initiated the 2013–2014 Israeli–Palestinian peace talks and negotiated agreements restricting the nuclear program of Iran, including the 2013 Joint Plan of Action and the 2015 Joint Comprehensive Plan of Action. In 2015, Kerry signed the Paris Agreement on climate change on behalf of the United States. Kerry served as Secretary of State until the end of the Obama administration in January 2017, when he left government service. Kerry remained active in public affairs as a vocal opponent of Obama's successor, Donald Trump, president from 2017 to 2021, and as a supporter of Trump's successor, Kerry's former U.S. Senate colleague Joe Biden, who had served as Obama's vice president. Kerry returned to government in January 2021, becoming the first person to hold a new position, U.S. special presidential envoy for climate, in Biden's administration."
kerry's threat,"John Forbes Kerry (born December 11, 1943) is an American politician and diplomat, currently serving as the first United States special presidential envoy for climate. He previously served as the 68th United States secretary of state from 2013 to 2017. An attorney and former naval officer, Kerry first drew public attention as a decorated Vietnam veteran turned anti-war activist. He went on to serve as a prosecutor and as Lieutenant Governor of Massachusetts, before serving as United States Senator from Massachusetts from 1985 to 2013. A member of the Democratic Party, he was the Democratic nominee for President of the United States in the 2004 election, which he lost to incumbent President George W. Bush.

Kerry grew up as a military brat in Massachusetts and Washington, D.C. before attending boarding school in Massachusetts and New Hampshire. In 1966, after graduating from Yale University, Kerry enlisted in the United States Naval Reserve, ultimately attaining the rank of lieutenant. From 1968 to 1969, during the Vietnam War, he served an abbreviated four-month tour of duty in South Vietnam. While serving as the commanding officer of a Swift boat, Kerry sustained three wounds in combat with the Viet Cong, for which he earned three Purple Heart Medals. Kerry was awarded the Silver Star Medal and the Bronze Star Medal for valorous conduct in separate military engagements. After completing his active military service, Kerry returned to the United States and became an outspoken opponent of the Vietnam War. He gained national recognition as an anti-war activist, serving as a spokesperson for the Vietnam Veterans Against the War organization. Kerry testified in the Fulbright Hearings before the Senate Committee on Foreign Relations, where he described the United States government's policy in Vietnam as the cause of war crimes.

In 1972, Kerry entered electoral politics as a Democratic candidate for the United States House of Representatives in Massachusetts' 5th congressional district. Kerry won the Democratic nomination but was defeated in the general election by his Republican opponent. He subsequently worked as a radio talk show host in Lowell and as the executive director of an advocacy organization while attending the Boston College School of Law. After obtaining his juris doctor in 1976, Kerry served from 1977 to 1979 as the first assistant district attorney of Middlesex County, where he tried criminal cases and managed the district attorney's office. After a period in private legal practice, Kerry was elected Lieutenant Governor of Massachusetts in 1982. In 1984, Kerry was elected to the United States Senate. As a member of the Senate Committee on Foreign Relations, he led a series of hearings investigating narcotics trafficking in Latin America, which exposed aspects of the Iran–Contra affair. He was reelected to additional terms in 1990, 1996, 2002 and 2008.

Kerry won the Democratic Party presidential nomination in 2004, alongside vice presidential nominee and North Carolina Senator John Edwards. Kerry campaigned as a critic of Republican President George W. Bush's prosecution of the Iraq War and advocated a liberal domestic policy. Kerry lost the Electoral College and the popular vote by narrow margins, winning 251 electors to Bush's 286 and 48.3% of the popular vote to Bush's 50.7%. Kerry remained in the Senate and chaired the Committee on Foreign Relations from 2009 to 2013. In January 2013, he was nominated by President Barack Obama to succeed outgoing Secretary of State Hillary Clinton, and was confirmed by his Senate colleagues on a vote of 94 to 3. As Secretary of State, Kerry initiated the 2013–2014 Israeli–Palestinian peace talks and negotiated agreements restricting the nuclear program of Iran, including the 2013 Joint Plan of Action and the 2015 Joint Comprehensive Plan of Action. In 2015, Kerry signed the Paris Agreement on climate change on behalf of the United States. Kerry served as Secretary of State until the end of the Obama administration in January 2017, when he left government service. Kerry remained active in public affairs as a vocal opponent of Obama's successor, Donald Trump, president from 2017 to 2021, and as a supporter of Trump's successor, Kerry's former U.S. Senate colleague Joe Biden, who had served as Obama's vice president. Kerry returned to government in January 2021, becoming the first person to hold a new position, U.S. special presidential envoy for climate, in Biden's administration."
dual citizienship,"Multiple/dual citizenship (or multiple/dual nationality) is a legal status in which a person is concurrently regarded as a national or citizen of more than one country under the laws of those countries.[1] Conceptually, citizenship is focused on the internal political life of the country and nationality is a matter of international dealings.[2] There is no international convention which determines the nationality or citizenship status of a person. This is defined exclusively by national laws, which can vary and conflict with each other. Multiple citizenship arises because different countries use different, and not necessarily mutually exclusive, criteria for citizenship. Colloquially, people may ""hold"" multiple citizenship but, technically, each nation makes a claim that a particular person is considered its national.

A person holding multiple citizenship is, generally, entitled to the rights of citizenship in each country whose citizenship they are holding (such as right to a passport, right to enter the country, right to residence and work, right to vote, etc.), but may also be subject to obligations of citizenship (such as a potential obligation for national service, becoming subject to taxation on worldwide income, etc.).

Some countries do not permit dual citizenship or only do in certain cases (e.g. inheriting multiple nationalities at birth). This may be by requiring an applicant for naturalization to renounce all existing citizenship, or by withdrawing its citizenship from someone who voluntarily acquires another citizenship, or by other devices. Some countries permit a renunciation of citizenship, while others do not. Some countries permit a general dual citizenship while others permit dual citizenship but only of a limited number of countries.

A country that allows dual citizenship may still not recognize the other citizenship of its nationals within its own territory (for example, in relation to entry into the country, national service, duty to vote, etc.). Similarly, it may not permit consular access by another country for a person who is also its national. Some countries prohibit dual citizenship holders from serving in their armed forces or on police forces or holding certain public offices.[3]"
2big2fail banks,"""Too big to fail"" (TBTF) is a theory in banking and finance that asserts that certain corporations, particularly financial institutions, are so large and so interconnected that their failure would be disastrous to the greater economic system, and that they therefore must be supported by governments when they face potential failure.[1] The colloquial term ""too big to fail"" was popularized by U.S. Congressman Stewart McKinney in a 1984 Congressional hearing, discussing the Federal Deposit Insurance Corporation's intervention with Continental Illinois.[2] The term had previously been used occasionally in the press,[3] and similar thinking had motivated earlier bank bailouts.[4]

The term emerged as prominent in public discourse following the global financial crisis of 2007–2008.[5][6] Critics see the policy as counterproductive and that large banks or other institutions should be left to fail if their risk management is not effective.[7][8] Some critics, such as Alan Greenspan, believe that such large organisations should be deliberately broken up: ""If they're too big to fail, they're too big"".[9] Some economists such as Paul Krugman hold that financial crises arise principally from banks being under-regulated rather than their size, using the widespread collapse of small banks in the Great Depression to illustrate this argument.[10][11][12][13]

In 2014, the International Monetary Fund and others said the problem still had not been dealt with.[14][15] While the individual components of the new regulation for systemically important banks (additional capital requirements, enhanced supervision and resolution regimes) likely reduced the prevalence of TBTF, the fact that there is a definite list of systemically important banks considered TBTF has a partly offsetting impact.[16]"
population growth,"Population growth is the increase in the number of people in a population. Global human population growth amounts to around 83 million annually,[2] or 1.1% per year. The global population has grown from 1 billion in 1800 to 7.9 billion[3] in 2020. The UN projected population to keep growing, and estimates have put the total population at 8.6 billion by mid-2030, 9.8 billion by mid-2050 and 11.2 billion by 2100.[4] However, some academics outside the UN have increasingly developed human population models that account for additional downward pressures on population growth; in such a scenario population would peak before 2100.[5] A popular estimate of sustainable population is 8 billion people.

World human population has been growing since the end of the Black Death, around the year 1350.[6] A mix of technological advancement that improved agricultural productivity and sanitation and medical advancement that reduced mortality have caused an exponential population growth. In some geographies, this has slowed through the process called the demographic transition, where many nations with high standards of living have seen a significant slowing of population growth. This is in direct contrast with less developed contexts, where population growth is still happening.[7] However, the global human population is projected to peak during the mid-21st century and decline by 2100.[8]

Population growth alongside overconsumption is a key driver of environmental concerns, such as biodiversity loss and climate change, due to resource-intensive human development that exceed planetary boundaries.[9] International policy focused on mitigating the impact of human population growth is concentrated in the Sustainable Development Goals which seek to improve the standard of living globally while reducing the impact of society on the environment."
deportation,"Deportation refers to the expulsion of a person or group of people from a place or country. The term expulsion is often used as a synonym for deportation, though expulsion is more often used in the context of international law, while deportation is more used in national (municipal) law.[1] Forced displacement or forced migration of an individual or a group may be caused by deportation, for example ethnic cleansing, and other reasons. A person who has been deported or is under sentence of deportation is called a deportee.[2]"
legalization of marijuanua,"n the United States, the use and possession of cannabis is illegal under federal law for any purpose pursuant to the Controlled Substances Act of 1970 (CSA). Under the CSA, cannabis is classified as a Schedule I substance, determined to have a high potential for abuse and no accepted medical use – thereby prohibiting even medical use of the drug.[1] However, at the state level policies regarding the medical and recreational use of cannabis vary greatly, and in many states conflict significantly with federal law.

The medical use of cannabis is legal with a doctor's recommendation in 36 states, four out of five permanently inhabited U.S. territories, and the District of Columbia.[2] Twelve other states have laws that limit THC content, for the purpose of allowing access to products that are rich in cannabidiol (CBD), a non-psychoactive component of cannabis.[2] Although cannabis remains a Schedule I drug, the Rohrabacher–Farr amendment prohibits federal prosecution of individuals complying with state medical cannabis laws.[3]

The recreational use of cannabis is legalized in 18 states,[a] the District of Columbia, the Northern Mariana Islands, and Guam. Another 13 states and the U.S. Virgin Islands have decriminalized its use.[4] Commercial distribution of cannabis has been legalized in all jurisdictions where possession has been legalized, except the District of Columbia. Prior to January 2018, the Cole Memorandum provided some protection against enforcement of federal law in states that have legalized cannabis, but the memorandum was rescinded by Attorney General Jeff Sessions.[5]

Although the use of cannabis remains federally illegal, some of its derivative compounds have been approved by the Food and Drug Administration for prescription use. Cannabinoid drugs which have received FDA approval are Marinol (THC), Syndros (THC), Cesamet (nabilone), and Epidiolex (cannabidiol). For non-prescription use, cannabidiol derived from industrial hemp is legal at the federal level, but legality and enforcement varies by state.[6][7]"
conservative,"Conservatism is an aesthetic, cultural, social, and political philosophy, which seeks to promote and to preserve traditional social institutions.[1] The central tenets of conservatism may vary in relation to the traditional values or practices of the culture and civilization in which it appears. In Western culture, conservatives seek to preserve a range of institutions such as organized religion, parliamentary government, and property rights.[2] Adherents of conservatism often oppose modernism and seek a return to traditional values.[3][4]

The first established use of the term in a political context originated in 1818 with François-René de Chateaubriand[5] during the period of Bourbon Restoration that sought to roll back the policies of the French Revolution. Historically associated with right-wing politics, the term has since been used to describe a wide range of views. There is no single set of policies regarded as conservative because the meaning of conservatism depends on what is considered traditional in a given place and time. Conservative thought has varied considerably as it has adapted itself to existing traditions and national cultures.[6] For example, some conservatives advocate for greater government intervention in the economy[7] while others advocate for a more laissez faire free market economic system.[8] Thus conservatives from different parts of the world—each upholding their respective traditions—may disagree on a wide range of issues. Edmund Burke, an 18th-century politician who opposed the French Revolution, but earlier paradoxically supported the American Revolution, is credited as one of the main theorists of conservatism in the 1790s.[9]"
jp morgan chase,"JPMorgan Chase & Co. is an American multinational investment bank and financial services holding company headquartered in New York City. JPMorgan Chase is incorporated in Delaware.[4] As of June 30, 2021, JPMorgan Chase is the largest bank in the United States, the world's largest bank by market capitalization, and the fifth-largest worldwide in terms of total assets, controlling US$3.684 trillion.[5]

As a ""Bulge Bracket"" bank, it is a major provider of various investment banking and financial services. As of 2021 it is the largest lender to the fossil fuel industry in the world.[6] It is one of America's Big Four banks, along with Bank of America, Citigroup, and Wells Fargo.[7] JPMorgan Chase is considered to be a universal bank and a custodian bank. The J.P. Morgan brand is used by the investment banking, asset management, private banking, private wealth management, and treasury services divisions. Fiduciary activity within private banking and private wealth management is done under the aegis of JPMorgan Chase Bank, N.A.—the actual trustee. The Chase brand is used for credit card services in the United States and Canada, the bank's retail banking activities in the United States, and commercial banking. Both the retail and commercial bank and the bank's corporate headquarters are currently located at 383 Madison Avenue in Midtown Manhattan, New York City, since the prior headquarters building directly across the street, 270 Park Avenue, was demolished and a larger replacement headquarters is being built on the same site.[8] It is considered a systemically important bank by the Financial Stability Board.

The current company was originally known as Chemical Bank, which acquired Chase Manhattan and assumed that company's name. The present company was formed in 2000, when Chase Manhattan Corporation merged with J.P. Morgan & Co.[8]"
unpaid intenship,"Internships can be a true win-win situation: Companies get an opportunity to utilize unpaid labor and find their employees of the future. And interns get to learn the ropes, gain real-world experience, and build up their resumes for the post-graduation job search.

While internships have numerous advantages, employers who hire unpaid interns might face legal battles and class action lawsuits. If you don't follow the rules to make sure an internship is a true learning experience, that intern starts to look a lot like an employee who just isn't getting paid. And that's when the wage and hour problems begin."
peace corpsp,"Frank Zappa and The Mothers of Invention have a song named ""Who Needs the Peace Corps?"" on their 1968 album We're Only in It for the Money.

In popular culture, the Peace Corps has been used as a comedic plot device in such movies as Airplane!, Christmas with the Kranks, Shallow Hal, and Volunteers or used to set the scene for a historic era, as when Frances ""Baby"" Houseman tells the audience she plans to join the Peace Corps in the introduction to the movie Dirty Dancing.[133]

The Peace Corps has also been documented on film and examined more seriously and in more depth. The 2006 documentary film Death of Two Sons, directed by Micah Schaffer, juxtaposes the deaths of Amadou Diallo, a Guinean-American who was gunned down by four New York City policemen with 41 bullets, and Peace Corps volunteer Jesse Thyne who lived with Amadou's family in Guinea and died in a car crash there.[134] Jimi Sir, released in 2007, is a documentary portrait of volunteer James Parks' experiences as a high school science, math and English teacher during the last 10 weeks of his service in Nepal.[135] James speaks Nepali fluently and shows a culture where there are no roads, vehicles, electricity, plumbing, telephone or radio.[135] The movie El Rey, directed and written by Antonio Dorado in 2004, attacks corrupt police, unscrupulous politicians and half-hearted revolutionaries but also depicts the urban legend of Peace Corps Volunteers ""training"" native Colombians how to process coca leaves into cocaine.[136]

In the 1969 film, Yawar Mallku/Sangre de cóndor/Blood of the Condor, Bolivian director Jorge Sanjinés portrayed Peace Corps volunteers in the camp as arrogant, ethnocentric, and narrow-minded imperialists out to destroy Indian culture. One particularly powerful scene showed Indians attacking a clinic while the volunteers inside sterilized Indian women against their will. The film is thought to be at least partially responsible for the expulsion of the Peace Corps from Bolivia in 1971. Peace Corps volunteer Fred Krieger who was serving in Bolivia at the time said, ""It was an effective movie – emotionally very arousing – and it directly targeted Peace Corps volunteers. I thought I would be lynched before getting out of the theatre. To my amazement, people around me smiled courteously as we left, no one commented, it was just like any other movie.""[137]

In 2016, Peace Corps partnered with jewelry retailer Alex and Ani to create cord bracelets to raise money for the Peace Corps' Let Girls Learn Fund.[138]

Fictional Peace Corps volunteers

Frances ""Baby"" Houseman in the 1987 film Dirty Dancing plans to join the Peace Corps after graduating from Mount Holyoke.[139]
In Boy Meets World, Jack and Rachel graduate from the fictional Pennbrook University and join the Peace Corps.[140]"
humans are devolving,"Devolution, de-evolution, or backward evolution is the notion that species can revert to supposedly more primitive forms over time. The concept relates to the idea that evolution has a purpose (teleology) and is progressive (orthogenesis), for example that feet might be better than hooves or lungs than gills. However, evolutionary biology makes no such assumptions, and natural selection shapes adaptations with no foreknowledge of any kind. It is possible for small changes (such as in the frequency of a single gene) to be reversed by chance or selection, but this is no different from the normal course of evolution and as such de-evolution is not compatible with a proper understanding of evolution due to natural selection.

In the 19th century, when belief in orthogenesis was widespread, zoologists (such as Ray Lankester and Anton Dohrn) and the palaeontologists Alpheus Hyatt and Carl H. Eigenmann advocated the idea of devolution. The concept appears in Kurt Vonnegut's 1985 novel Galápagos, which portrays a society that has evolved backwards to have small brains.

Dollo's law of irreversibility, first stated in 1893 by the palaeontologist Louis Dollo, denies the possibility of devolution. The evolutionary biologist Richard Dawkins explains Dollo's law as being simply a statement about the improbability of evolution's following precisely the same path twice.

The term ""devolution"" and its associated concepts never were prominent in biology and now are at most of historical interest, except where they have been adopted by creationists."
justice,"Justice, in its broadest sense, is the principle that people receive that which they deserve, with the interpretation of what then constitutes ""deserving"" being impacted upon by numerous fields, with many differing viewpoints and perspectives, including the concepts of moral correctness based on ethics, rationality, law, religion, equity and fairness. The state will sometimes endeavour to increase justice by operating courts and enforcing their rulings.

Consequently, the application of justice differs in every culture. Early theories of justice were set out by the Ancient Greek philosophers Plato in his work The Republic, and Aristotle in his Nicomachean Ethics. Throughout history various theories have been established. Advocates of divine command theory have said that justice issues from God. In the 1600s, philosophers such as John Locke said that justice derives from natural law. Social contract theory said that justice is derived from the mutual agreement of everyone. In the 1800s, utilitarian philosophers such as John Stuart Mill said that justice is based on the best outcomes for the greatest number of people. Theories of distributive justice study what is to be distributed, between whom they are to be distributed, and what is the proper distribution. Egalitarians have said that justice can only exist within the coordinates of equality. John Rawls used a social contract theory to say that justice, and especially distributive justice, is a form of fairness. Robert Nozick and others said that property rights, also within the realm of distributive justice and natural law, maximizes the overall wealth of an economic system. Theories of retributive justice say that wrongdoing should be punished to insure justice. The closely related restorative justice (also sometimes called ""reparative justice"") is an approach to justice that focuses on the needs of victims and offenders."
roger clegg,"Roger Clegg is a Board Member at and former President and General Counsel of the Center for Equal Opportunity. He focuses on legal issues arising from civil rights laws--including the regulatory impact on business and the problems in higher education created by affirmative action. A former Deputy Assistant Attorney General in the Reagan and Bush administrations, Clegg held the second highest positions in both the Civil Rights Division (1987-91) and in the Environment and Natural Resources Division (1991-93). He has held several other positions at the U.S. Justice Department, including Assistant to the Solicitor General (1985-87), Associate Deputy Attorney General (1984-85), and Acting Assistant Attorney General in the Office of Legal Policy (1984). Clegg is a graduate of Yale University Law School (1981)."
medicare,"Medicare is a national health insurance program in the United States, begun in 1965 under the Social Security Administration (SSA) and now administered by the Centers for Medicare and Medicaid Services (CMS). It primarily provides health insurance for Americans aged 65 and older, but also for some younger people with disability status as determined by the SSA, including people with end stage renal disease and amyotrophic lateral sclerosis (ALS or Lou Gehrig's disease).

In 2018, according to the 2019 Medicare Trustees Report, Medicare provided health insurance for over 59.9 million individuals—more than 52 million people aged 65 and older and about 8 million younger people.[1] According to annual Medicare Trustees reports and research by the government's MedPAC group, Medicare covers about half of healthcare expenses of those enrolled. Enrollees almost always cover most of the remaining costs by taking additional private insurance and/or by joining a public Part C or Part D Medicare health plan.[2] In 2020, US federal government spending on Medicare was $776.2 billion.[3]

No matter which of those two options the beneficiaries choose—or if they choose to do nothing extra (around 1% according to annual Medicare Trustees reports over time), beneficiaries also have other healthcare-related costs. These additional so-called out of pocket (OOP) costs can include deductibles and co-pays; the costs of uncovered services—such as for long-term custodial, dental, hearing, and vision care; the cost of annual physical exams (for those not on Part C health plans that include physicals); and the costs related to basic Medicare's lifetime and per-incident limits. Medicare is funded by a combination of a specific payroll tax, beneficiary premiums, and surtaxes from beneficiaries, co-pays and deductibles, and general U.S. Treasury revenue.

Medicare is divided into four Parts: A, B, C and D.

Part A covers hospital (inpatient, formally admitted only), skilled nursing (only after being formally admitted to a hospital for three days and not for custodial care), and hospice services.
Part B covers outpatient services including some providers' services while inpatient at a hospital, outpatient hospital charges, most provider office visits even if the office is ""in a hospital"", and most professionally administered prescription drugs.
Part D covers mostly self-administered prescription drugs.
Part C is an alternative called Managed Medicare or Medicare Advantage which allows patients to choose health plans with at least the same service coverage as Parts A and B (and most often more), often the benefits of Part D, and always an annual out-of-pocket expense limit which A and B lack. A beneficiary must enroll in Parts A and B first before signing up for Part C.[4]"
diet,"A healthy diet is a diet that helps maintain or improve overall health. A healthy diet provides the body with essential nutrition: fluid, macronutrients, micronutrients, and adequate food energy.[2][3]

A healthy diet may contain fruits, vegetables, and whole grains, and may include little to no processed food or sweetened beverages. The requirements for a healthy diet can be met from a variety of plant-based and animal-based foods, although a non-plant source of vitamin B12 is needed for those following a vegan diet.[4] Various nutrition guides are published by medical and governmental institutions to educate individuals on what they should be eating to be healthy. Nutrition facts labels are also mandatory in some countries to allow consumers to choose between foods based on the components relevant to health.[5][6]"
democracy,"Democracy is a form of government in which the people have the authority to deliberate and decide legislation (""direct democracy""), or to choose governing officials to do so (""representative democracy""). Who is considered part of ""the people"" and how authority is shared among or delegated by the people has changed over time and at different rates in different countries, but over time more and more of a democratic country's inhabitants have generally been included. Cornerstones of democracy include freedom of assembly, association and speech, inclusiveness and equality, citizenship, consent of the governed, voting rights, freedom from unwarranted governmental deprivation of the right to life and liberty, and minority rights.

The notion of democracy has evolved over time considerably. The original form of democracy was a direct democracy. The most common form of democracy today is a representative democracy, where the people elect government officials to govern on their behalf such as in a parliamentary or presidential democracy.[2]

Prevalent day-to-day decision making of democracies is the majority rule,[3][4] though other decision making approaches like supermajority and consensus have also been integral to democracies. They serve the crucial purpose of inclusiveness and broader legitimacy on sensitive issues—counterbalancing majoritarianism—and therefore mostly take precedence on a constitutional level. In the common variant of liberal democracy, the powers of the majority are exercised within the framework of a representative democracy, but the constitution limits the majority and protects the minority—usually through the enjoyment by all of certain individual rights, e.g. freedom of speech or freedom of association.[5][6]

The term appeared in the 5th century BC to denote the political systems then existing in Greek city-states, notably Classical Athens, to mean ""rule of the people"", in contrast to aristocracy (ἀριστοκρατία, aristokratía), meaning ""rule of an elite"".[7] Western democracy, as distinct from that which existed in antiquity, is generally considered to have originated in city-states such as those in Classical Athens and the Roman Republic, where various schemes and degrees of enfranchisement of the free male population were observed before the form disappeared in the West at the beginning of late antiquity. In virtually all democratic governments throughout ancient and modern history, democratic citizenship consisted of an elite class until full enfranchisement was won for all adult citizens in most modern democracies through the suffrage movements of the 19th and 20th centuries.

Democracy contrasts with forms of government where power is either held by an individual, as in autocratic systems like absolute monarchy, or where power is held by a small number of individuals, as in an oligarchy—oppositions inherited from ancient Greek philosophy.[8] Karl Popper defined democracy in contrast to dictatorship or tyranny, focusing on opportunities for the people to control their leaders and to oust them without the need for a revolution.[9]"
islam,"Islam is an Abrahamic monotheistic religion teaching that Muhammad is a messenger of God.[2][3] It is the world's second-largest religion with 1.9 billion followers, or 24.9% of the world's population,[4][5] known as Muslims.[6] Muslims make up a majority of the population in 47 countries.[7][8] Islam teaches that God is merciful, all-powerful, and unique,[9] and has guided humanity through prophets, revealed scriptures, and natural signs.[3][10] The primary scriptures of Islam are the Quran, believed to be the verbatim word of God, as well as the teachings and normative examples (called the sunnah, composed of accounts called hadith) of Muhammad (c. 570 – 632 CE).[11]

Muslims believe that Islam is the complete and universal version of a primordial faith that was revealed many times before through prophets such as Adam, Abraham, Moses, and Jesus.[12] Muslims consider the Quran, in Arabic, to be the unaltered and final revelation of God.[13] Like other Abrahamic religions, Islam also teaches a final judgment with the righteous rewarded in paradise and the unrighteous punished in hell.[14] Religious concepts and practices include the Five Pillars of Islam, which are obligatory acts of worship, as well as following Islamic law (sharia), which touches on virtually every aspect of life and society, from banking and welfare to women and the environment.[15][16] The cities of Mecca, Medina and Jerusalem are home to the three holiest sites in Islam.[17]

From a historical point of view, Islam originated in early 7th century CE in the Arabian Peninsula, in Mecca,[18] and by the 8th century, the Umayyad Caliphate extended from Iberia in the west to the Indus River in the east. The Islamic Golden Age refers to the period traditionally dated from the 8th century to the 13th century, during the Abbasid Caliphate, when much of the historically Muslim world was experiencing a scientific, economic, and cultural flourishing.[19][20][21] The expansion of the Muslim world involved various states and caliphates such as the Ottoman Empire, trade, and conversion to Islam by missionary activities (dawah).[22]

Most Muslims are of one of two denominations: Sunni (85–90%)[23] or Shia (10–15%).[24][25][26] Sunni and Shia differences arose from disagreement over the succession to Muhammad and acquired broader political significance, as well as theological and juridical dimensions.[27] About 12% of Muslims live in Indonesia, the most populous Muslim-majority country;[28] 31% live in South Asia,[29] the largest percentage of Muslims in the world;[30] 20% in the Middle East–North Africa, where it is the dominant religion;[31] and 15% in sub-Saharan Africa.[31] Sizable Muslim communities can also be found in the Americas, China, and Europe.[32][33] Islam is the fastest-growing major religion in the world.[34][35]"
vaccanations,"Vaccination is the administration of a vaccine to help the immune system develop protection from a disease. Vaccines contain a microorganism or virus in a weakened, live or killed state, or proteins or toxins from the organism. In stimulating the body's adaptive immunity, they help prevent sickness from an infectious disease. When a sufficiently large percentage of a population has been vaccinated, herd immunity results. Herd immunity protects those who may be immunocompromised and cannot get a vaccine because even a weakened version would harm them.[1] The effectiveness of vaccination has been widely studied and verified.[2][3][4] Vaccination is the most effective method of preventing infectious diseases;[5][6][7][8] widespread immunity due to vaccination is largely responsible for the worldwide eradication of smallpox and the elimination of diseases such as polio and tetanus from much of the world. However, some diseases, such as measles outbreaks in America, have seen rising cases due to relatively low vaccination rates in the 2010s – attributed, in part, to vaccine hesitancy.[9]

The first disease people tried to prevent by inoculation was most likely smallpox, with the first recorded use of variolation occurring in the 16th century in China.[10] It was also the first disease for which a vaccine was produced.[11][12] Although at least six people had used the same principles years earlier, the smallpox vaccine was invented in 1796 by English physician Edward Jenner. He was the first to publish evidence that it was effective and to provide advice on its production.[13] Louis Pasteur furthered the concept through his work in microbiology. The immunization was called vaccination because it was derived from a virus affecting cows (Latin: vacca 'cow').[11][13] Smallpox was a contagious and deadly disease, causing the deaths of 20–60% of infected adults and over 80% of infected children.[14] When smallpox was finally eradicated in 1979, it had already killed an estimated 300–500 million people in the 20th century.[15][16][17]

Vaccination and immunization have a similar meaning in everyday language. This is distinct from inoculation, which uses unweakened live pathogens. Vaccination efforts have been met with some reluctance on scientific, ethical, political, medical safety, and religious grounds, although no major religions oppose vaccination, and some consider it an obligation due to the potential to save lives.[18] In the United States, people may receive compensation for alleged injuries under the National Vaccine Injury Compensation Program. Early success brought widespread acceptance, and mass vaccination campaigns have greatly reduced the incidence of many diseases in numerous geographic regions."
conservative resistance,"Conservatism in the United States is a political and social philosophy which characteristically prioritizes American traditions, republicanism, and limited federal governmental power in relation to the states, referred to more simply as limited government and states' rights. It typically supports Christian values,[1] moral absolutism,[2] traditional family values,[3] American exceptionalism,[4] and individualism.[5] It is generally pro-capitalist[6] and pro-business while opposing trade unions. It often advocates for a strong national defense, gun rights, free trade,[7] and a defense of Western culture from perceived threats posed by communism,[8] socialism, and moral relativism.[9]

American conservatives generally consider individual liberty—within the bounds of conservative values—as the fundamental trait of democracy.[10][11] They typically believe in a balance between federal government and states' rights. Apart from some right-libertarians, American conservatives tend to favor strong action in areas they believe to be within government's legitimate jurisdiction, particularly national defense and law enforcement. Social conservatives—many of them religious—often oppose abortion, civil unions, and same-sex marriage. They often favor Christian prayer in public schools and government funding for private Christian schools.[12][13][3][14]

Like most political ideologies in the United States, conservatism originates from republicanism, which rejects aristocratic and monarchical government and upholds the principles of the 1776 U.S. Declaration of Independence (""that all men are created equal, that they are endowed by their Creator with certain unalienable Rights, that among these are Life, Liberty and the Pursuit of Happiness"") and of the U.S. Constitution (which established a federal republic under the rule of law). Conservative philosophy also derives in part from the classical liberal tradition of the 18th and 19th centuries, which advocated for laissez-faire economics (i.e. economic freedom and deregulation).[15][16]

While historians such as Patrick Allitt (1956–) and political theorists such as Russell Kirk (1918–1994) assert that conservative principles have played a major role in U.S. politics and culture since 1776, they also argue that an organized conservative movement—with beliefs that differ from those of other American political parties—did not emerge in the U.S. until the 1950s.[17][18][19] The recent movement conservatism has its base in the Republican Party, which has adopted conservative policies since the 1950s; Southern Democrats also became important early figures in the movement's history.[20][21][22][23] In 1937, conservative Republicans and Southern Democrats formed the congressional conservative coalition, which played an influential role in Congress from the late 1930s to the mid 1960s. In recent decades Southern conservatives vote heavily Republican."
hypogycemia,"Hypoglycemia, also known as low blood sugar, is a fall in blood sugar to levels below normal.[1] This may result in a variety of symptoms, including clumsiness, trouble talking, confusion, loss of consciousness, seizures, or death.[1] Feelings of hunger, sweating, shakiness, or weakness may also be present.[1] Symptoms typically come on quickly.[1]

The most common cause of hypoglycemia is medications used to treat diabetes such as insulin and sulfonylureas.[2][3] Risk is greater in diabetics who have eaten less than usual, recently exercised,[4] or drunk alcohol.[1] Other causes of hypoglycemia include kidney failure, certain tumors (such as insulinoma), liver disease, hypothyroidism, starvation, inborn error of metabolism, severe infections, reactive hypoglycemia, and a number of drugs, including alcohol.[1][3] Low blood sugar may occur in otherwise healthy babies who have not eaten for a few hours.[5]

The glucose level that defines hypoglycemia is variable.[1] In people with diabetes, levels below 3.9 mmol/l (70 mg/dl) are diagnostic.[1] In adults without diabetes, symptoms related to low blood sugar, low blood sugar at the time of symptoms, and improvement when blood sugar is restored to normal confirm the diagnosis.[6] Otherwise, a level below 2.8 mmol/l (50 mg/dl) after not eating or following exercise may be used.[1] In newborns, a level below 2.2 mmol/l (40 mg/dl), or less than 3.3 mmol/l (60 mg/dl) if symptoms are present, indicates hypoglycemia.[5] Other tests that may be useful in determining the cause include insulin and C peptide levels in the blood.[3]

Among people with diabetes, prevention is by matching the foods eaten with the amount of exercise and the medications used.[1] When people feel their blood sugar is low, testing with a glucose monitor is recommended.[1] Some people have few initial symptoms of low blood sugar, and frequent routine testing in this group is recommended.[1] Treatment of hypoglycemia is by eating foods high in simple sugars or taking dextrose.[1] If a person is not able to take food by mouth, glucagon by injection or in the nose may help.[1][7] The treatment of hypoglycemia unrelated to diabetes includes treating the underlying problem and a healthy diet.[1] The term ""hypoglycemia"" is sometimes incorrectly used to refer to idiopathic postprandial syndrome, a controversial condition with similar symptoms that occurs following eating, but with normal blood sugar levels.[8][9]"
illegal immegration,"Illegal immigration to the United States is the process of migrating into the United States in violation of federal immigration laws. This can include foreign nationals (aliens) who have entered the United States unlawfully,[1][2] as well as those who lawfully entered but then remained after the expiration of their visas, parole, TPS, etc.[3] Illegal immigration has been a matter of intense debate in the United States since the 1980s.

Research shows that illegal immigrants increase the size of the U.S. economy, contribute to economic growth, enhance the welfare of natives, contribute more in tax revenue than they collect, reduce American firms' incentives to offshore jobs and import foreign-produced goods, and benefit consumers by reducing the prices of goods and services.[4][5][6][7] Economists estimate that legalization of the illegal immigrant population would increase the immigrants' earnings and consumption considerably, and increase U.S. gross domestic product.[8][9][10][11]

There is scholarly consensus that illegal immigrants commit less crime than natives.[12][13] Sanctuary cities—which adopt policies designed to avoid prosecuting people solely for being in the country illegally—have no statistically meaningful impact on crime, and may reduce the crime rate.[14][15] Research suggests that immigration enforcement has no impact on crime rates.[16][17][14] Stricter border controls have been linked to increased levels of undocumented immigrants in the United States, as temporary undocumented workers who used to enter the U.S. for seasonal work increasingly settled permanently in the U.S. when regular travels across the border became harder.[18]

The illegal immigrant population of the United States peaked by 2007, when it was at 12.2 million and 4% of the total U.S. population.[19][20] Estimates in 2016 put the number of unauthorized immigrants at 10.7 million, representing 3.3% of the total U.S. population.[19] Since the Great Recession, more illegal immigrants have left the United States than entered it, and illegal border crossings are at the lowest in decades.[21][22][23][24] Since 2007, visa overstays have accounted for a larger share of the growth in the illegal immigrant population than illegal border crossings,[25] which have declined considerably from 2000 to 2018.[26] In 2012, 52% of unauthorized immigrants were from Mexico, 15% from Central America, 12% from Asia, 6% from South America, 5% from the Caribbean, and another 5% from Europe and Canada.[27] As of 2016, approximately two-thirds of unauthorized adult immigrants had lived in the U.S. for at least a decade.[19]"
race-based affirmative actions,"Affirmative action in the United States is a set of laws, policies, guidelines, and administrative practices ""intended to end and correct the effects of a specific form of discrimination""[1] that include government-mandated, government-approved, and voluntary private programs. The programs tend to focus on access to education and employment, granting special consideration to historically excluded groups, specifically racial minorities or women.[1][2] The impetus toward affirmative action is redressing the disadvantages[3][4][5][6][7] associated with past and present discrimination.[8] Further impetus is a desire to ensure public institutions, such as universities, hospitals, and police forces, are more representative of the populations they serve.[9]

In the United States, affirmative action included the use of racial quotas until the Supreme Court ruled that quotas were unconstitutional.[10] Affirmative action currently tends to emphasize not specific quotas but rather ""targeted goals"" to address past discrimination in a particular institution or in broader society through ""good-faith efforts ... to identify, select, and train potentially qualified minorities and women.""[1][11] For example, many higher education institutions have voluntarily adopted policies which seek to increase recruitment of racial minorities.[12] Outreach campaigns, targeted recruitment, employee and management development, and employee support programs are examples of affirmative action in employment.[13] Nine states in the United States have banned affirmative action: California (1996), Washington (1998), Florida (1999), Michigan (2006), Nebraska (2008), Arizona (2010), New Hampshire (2012), Oklahoma (2012), and Idaho (2020). Florida's ban was via an executive order and New Hampshire and Idaho's bans were passed by the legislature. The other six bans were approved at the ballot.[14] The 1996 Hopwood v. Texas decision effectively barred affirmative action in the three states within the United States Court of Appeals for the Fifth Circuit—Louisiana, Mississippi, and Texas—until Grutter v. Bollinger abrogated it in 2003.[15]

Affirmative action policies were developed to address long histories of discrimination faced by minorities and women, which reports suggest produced corresponding unfair advantages for whites and males.[16][17] They first emerged from debates over non-discrimination policies in the 1940s and during the civil rights movement.[18] These debates led to federal executive orders requiring non-discrimination in the employment policies of some government agencies and contractors in the 1940s and onward, and to Title VII of the Civil Rights Act of 1964 which prohibited racial discrimination in firms with over 25 employees. The first federal policy of race-conscious affirmative action was the Revised Philadelphia Plan, implemented in 1969, which required certain government contractors to set ""goals and timetables"" for integrating and diversifying their workforce. Similar policies emerged through a mix of voluntary practices and federal and state policies in employment and education. Affirmative action as a practice was partially upheld by the Supreme Court in Grutter v. Bollinger (2003), while the use of racial quotas for college admissions was concurrently ruled unconstitutional by the Court in Gratz v. Bollinger (2003).

Affirmative action often gives rise to controversy in American politics. Supporters argue that affirmative action is still needed to counteract continuing bias and prejudice against women and minorities. Opponents argue that these policies constitute reverse racism and/or amount to discrimination against other minorities, such as Asian Americans, which entails favoring one group over another based upon racial preference rather than achievement, and many believe that the diversity of current American society suggests that affirmative action policies succeeded and are no longer required.[19] Supporters point to contemporary examples of conscious and unconscious biases, such as the finding that job-seekers with African American sounding names may be less likely to get a callback than those with white-sounding names, as proof that affirmative action is not obsolete.[11]"
open office,"It’s never been easier for workers to collaborate—or so it seems. Open, flexible, activity-based spaces are displacing cubicles, making people more visible. Messaging is displacing phone calls, making people more accessible. Enterprise social media such as Slack and Microsoft Teams are displacing watercooler conversations, making people more connected. Virtual-meeting software such as Zoom, GoToMeeting, and Webex is displacing in-person meetings, making people ever-present. The architecture of collaboration has not changed so quickly since technological advances in lighting and ventilation made tall office buildings feasible, and one could argue that it has never before been so efficient. Designing workplaces for interaction between two or more individuals—or collaboration, from the Latin collaborare, meaning to work together—has never seemed so easy."
isi,"Islamic State (official name since June 2014;[98] abbreviated IS), at times known as the Islamic State of Iraq and the Levant (ISIL; /ˈaɪsəl, ˈaɪsɪl/), and as the Islamic State of Iraq and Syria (ISIS; /ˈaɪsɪs/),[99] or by its Arabic acronym, Daesh ,[100] is a militant Sunni Islamist group and former unrecognized quasi-state[101] that follows a Salafi jihadist doctrine.[102] Islamic State was founded by Abu Musab al-Zarqawi and gained global prominence in 2014 when it drove Iraqi security forces out of key cities in its Western Iraq offensive,[103] followed by its capture of Mosul[104] and the Sinjar massacre.[105]

IS originated in 1999, pledged allegiance to Al-Qaeda and participated in the Iraqi insurgency following the 2003 invasion of Iraq by Western forces. In June 2014, the group proclaimed itself a worldwide caliphate[106][107] and began referring to itself as the Islamic State (الدولة الإسلامية ad-Dawlah al-Islāmiyah; IS).[1] As a caliphate, it claimed religious, political, and military authority over all Muslims worldwide.[108] Its adoption of the name Islamic State and its idea of a caliphate have been criticised, with the United Nations, various governments, and mainstream Muslim groups rejecting its statehood.[109] In Syria, the group conducted ground attacks on both government forces and opposition factions, and by December 2015, it held an area extending from western Iraq to eastern Syria, containing an estimated eight to twelve million people,[54][55][110] where it enforced its interpretation of sharia law. They were estimated at the time to have an annual budget of more than US$1 billion and more than 30,000 fighters.[111]

In mid-2014, an international coalition led by the United States intervened against ISIL in Syria and Iraq with an airstrike campaign, in addition to supplying advisors, weapons, training, and supplies to ISIL's enemies in the Iraqi Security Forces and Syrian Democratic Forces. This campaign reinvigorated the latter two forces and damaged ISIL, killing tens of thousands of its troops[112] and reducing its financial and military infrastructure.[113] This was followed by a smaller-scale Russian intervention exclusively in Syria, in which ISIL lost thousands more fighters to airstrikes, cruise missile attacks, and other Russian military activities and had its financial base further degraded.[114] In July 2017, the group lost control of its largest city, Mosul, to the Iraqi army, followed by the loss of its de facto political capital of Raqqa to the Syrian Democratic Forces.[115] By December 2017, the Islamic State controlled just 2% of its maximum territory (in May 2015).[116] In December 2017, Iraqi forces had driven the last remnants of the Islamic State underground, three years after the group captured about a third of Iraq's territory.[117] By March 2019, ISIL lost one of their last significant territories in the Middle East in the Deir ez-Zor campaign, surrendering their ""tent city"" and pockets in Al-Baghuz Fawqani to the Syrian Democratic Forces after the Battle of Baghuz Fawqani.[30]

The group has been designated as a terrorist organisation by the United Nations. The IS is known for its videos of beheadings and other types of executions[118] of both soldiers and civilians, including journalists and aid workers, and its destruction of cultural heritage sites.[119] The United Nations holds ISIL responsible for committing human rights abuses, genocide, war crimes, and crimes against humanity.[120] The Islamic State committed genocide and ethnic cleansing on a historic scale in northern Iraq.[121][122] In October 2019, IS media announced that Abu Ibrahim al-Hashimi al-Qurashi was the new leader of the Islamic State,[123] after Abu Bakr al-Baghdadi killed himself by detonating a suicide vest during the US Barisha raid in Syria.[124][125][126] The Islamic State has also had a presence outside the Middle East through its affiliates and various ""provinces"",[127] and has been militarily active in countries including notably in Nigeria, Afghanistan and the Philippines.[128]"
leaf,"The Nissan Leaf, styled as LEAF, is a compact five-door hatchback battery electric vehicle (BEV) manufactured by Nissan. It was introduced in Japan and the United States in December 2010 and is currently in its second generation, introduced in October 2017.

The Leaf's range on a full charge has been increased gradually from 117 km (73 miles) to 364 km (226 miles) (EPA rated), due to the use of a larger battery pack along with several minor improvements.[2]

Among other awards and recognition, the Leaf has won the 2010 Green Car Vision Award, the 2011 European Car of the Year, the 2011 World Car of the Year, and the 2011–2012 Car of the Year Japan.

Global sales totaled 500,000 Leafs by December 2020.[3] As of September 2021, European sales totaled more than 208,000 units,[4] U.S. sales over 161,000 units,[5] and more than 150,000 in Japan.[6]

The Leaf listed as the world's all-time top selling plug-in electric car through December 2019. The Tesla Model 3 surpassed the Leaf in early 2020 to become the all-time best selling electric car.[7][8]"
capitial gains,"Capital gain is an economic concept defined as the profit earned on the sale of an asset which has increased in value over the holding period. An asset may include tangible property, a car, a business, or intangible property such as shares.

A capital gain is only possible when the selling price of the asset is greater than the original purchase price. In the event that the purchase price exceeds the sale price, a capital loss occurs. Capital gains are often subject to taxation, of which rates and exemptions may differ between countries. The history of capital gain originates at the birth of the modern economic system and its evolution has been described as complex and multidimensional by a variety of economic thinkers. The concept of capital gain may be considered comparable with other key economic concepts such as profit and rate of return, however its distinguishing feature is that individuals, not just businesses, can accrue capital gains through everyday acquisition and disposal of assets."
bankcharges,"The term bank charge covers all charges and fees made by a bank to their customers. In common parlance, the term often relates to charges in respect of personal current accounts or checking account. These charges may take many forms, including:

monthly charges for the provision of an account
charges for specific transactions (other than overdraft limit excesses)
interest in respect of overdrafts (whether authorised or unauthorised by the bank)
charges for exceeding authorised overdraft limits, or making payments (or attempting to make payments) where no authorised overdraft exists.
Much of the following discussion relates to the UK personal current account market."
democracies,"Democracy (Greek: δημοκρατία, dēmokratiā, from dēmos 'people' and kratos 'rule'[1]) is a form of government in which the people have the authority to deliberate and decide legislation (""direct democracy""), or to choose governing officials to do so (""representative democracy""). Who is considered part of ""the people"" and how authority is shared among or delegated by the people has changed over time and at different rates in different countries, but over time more and more of a democratic country's inhabitants have generally been included. Cornerstones of democracy include freedom of assembly, association and speech, inclusiveness and equality, citizenship, consent of the governed, voting rights, freedom from unwarranted governmental deprivation of the right to life and liberty, and minority rights.

The notion of democracy has evolved over time considerably. The original form of democracy was a direct democracy. The most common form of democracy today is a representative democracy, where the people elect government officials to govern on their behalf such as in a parliamentary or presidential democracy.[2]

Prevalent day-to-day decision making of democracies is the majority rule,[3][4] though other decision making approaches like supermajority and consensus have also been integral to democracies. They serve the crucial purpose of inclusiveness and broader legitimacy on sensitive issues—counterbalancing majoritarianism—and therefore mostly take precedence on a constitutional level. In the common variant of liberal democracy, the powers of the majority are exercised within the framework of a representative democracy, but the constitution limits the majority and protects the minority—usually through the enjoyment by all of certain individual rights, e.g. freedom of speech or freedom of association.[5][6]

The term appeared in the 5th century BC to denote the political systems then existing in Greek city-states, notably Classical Athens, to mean ""rule of the people"", in contrast to aristocracy (ἀριστοκρατία, aristokratía), meaning ""rule of an elite"".[7] Western democracy, as distinct from that which existed in antiquity, is generally considered to have originated in city-states such as those in Classical Athens and the Roman Republic, where various schemes and degrees of enfranchisement of the free male population were observed before the form disappeared in the West at the beginning of late antiquity. In virtually all democratic governments throughout ancient and modern history, democratic citizenship consisted of an elite class until full enfranchisement was won for all adult citizens in most modern democracies through the suffrage movements of the 19th and 20th centuries.

Democracy contrasts with forms of government where power is either held by an individual, as in autocratic systems like absolute monarchy, or where power is held by a small number of individuals, as in an oligarchy—oppositions inherited from ancient Greek philosophy.[8] Karl Popper defined democracy in contrast to dictatorship or tyranny, focusing on opportunities for the people to control their leaders and to oust them without the need for a revolution.[9]"
vacvines,"A vaccine is a biological preparation that provides active acquired immunity to a particular infectious disease.[1] A vaccine typically contains an agent that resembles a disease-causing microorganism and is often made from weakened or killed forms of the microbe, its toxins, or one of its surface proteins. The agent stimulates the body's immune system to recognize the agent as a threat, destroy it, and to further recognize and destroy any of the microorganisms associated with that agent that it may encounter in the future. Vaccines can be prophylactic (to prevent or ameliorate the effects of a future infection by a natural or ""wild"" pathogen), or therapeutic (to fight a disease that has already occurred, such as cancer).[2][3][4][5] Some vaccines offer full sterilizing immunity, in which infection is prevented completely.[6]

The administration of vaccines is called vaccination. Vaccination is the most effective method of preventing infectious diseases;[7] widespread immunity due to vaccination is largely responsible for the worldwide eradication of smallpox and the restriction of diseases such as polio, measles, and tetanus from much of the world. The effectiveness of vaccination has been widely studied and verified;[8] for example, vaccines that have proven effective include the influenza vaccine,[9] the HPV vaccine,[10] and the chickenpox vaccine.[11] The World Health Organization (WHO) reports that licensed vaccines are currently available for twenty-five different preventable infections.[12]

The terms vaccine and vaccination are derived from Variolae vaccinae (smallpox of the cow), the term devised by Edward Jenner (who both developed the concept of vaccines and created the first vaccine) to denote cowpox. He used the phrase in 1798 for the long title of his Inquiry into the Variolae vaccinae Known as the Cow Pox, in which he described the protective effect of cowpox against smallpox.[13] In 1881, to honor Jenner, Louis Pasteur proposed that the terms should be extended to cover the new protective inoculations then being developed.[14] The science of vaccine development and production is termed vaccinology."
respect and dignity,"Dignity: From the Latin word dignitas, meaning “to be worthy.”
As in: All people have the right to be recognized for their inherent humanity and treated ethically. Dignity is a given. You just have it and no one can take it away.
Respect: From the Latin word respectus, meaning “to look back at.”
As in: showing admiration for someone because of their abilities, qualities or achievements. Respect is earned. You are respected by others for what you have achieved, experienced and how you have handled yourself as you have achieved accomplishments."
environmental salvaguards,"he term “Environmental and Social Safeguards (or Standards)” is used by development institutions, international treaties and agencies to refer to policies, standards and operational procedures designed to first identify and then try to avoid, mitigate and minimize adverse environmental and social impacts that may arise in the implementation of development projects. ESS also have a pro-active dimension to try to increase chances that development projects deliver better outcomes for people and the environment."
icann,"The Internet Corporation for Assigned Names and Numbers (ICANN /ˈaɪkæn/ EYE-kan) is an American multistakeholder group and nonprofit organization responsible for coordinating the maintenance and procedures of several databases related to the namespaces and numerical spaces of the Internet, ensuring the network's stable and secure operation.[2] ICANN performs the actual technical maintenance work of the Central Internet Address pools and DNS root zone registries pursuant to the Internet Assigned Numbers Authority (IANA) function contract. The contract regarding the IANA stewardship functions between ICANN and the National Telecommunications and Information Administration (NTIA) of the United States Department of Commerce ended on October 1, 2016, formally transitioning the functions to the global multistakeholder community.[3][4][5][6]

Much of its work has concerned the Internet's global Domain Name System (DNS), including policy development for internationalization of the DNS system, introduction of new generic top-level domains (TLDs), and the operation of root name servers. The numbering facilities ICANN manages include the Internet Protocol address spaces for IPv4 and IPv6, and assignment of address blocks to regional Internet registries. ICANN also maintains registries of Internet Protocol identifiers.

ICANN's primary principles of operation have been described as helping preserve the operational stability of the Internet; to promote competition; to achieve broad representation of the global Internet community; and to develop policies appropriate to its mission through bottom-up, consensus-based processes.[7]

ICANN's creation was announced publicly on September 17, 1998,[8] and it formally came into being on September 30, 1998, incorporated in the U.S. state of California.[9] Originally headquartered in Marina del Rey in the same building as the University of Southern California's Information Sciences Institute (ISI), its offices are now in the Playa Vista neighborhood of Los Angeles."
opinion,"An opinion is a judgement, viewpoint, or statement that is not conclusive, rather than facts, which are true statements."
walmarts,"Walmart Inc. ( /ˈwɔːlmɑːrt/; formerly Wal-Mart Stores, Inc.) is an American multinational retail corporation that operates a chain of hypermarkets (also called supercenters), discount department stores, and grocery stores from the United States, headquartered in Bentonville, Arkansas.[9] The company was founded by Sam Walton in nearby Rogers, Arkansas in 1962 and incorporated under Delaware General Corporation Law on October 31, 1969. It also owns and operates Sam's Club retail warehouses.[10][11] As of July 31, 2021, Walmart has 10,524 stores and clubs in 24 countries, operating under 48 different names.[2][3][12] The company operates under the name Walmart in the United States and Canada, as Walmart de México y Centroamérica in Mexico and Central America, and as Flipkart Wholesale in India. It has wholly owned operations in Chile, Canada, and South Africa. Since August 2018, Walmart holds only a minority stake in Walmart Brasil, which was renamed Grupo Big in August 2019, with 20 percent of the company's shares, and private equity firm Advent International holding 80 percent ownership of the company.

Walmart is the world's largest company by revenue, with US$548.743 billion, according to the Fortune Global 500 list in 2020. It is also the largest private employer in the world with 2.2 million employees. It is a publicly traded family-owned business, as the company is controlled by the Walton family. Sam Walton's heirs own over 50 percent of Walmart through both their holding company Walton Enterprises and their individual holdings.[13] Walmart was the largest United States grocery retailer in 2019, and 65 percent of Walmart's US$510.329 billion sales came from U.S. operations.[14][15]

Walmart was listed on the New York Stock Exchange in 1972. By 1988, it was the most profitable retailer in the U.S.,[16] and it had become the largest in terms of revenue by October 1989.[17] The company was originally geographically limited to the South and lower Midwest, but it had stores from coast to coast by the early 1990s. Sam's Club opened in New Jersey in November 1989, and the first California outlet opened in Lancaster, in July 1990. A Walmart in York, Pennsylvania, opened in October 1990, the first main store in the Northeast.[18]

Walmart's investments outside the U.S. have seen mixed results. Its operations and subsidiaries in Canada,[19] the United Kingdom,[20] Central America, South America, and China are successful, but its ventures failed in Germany, Japan, and South Korea."
wal mart's unfair competition,"Walmart Inc. ( /ˈwɔːlmɑːrt/; formerly Wal-Mart Stores, Inc.) is an American multinational retail corporation that operates a chain of hypermarkets (also called supercenters), discount department stores, and grocery stores from the United States, headquartered in Bentonville, Arkansas.[9] The company was founded by Sam Walton in nearby Rogers, Arkansas in 1962 and incorporated under Delaware General Corporation Law on October 31, 1969. It also owns and operates Sam's Club retail warehouses.[10][11] As of July 31, 2021, Walmart has 10,524 stores and clubs in 24 countries, operating under 48 different names.[2][3][12] The company operates under the name Walmart in the United States and Canada, as Walmart de México y Centroamérica in Mexico and Central America, and as Flipkart Wholesale in India. It has wholly owned operations in Chile, Canada, and South Africa. Since August 2018, Walmart holds only a minority stake in Walmart Brasil, which was renamed Grupo Big in August 2019, with 20 percent of the company's shares, and private equity firm Advent International holding 80 percent ownership of the company.

Walmart is the world's largest company by revenue, with US$548.743 billion, according to the Fortune Global 500 list in 2020. It is also the largest private employer in the world with 2.2 million employees. It is a publicly traded family-owned business, as the company is controlled by the Walton family. Sam Walton's heirs own over 50 percent of Walmart through both their holding company Walton Enterprises and their individual holdings.[13] Walmart was the largest United States grocery retailer in 2019, and 65 percent of Walmart's US$510.329 billion sales came from U.S. operations.[14][15]

Walmart was listed on the New York Stock Exchange in 1972. By 1988, it was the most profitable retailer in the U.S.,[16] and it had become the largest in terms of revenue by October 1989.[17] The company was originally geographically limited to the South and lower Midwest, but it had stores from coast to coast by the early 1990s. Sam's Club opened in New Jersey in November 1989, and the first California outlet opened in Lancaster, in July 1990. A Walmart in York, Pennsylvania, opened in October 1990, the first main store in the Northeast.[18]

Walmart's investments outside the U.S. have seen mixed results. Its operations and subsidiaries in Canada,[19] the United Kingdom,[20] Central America, South America, and China are successful, but its ventures failed in Germany, Japan, and South Korea."
wal mart,"Walmart Inc. ( /ˈwɔːlmɑːrt/; formerly Wal-Mart Stores, Inc.) is an American multinational retail corporation that operates a chain of hypermarkets (also called supercenters), discount department stores, and grocery stores from the United States, headquartered in Bentonville, Arkansas.[9] The company was founded by Sam Walton in nearby Rogers, Arkansas in 1962 and incorporated under Delaware General Corporation Law on October 31, 1969. It also owns and operates Sam's Club retail warehouses.[10][11] As of July 31, 2021, Walmart has 10,524 stores and clubs in 24 countries, operating under 48 different names.[2][3][12] The company operates under the name Walmart in the United States and Canada, as Walmart de México y Centroamérica in Mexico and Central America, and as Flipkart Wholesale in India. It has wholly owned operations in Chile, Canada, and South Africa. Since August 2018, Walmart holds only a minority stake in Walmart Brasil, which was renamed Grupo Big in August 2019, with 20 percent of the company's shares, and private equity firm Advent International holding 80 percent ownership of the company.

Walmart is the world's largest company by revenue, with US$548.743 billion, according to the Fortune Global 500 list in 2020. It is also the largest private employer in the world with 2.2 million employees. It is a publicly traded family-owned business, as the company is controlled by the Walton family. Sam Walton's heirs own over 50 percent of Walmart through both their holding company Walton Enterprises and their individual holdings.[13] Walmart was the largest United States grocery retailer in 2019, and 65 percent of Walmart's US$510.329 billion sales came from U.S. operations.[14][15]

Walmart was listed on the New York Stock Exchange in 1972. By 1988, it was the most profitable retailer in the U.S.,[16] and it had become the largest in terms of revenue by October 1989.[17] The company was originally geographically limited to the South and lower Midwest, but it had stores from coast to coast by the early 1990s. Sam's Club opened in New Jersey in November 1989, and the first California outlet opened in Lancaster, in July 1990. A Walmart in York, Pennsylvania, opened in October 1990, the first main store in the Northeast.[18]

Walmart's investments outside the U.S. have seen mixed results. Its operations and subsidiaries in Canada,[19] the United Kingdom,[20] Central America, South America, and China are successful, but its ventures failed in Germany, Japan, and South Korea."
corporate responsibilty,"Corporate social responsibility (CSR) is a self-regulating business model that helps a company be socially accountable—to itself, its stakeholders, and the public. By practicing corporate social responsibility, also called corporate citizenship, companies can be conscious of the kind of impact they are having on all aspects of society, including economic, social, and environmental.

To engage in CSR means that, in the ordinary course of business, a company is operating in ways that enhance society and the environment, instead of contributing negatively to them."
immugrayion,"Immigration to the United States is the international movement of non-United States nationals to reside permanently in the country.[1] Immigration has been a major source of population growth and cultural change throughout much of the United States. history. All Americans, except for Native Americans, can trace their ancestry to immigrants from other nations around the world.

In absolute numbers, the United States has a larger immigrant population than any other country, with 47 million immigrants as of 2015.[2] This represents 19.1% of the 244 million international migrants worldwide, and 14.4% of the United States population. Some other countries have larger proportions of immigrants, such as Australia with 30%[3] and Canada with 21.9%.[4]

According to the 2016 Yearbook of Immigration Statistics, the United States admitted a total of 1.18 million legal immigrants (618k new arrivals, 565k status adjustments) in 2016.[5] Of these, 48% were the immediate relatives of United States citizens, 20% were family-sponsored, 13% were refugees or asylum seekers, 12% were employment-based preferences, 4.2% were part of the Diversity Immigrant Visa program, 1.4% were victims of a crime (U1) or their family members were (U2 to U5),[6] and 1.0% who were granted the Special Immigrant Visa (SIV) for Iraqis and Afghans employed by the United States Government.[5] The remaining 0.4% included small numbers from several other categories, including 0.2% who were granted suspension of deportation as an immediate relative of a citizen (Z13);[7] persons admitted under the Nicaraguan and Central American Relief Act; children born after the issuance of a parent's visa; and certain parolees from the former Soviet Union, Cambodia, Laos, and Vietnam who were denied refugee status.[5]

The economic, social, and political aspects of immigration have caused controversy regarding such issues as maintaining ethnic homogeneity, workers for employers versus jobs for non-immigrants, settlement patterns, impact on upward social mobility, crime, and voting behavior.

Between 1921 and 1965, policies such as the national origins formula limited immigration and naturalization opportunities for people from areas outside Western Europe. Exclusion laws enacted as early as the 1880s generally prohibited or severely restricted immigration from Asia, and quota laws enacted in the 1920s curtailed Eastern European immigration. The civil rights movement led to the replacement[8] of these ethnic quotas with per-country limits for family-sponsored and employment-based preference visas.[9] Since then, the number of first-generation immigrants living in the United States has quadrupled.[10][11]

Research suggests that immigration to the United States is beneficial to the United States economy. With few exceptions, the evidence suggests that on average, immigration has positive economic effects on the native population, but it is mixed as to whether low-skilled immigration adversely affects low-skilled natives. Studies also show that immigrants have lower crime rates than natives in the United States.[12][13][14]"
service jobs,"A career in public service means you can pick your profession and go to work for a government or non-profit entity. Although public service professions may require a degree in a specific field, such as accounting, nursing or law enforcement, there are often entry-level positions that require minimal formal education. Public service professions can include a wide range of specific job duties. Everything from wildlife management to health maintenance of elderly and children can be found in the career path of those in public service professions."
iraq war,"The Iraq War[nb 1] was a protracted armed conflict from 2003 to 2011 that began with the invasion of Iraq by the United States–led coalition which overthrew the Iraqi government of Saddam Hussein. The conflict continued for much of the next decade as an insurgency emerged to oppose the coalition forces and the post-invasion Iraqi government.[56] An estimated 151,000 to 1,033,000 Iraqis died in the first three to five years of conflict. US troops were officially withdrawn in 2011. The United States became re-involved in 2014 at the head of a new coalition; the insurgency and many dimensions of the armed conflict continue. The invasion occurred as part of the George W. Bush administration's War on Terror following the September 11 attacks despite no connection of the latter to Iraq.[57]

In October 2002, Congress granted President Bush the power to decide whether to launch any military attack in Iraq.[58] The Iraq War began on 20 March 2003,[59] when the US, joined by the UK, Australia, and Poland launched a ""shock and awe"" bombing campaign. Iraqi forces were quickly overwhelmed as coalition forces swept through the country. The invasion led to the collapse of the Ba'athist government; Saddam Hussein was captured during Operation Red Dawn in December of that same year and executed three years later. The power vacuum following Saddam's demise and mismanagement by the Coalition Provisional Authority led to widespread civil war between Shias and Sunnis, as well as a lengthy insurgency against coalition forces. Many of the violent insurgent groups were supported by Iran and al-Qaeda in Iraq. The United States responded with a build-up of 170,000 troops in 2007.[60] This build-up gave greater control to Iraq's government and military, and was judged a success by many.[61] In 2008, President Bush agreed to a withdrawal of all US combat troops from Iraq. The withdrawal was completed under President Barack Obama in December 2011.[62][63]

The Bush administration based its rationale for the Iraq War on the claim that Iraq had a weapons of mass destruction (WMD) program,[64] and that Iraq posed a threat to the United States and its allies.[65][66] Some US officials falsely accused Saddam of harbouring and supporting al-Qaeda.[67] In 2004, the 9/11 Commission concluded there was no evidence of an relationship between the Saddam Hussein regime and al-Qaeda.[68] No stockpiles of WMDs or an active WMD program were ever found in Iraq.[69] Bush administration officials made numerous claims about a purported Saddam–al-Qaeda relationship and WMDs that were based on sketchy evidence rejected by intelligence officials.[69][70] The rationale for war faced heavy criticism both domestically and internationally.[71] Kofi Annan called the invasion illegal under international law as it violated the UN Charter.[72] The Chilcot Report, a British inquiry into its decision to go to war, was published in 2016 and concluded peaceful alternatives to war had not been exhausted, that the United Kingdom and the United States had undermined the authority of the United Nations Security Council, that the process of identifying the legal basis was ""far from satisfactory"", and that the war was unnecessary.[73][74][75] When interrogated by the FBI, Saddam Hussein confirmed that Iraq did not have weapons of mass destruction prior to the US invasion.[76]

In the aftermath of the invasion, Iraq held multi-party elections in 2005. Nouri al-Maliki became Prime Minister in 2006 and remained in office until 2014. The al-Maliki government enacted policies that alienated the country's previously dominant Sunni minority and worsened sectarian tensions. In the summer of 2014, ISIL launched a military offensive in northern Iraq and declared a worldwide Islamic caliphate, leading to Operation Inherent Resolve, another military response from the United States and its allies. According to a 2019 US Army study, Iran has emerged as ""the only victor"" of the war.[14]

The Iraq War caused at least one hundred thousand civilian deaths, as well as tens of thousands of military deaths (see estimates below). The majority of deaths occurred as a result of the insurgency and civil conflicts between 2004 and 2007. Subsequently, the War in Iraq of 2013 to 2017, which is considered a domino effect of the invasion and occupation, caused at least 155,000 deaths, in addition to the displacement of more than 3.3 million people within the country.[77][78][79]"
arab control of jerusalem,"The siege of Jerusalem (636–637) was part of the Muslim conquest of the Levant and the result of the military efforts of the Rashidun Caliphate against the Byzantine Empire in the year 636–637/38. It began when the Rashidun army, under the command of Abu Ubaidah, besieged Jerusalem beginning in November 636. After six months, the Patriarch Sophronius agreed to surrender, on condition that he submit only to the Caliph. According to tradition, in 637 or 638, Caliph Umar traveled to Jerusalem in person to receive the submission of the city. The Patriarch thus surrendered to him.

The Muslim conquest of the city solidified Arab control over Palestine, which would not again be threatened until the First Crusade in 1099."
bank fees,"The term bank charge covers all charges and fees made by a bank to their customers. In common parlance, the term often relates to charges in respect of personal current accounts or checking account. These charges may take many forms, including:

monthly charges for the provision of an account
charges for specific transactions (other than overdraft limit excesses)
interest in respect of overdrafts (whether authorised or unauthorised by the bank)
charges for exceeding authorised overdraft limits, or making payments (or attempting to make payments) where no authorised overdraft exists.
Much of the following discussion relates to the UK personal current account market."
ego,the ethical theory that achieving one's own happiness is the proper goal of all conduct
"ego and hedonism,",the ethical theory that achieving one's own happiness is the proper goal of all conduct
free marlet,"In economics, a free market is a system in which the prices for goods and services are self-regulated by buyers and sellers negotiating in an open market. In a free market, the laws and forces of supply and demand are free from any intervention by a government or other authority, and from all forms of economic privilege, monopolies and artificial scarcities.[1] Proponents of the concept of free market contrast it with a regulated market in which a government intervenes in supply and demand through various methods such as tariffs used to restrict trade and to protect the local economy. In an idealized free-market economy, also called a liberal market economy, prices for goods and services are set freely by the forces of supply and demand and are allowed to reach their point of equilibrium without intervention by government policy.

Scholars contrast the concept of a free market with the concept of a coordinated market in fields of study such as political economy, new institutional economics, economic sociology and political science. All of these fields emphasize the importance in currently existing market systems of rule-making institutions external to the simple forces of supply and demand which create space for those forces to operate to control productive output and distribution. Although free markets are commonly associated with capitalism in contemporary usage and popular culture, free markets have also been components in some forms of socialism.[2]

Criticism of the theoretical concept may regard systems with significant market power, inequality of bargaining power, or information asymmetry as less than free, with regulation being necessary to control those imbalances in order to allow markets to function more efficiently as well as produce more desirable social outcomes."
improper use of supplements,improper use of supplements
nyc,"New York, often called New York City to distinguish it from New York State, or NYC for short, is the most populous city in the United States. With a 2020 population of 8,804,190 distributed over 300.46 square miles (778.2 km2), New York City is also the most densely populated major city in the United States. Located at the southern tip of the State of New York, the city is the center of the New York metropolitan area, the largest metropolitan area in the world by urban area.[9] With over 20 million people in its metropolitan statistical area and approximately 23 million in its combined statistical area, it is one of the world's most populous megacities. New York City has been described as the cultural, financial, and media capital of the world, significantly influencing commerce, entertainment, research, technology, education, politics, tourism, art, fashion, and sports, and is the most photographed city in the world.[10] Home to the headquarters of the United Nations, New York is an important center for international diplomacy,[11][12] and has sometimes been called the capital of the world.[13][14]

Situated on one of the world's largest natural harbors, New York City is composed of five boroughs, each of which is coextensive with a respective county of the State of New York. The five boroughs—Brooklyn (Kings County), Queens (Queens County), Manhattan (New York County), the Bronx (Bronx County), and Staten Island (Richmond County)—were created when local governments were consolidated into a single municipal entity in 1898.[15] The city and its metropolitan area constitute the premier gateway for legal immigration to the United States. As many as 800 languages are spoken in New York,[16] making it the most linguistically diverse city in the world. New York is home to more than 3.2 million residents born outside the United States, the largest foreign-born population of any city in the world as of 2016.[17][18] As of 2019, the New York metropolitan area is estimated to produce a gross metropolitan product (GMP) of $2.0 trillion. If the New York metropolitan area were a sovereign state, it would have the eighth-largest economy in the world. New York is home to the highest number of billionaires of any city in the world.[19]

New York City traces its origins to a trading post founded on the southern tip of Manhattan Island by Dutch colonists in approximately 1624. The settlement was named New Amsterdam (Dutch: Nieuw Amsterdam) in 1626 and was chartered as a city in 1653. The city came under English control in 1664 and was renamed New York after King Charles II of England granted the lands to his brother, the Duke of York.[20][21] The city was regained by the Dutch in July 1673 and was renamed New Orange for one year and three months; the city has been continuously named New York since November 1674. New York City was the capital of the United States from 1785 until 1790,[22] and has been the largest U.S. city since 1790. The Statue of Liberty greeted millions of immigrants as they came to the U.S. by ship in the late 19th and early 20th centuries, and is a symbol of the U.S. and its ideals of liberty and peace.[23] In the 21st century, New York has emerged as a global node of creativity, entrepreneurship,[24] and environmental sustainability,[25][26] and as a symbol of freedom and cultural diversity.[27] In 2019, New York was voted the greatest city in the world per a survey of over 30,000 people from 48 cities worldwide, citing its cultural diversity.[28]

Many districts and monuments in New York City are major landmarks, including three of the world's ten most visited tourist attractions in 2013.[29] A record 66.6 million tourists visited New York City in 2019. Times Square is the brightly illuminated hub of the Broadway Theater District,[30] one of the world's busiest pedestrian intersections,[29][31] and a major center of the world's entertainment industry.[32] Many of the city's landmarks, skyscrapers, and parks are known around the world. The Empire State Building has become the global standard of reference to describe the height and length of other structures.[33][34][35] Manhattan's real estate market is among the most expensive in the world.[36][37] Providing continuous 24/7 service and contributing to the nickname The City That Never Sleeps, the New York City Subway is the largest single-operator rapid transit system worldwide, with 472 rail stations. The city has over 120 colleges and universities, including Columbia University, New York University, Rockefeller University, and the City University of New York system, which is the largest urban public university system in the United States. Anchored by Wall Street in the Financial District of Lower Manhattan, New York City has been called both the world's leading financial center and the most financially powerful city in the world, and is home to the world's two largest stock exchanges by total market capitalization, the New York Stock Exchange and NASDAQ.[38][39]"
daesh and al queda,"Islamic State (official name since June 2014;[98] abbreviated IS), at times known as the Islamic State of Iraq and the Levant (ISIL; /ˈaɪsəl, ˈaɪsɪl/), and as the Islamic State of Iraq and Syria (ISIS; /ˈaɪsɪs/),[99] or by its Arabic acronym, Daesh (داعش, Dāʿish, IPA: [ˈdaːʕɪʃ]),[100] is a militant Sunni Islamist group and former unrecognized quasi-state[101] that follows a Salafi jihadist doctrine.[102] Islamic State was founded by Abu Musab al-Zarqawi and gained global prominence in 2014 when it drove Iraqi security forces out of key cities in its Western Iraq offensive,[103] followed by its capture of Mosul[104] and the Sinjar massacre.[105]

IS originated in 1999, pledged allegiance to Al-Qaeda and participated in the Iraqi insurgency following the 2003 invasion of Iraq by Western forces. In June 2014, the group proclaimed itself a worldwide caliphate[106][107] and began referring to itself as the Islamic State (الدولة الإسلامية ad-Dawlah al-Islāmiyah; IS).[1] As a caliphate, it claimed religious, political, and military authority over all Muslims worldwide.[108] Its adoption of the name Islamic State and its idea of a caliphate have been criticised, with the United Nations, various governments, and mainstream Muslim groups rejecting its statehood.[109] In Syria, the group conducted ground attacks on both government forces and opposition factions, and by December 2015, it held an area extending from western Iraq to eastern Syria, containing an estimated eight to twelve million people,[54][55][110] where it enforced its interpretation of sharia law. They were estimated at the time to have an annual budget of more than US$1 billion and more than 30,000 fighters.[111]

In mid-2014, an international coalition led by the United States intervened against ISIL in Syria and Iraq with an airstrike campaign, in addition to supplying advisors, weapons, training, and supplies to ISIL's enemies in the Iraqi Security Forces and Syrian Democratic Forces. This campaign reinvigorated the latter two forces and damaged ISIL, killing tens of thousands of its troops[112] and reducing its financial and military infrastructure.[113] This was followed by a smaller-scale Russian intervention exclusively in Syria, in which ISIL lost thousands more fighters to airstrikes, cruise missile attacks, and other Russian military activities and had its financial base further degraded.[114] In July 2017, the group lost control of its largest city, Mosul, to the Iraqi army, followed by the loss of its de facto political capital of Raqqa to the Syrian Democratic Forces.[115] By December 2017, the Islamic State controlled just 2% of its maximum territory (in May 2015).[116] In December 2017, Iraqi forces had driven the last remnants of the Islamic State underground, three years after the group captured about a third of Iraq's territory.[117] By March 2019, ISIL lost one of their last significant territories in the Middle East in the Deir ez-Zor campaign, surrendering their ""tent city"" and pockets in Al-Baghuz Fawqani to the Syrian Democratic Forces after the Battle of Baghuz Fawqani.[30]

The group has been designated as a terrorist organisation by the United Nations. The IS is known for its videos of beheadings and other types of executions[118] of both soldiers and civilians, including journalists and aid workers, and its destruction of cultural heritage sites.[119] The United Nations holds ISIL responsible for committing human rights abuses, genocide, war crimes, and crimes against humanity.[120] The Islamic State committed genocide and ethnic cleansing on a historic scale in northern Iraq.[121][122] In October 2019, IS media announced that Abu Ibrahim al-Hashimi al-Qurashi was the new leader of the Islamic State,[123] after Abu Bakr al-Baghdadi killed himself by detonating a suicide vest during the US Barisha raid in Syria.[124][125][126] The Islamic State has also had a presence outside the Middle East through its affiliates and various ""provinces"",[127] and has been militarily active in countries including notably in Nigeria, Afghanistan and the Philippines.[128]"
intelligence methods used to identify potential terorrists,"The Bush administration has begun to revise cold war rules governing national security information in order to counter terrorist threats to the United States. The president’s homeland security plan calls for new intelligence efforts to protect the nation’s borders, defend against threats within the United States, minimize infrastructure vulnerabilities, and improve emergency responses. Congress has given the new Department of Homeland Security responsibility for coordinating these strategies and assuring that accurate and complete information gets to those who need it.

Policymakers must go further to build a new intelligence system to support transformed national security needs. Threats involving unknown perpetrators, methods, and targets cannot be countered with strategies designed for use by federal officials to combat more predictable adversaries. Today, state and local law enforcement, public health, and emergency response personnel are on the front lines of detecting and responding to terrorist threats; corporate managers are responsible for securing key infrastructure such as energy supplies, chemical plants, and telecommunications; and workers and neighborhood residents may hold information that can help prevent attacks."
lawyer,"A lawyer or attorney is a person who practices law, as an advocate, attorney at law, barrister, barrister-at-law, bar-at-law, canonist, canon lawyer, civil law notary, counsel, counselor, solicitor, legal executive, or public servant preparing, interpreting and applying the law, but not as a paralegal or charter executive secretary.[1] Working as a lawyer involves the practical application of abstract legal theories and knowledge to solve specific individualized problems, or to advance the interests of those who hire lawyers to perform legal services. The role of the lawyer varies greatly across different legal jurisdictions.[2][3]"
nuclear deals,"The Joint Comprehensive Plan of Action (JCPOA; Persian: برنامه جامع اقدام مشترک‎, romanized: barnāmeye jāme'e eqdāme moshtarak (برجام, BARJAM)),[4][5] known commonly as the Iran nuclear deal or Iran deal, is an agreement on the Iranian nuclear program reached in Vienna on 14 July 2015, between Iran and the P5+1 (the five permanent members of the United Nations Security Council—China, France, Russia, United Kingdom, United States—plus Germany)[a] together with the European Union.

Formal negotiations toward JCPOA began with the adoption of the Joint Plan of Action, an interim agreement signed between Iran and the P5+1 countries in November 2013. Iran and the P5+1 countries engaged in negotiations for the next 20 months and, in April 2015, agreed on a framework for the final agreement. In July 2015, Iran and the P5+1 confirmed agreement on the plan, along with the ""Roadmap Agreement"" between Iran and the IAEA.[8]"
the nuclear deal,"The Joint Comprehensive Plan of Action (JCPOA; Persian: برنامه جامع اقدام مشترک‎, romanized: barnāmeye jāme'e eqdāme moshtarak (برجام, BARJAM)),[4][5] known commonly as the Iran nuclear deal or Iran deal, is an agreement on the Iranian nuclear program reached in Vienna on 14 July 2015, between Iran and the P5+1 (the five permanent members of the United Nations Security Council—China, France, Russia, United Kingdom, United States—plus Germany)[a] together with the European Union.

Formal negotiations toward JCPOA began with the adoption of the Joint Plan of Action, an interim agreement signed between Iran and the P5+1 countries in November 2013. Iran and the P5+1 countries engaged in negotiations for the next 20 months and, in April 2015, agreed on a framework for the final agreement. In July 2015, Iran and the P5+1 confirmed agreement on the plan, along with the ""Roadmap Agreement"" between Iran and the IAEA.[8]"
domestic airline competition,"ir transport has radically evolved over the last two decades. Liberalisation and deregulation of the sector have facilitated the entry of new firms, which in turn has had a positive impact on competition and innovation. Deregulation and liberalisation also significantly altered marked structure, giving rise to mergers of flag airline carriers and diverse forms of collaboration. 

However, in some cases, collaboration can actually be anti-competitive. It is essential to ensure that former regulatory barriers are not replaced by anti-competitive airline mergers, alliances, agreements and unilateral practices.

In June 2014, the OECD Competition Committee held a discussion on Airline Competition to examine the main competition issues in the sector and how competition enforcement authorities have been dealing with them. "
independence declartions,"The United States Declaration of Independence[a] is the pronouncement adopted by the Second Continental Congress meeting in Philadelphia, Pennsylvania, on July 4, 1776. The Declaration explains why the Thirteen Colonies at war with the Kingdom of Great Britain regarded themselves as thirteen independent sovereign states, no longer under British rule. With the Declaration, these new states took a collective first step toward forming the United States of America. The declaration was signed by representatives from New Hampshire, Massachusetts Bay, Rhode Island, Connecticut, New York, New Jersey, Pennsylvania, Maryland, Delaware, Virginia, North Carolina, South Carolina, and Georgia.

The Lee Resolution for independence was passed by the Second Continental Congress on July 2 with no opposing votes. The Committee of Five had drafted the Declaration to be ready when Congress voted on independence. John Adams, a leader in pushing for independence, had persuaded the committee to select Thomas Jefferson to compose the original draft of the document,[2] which Congress edited to produce the final version. The Declaration was a formal explanation of why Congress had voted to declare independence from Great Britain, more than a year after the outbreak of the American Revolutionary War. Adams wrote to his wife Abigail, ""The Second Day of July 1776, will be the most memorable Epocha, in the History of America""[3] – although Independence Day is actually celebrated on July 4, the date that the wording of the Declaration of Independence was approved.

After ratifying the text on July 4, Congress issued the Declaration of Independence in several forms. It was initially published as the printed Dunlap broadside that was widely distributed and read to the public. The source copy used for this printing has been lost and may have been a copy in Thomas Jefferson's hand.[4] Jefferson's original draft is preserved at the Library of Congress, complete with changes made by John Adams and Benjamin Franklin, as well as Jefferson's notes of changes made by Congress. The best-known version of the Declaration is a signed copy that is displayed at the National Archives in Washington, D.C., and which is popularly regarded as the official document. This engrossed copy (finalized, calligraphic copy) was ordered by Congress on July 19 and signed primarily on August 2.[5][6]

The sources and interpretation of the Declaration have been the subject of much scholarly inquiry. The Declaration justified the independence of the United States by listing 27 colonial grievances against King George III and by asserting certain natural and legal rights, including a right of revolution. Its original purpose was to announce independence, and references to the text of the Declaration were few in the following years. Abraham Lincoln made it the centerpiece of his policies and his rhetoric, as in the Gettysburg Address of 1863.[7] Since then, it has become a well-known statement on human rights, particularly its second sentence:

We hold these truths to be self-evident, that all men are created equal, that they are endowed by their Creator with certain unalienable Rights, that among these are Life, Liberty and the pursuit of Happiness.
The declaration was made to guarantee equal rights for every person, and if it had been intended for only a certain section of people, Congress would have left it as ""rights of Englishmen"".[8] Stephen Lucas called it ""one of the best-known sentences in the English language"",[9] with Joseph Ellis saying it contains ""the most potent and consequential words in American history"".[10] The passage came to represent a moral standard to which the United States should strive. This view was notably promoted by Lincoln, who considered the Declaration to be the foundation of his political philosophy and argued that it is a statement of principles through which the United States Constitution should be interpreted.[11]

The Declaration of Independence inspired many similar documents in other countries, the first being the 1789 Declaration of United Belgian States issued during the Brabant Revolution in the Austrian Netherlands. It also served as the primary model for numerous declarations of independence in Europe and Latin America, as well as Africa (Liberia) and Oceania (New Zealand) during the first half of the 19th century.[12]"
conservativism,"Conservatism is an aesthetic, cultural, social, and political philosophy, which seeks to promote and to preserve traditional social institutions.[1] The central tenets of conservatism may vary in relation to the traditional values or practices of the culture and civilization in which it appears. In Western culture, conservatives seek to preserve a range of institutions such as organized religion, parliamentary government, and property rights.[2] Adherents of conservatism often oppose modernism and seek a return to traditional values.[3][4]

The first established use of the term in a political context originated in 1818 with François-René de Chateaubriand[5] during the period of Bourbon Restoration that sought to roll back the policies of the French Revolution. Historically associated with right-wing politics, the term has since been used to describe a wide range of views. There is no single set of policies regarded as conservative because the meaning of conservatism depends on what is considered traditional in a given place and time. Conservative thought has varied considerably as it has adapted itself to existing traditions and national cultures.[6] For example, some conservatives advocate for greater government intervention in the economy[7] while others advocate for a more laissez faire free market economic system.[8] Thus conservatives from different parts of the world—each upholding their respective traditions—may disagree on a wide range of issues. Edmund Burke, an 18th-century politician who opposed the French Revolution, but earlier paradoxically supported the American Revolution, is credited as one of the main theorists of conservatism in the 1790s.[9]"
real estate,"Real estate is property consisting of land and the buildings on it, along with its natural resources such as crops, minerals or water; immovable property of this nature; an interest vested in this (also) an item of real property, (more generally) buildings or housing in general.[1][2]

Real estate is different from personal property, which is not permanently attached to the land, such as vehicles, boats, jewelry, furniture, tools and the rolling stock of a farm."
real estate in the city,"Real estate is property consisting of land and the buildings on it, along with its natural resources such as crops, minerals or water; immovable property of this nature; an interest vested in this (also) an item of real property, (more generally) buildings or housing in general.[1][2]

Real estate is different from personal property, which is not permanently attached to the land, such as vehicles, boats, jewelry, furniture, tools and the rolling stock of a farm."
new york city business growth,"The economy of New York City encompasses the largest municipal and regional economy in the United States. Anchored by Wall Street in Lower Manhattan, New York City has been characterized as the world's premier financial center.[1][2][3][4] It is home to the New York Stock Exchange (NYSE) and NASDAQ, the world's two largest stock exchanges by both market capitalization and trading activity. In 2012, the New York metropolitan area generated a gross metropolitan product (GMP) of over US$1.33 trillion with a population of 20.3 million people. The Combined Statistical Area[5] produced a GMP of over US$1.55 trillion. Both are ranked first nationally by a wide margin and being roughly equivalent to the GDP of South Korea at less than half its population.[6] The city's economy accounts for most of the economic activity in both the states of New York and New Jersey.[7]

Manhattan is the world's leading center of banking, finance, and communication. It is home to the New York Stock Exchange (NYSE) on Wall Street. Many of the world's largest corporations are headquartered in Manhattan. The borough contained over 500 million square feet (46.5 million m2) of office space in 2015, [8] making it the largest office market in the United States.[9] Midtown Manhattan, with nearly 400 million square feet (37.2 million m2) that same year,[8] is the largest central business district in the world.[10] New York City is distinctive for its high concentrations of advanced service sector firms in the law, accountancy, banking and management consultancy fields. It is the top global center for the advertising industry, which is metonymously called ""Madison Avenue"". Silicon Alley, metonymous for New York's broad-spectrum high technology sphere, continues to expand.

Finance, high technology, real estate, insurance, and health care all form the basis of New York City's economy. The city is also the nation's most important center for mass media, journalism, and publishing. Also, it is the country's preeminent arts center. Creative industries such as digital media, advertising, fashion, design, and architecture account for a growing share of employment. New York City possesses a strong competitive advantage in these industries.[11] Despite declining, manufacturing remains consequential."
immigrantt workers,"Foreign workers or guest workers are people who work in a country other than one of which they are a citizen. Some foreign workers use a guest worker program in a country with more preferred job prospects than in their home country. Guest workers are often either sent or invited to work outside their home country, or have acquired a job before they leave their home country, whereas migrant workers often leave their home country without having a specific job in prospect.

Tens of millions of people around the world operate as foreign workers. As of 2018, according to reports from the Bureau of Labor Statistics, there is an estimated 28 million foreign-born workers in the United States,[1] which draws most of its immigrants from Mexico, including 4 or 5 million undocumented workers. It is estimated[by whom?] that around 5 million foreign workers live in northwestern Europe, half a million in Japan, and around 5 million in Saudi Arabia. Between January and June in 2019 2.4 million foreigners arrived to work in Russia.[2] A comparable number of dependents may accompany international workers.[3]

Some foreign workers migrate from former colonies to a former colonial metropole (France, for example).[4] Chain migration may operate in building guest-worker communities."
fly,"Air travel is a form of travel in vehicles such as airplanes, jet aircraft, helicopters, hot air balloons, blimps, gliders, hang gliders, parachutes, or anything else that can sustain flight.[1] Use of air travel has greatly increased in recent decades – worldwide it doubled between the mid-1980s and the year 2000.[2] Modern air travel is much safer than road travel."
this paragraph only talk about the flight travel.,"Air travel is a form of travel in vehicles such as airplanes, jet aircraft, helicopters, hot air balloons, blimps, gliders, hang gliders, parachutes, or anything else that can sustain flight.[1] Use of air travel has greatly increased in recent decades – worldwide it doubled between the mid-1980s and the year 2000.[2] Modern air travel is much safer than road travel."
mathematical declline,"n the 1960s there were some major changes in the way schools taught their students in the United States. Reading began to be taught in a more random and illogical manner.  The teaching of systematic phonics was replaced with the ""look-say"" or ""whole language"" method, which relies on guessing and memorizing word shapes. The systematic teaching of phonics gives the students the tools they need to read any word.  It is only logical.  But our schools have given up on logic. 

Paul has studied the old math books and found that they used to be much more logical and systematic in their approach.  He began to teach math in the public schools in 1962.  There was a big change in the approach to the teaching of mathematics at that time.  There was a group called the ""School Mathematics Study Group"" or SMSG, which was formed to counter-act the lead that the Soviet Union had taken in the space race.  This group came up with something called ""modern math"", which was supposed to solve our problems.  In fact, it produced a generation of mathematical illiterates.  The notion of a ""spiral curriculum"" was also part of the changes in the approach to the teaching of mathematics. 

This approach stresses the continual introduction of various concepts in mathematics without developing an understanding of the laws behind them.  It is also based on memorization and guessing.  In math, guessing is called estimating.  Like reading, math is systematic and is best taught in a systematic manner.  According to the 1999 TIMMS test, which compares the mathematical  abilities of students in 21 countries, the students from the United States came in 19th.  They did better than students from South Africa and Cyprus.  All other students did better than those from the U.S.  As of 2007, the U.S. has quietly withdrawn from participation in these tests as math scores in the U.S. continue to decline.  If you use the materials and methods of the government schools, why would you expect a different outcome?"
optimism,"Optimism is an attitude reflecting a belief or hope that the outcome of some specific endeavor, or outcomes in general, will be positive, favorable, and desirable. A common idiom used to illustrate optimism versus pessimism is a glass filled with water to the halfway point: an optimist is said to see the glass as half full, while a pessimist sees the glass as half empty.

The term derives from the Latin optimum, meaning ""best"". Being optimistic, in the typical sense of the word, is defined as expecting the best possible outcome from any given situation.[1] This is usually referred to in psychology as dispositional optimism. It thus reflects a belief that future conditions will work out for the best.[2] For this reason, it is seen as a trait that fosters resilience in the face of stress.[3]

Theories of optimism include dispositional models, and models of explanatory style. Methods to measure optimism have been developed within both theoretical systems, such as various forms of the Life Orientation Test, for the original definition of optimism, or the Attributional Style Questionnaire designed to test optimism in terms of explanatory style.

Variation in optimism and pessimism is somewhat heritable[4] and reflects biological trait systems to some degree.[5] It is also influenced by environmental factors, including family environment,[4] with some suggesting it can be learned.[6] Optimism may also be linked to health.[7]"
delayed motherhood,"For the purpose of logistic regression modeling, we defined delayed motherhood as women having no children, who planned or were unsure about future children, and were either older than 30 or 33."
soviet kanukistan,"First attested in a 1993 article in the US anti-Semitic publication The Liberty Bell. Subsequently (and apparently independently) used by American conservative political commentator Pat Buchanan on the October 31, 2002 episode of his MSBNC television show, in reaction to Canadian objection to a U.S. law mandating Arab-Canadian visitors to the U.S. be photographed and fingerprinted."
news,"News is information about current events. This may be provided through many different media: word of mouth, printing, postal systems, broadcasting, electronic communication, or through the testimony of observers and witnesses to events. News is sometimes called ""hard news"" to differentiate it from soft media.

Common topics for news reports include war, government, politics, education, health, the environment, economy, business, fashion, and entertainment, as well as athletic events, quirky or unusual events. Government proclamations, concerning royal ceremonies, laws, taxes, public health, and criminals, have been dubbed news since ancient times. Technological and social developments, often driven by government communication and espionage networks, have increased the speed with which news can spread, as well as influenced its content. The genre of news as we know it today is closely associated with the newspaper."
better teachers,better teachers
self diagnosis,"Self-diagnosis is the process of diagnosing, or identifying, medical conditions in oneself. It may be assisted by medical dictionaries, books, resources on the Internet, past personal experiences, or recognizing symptoms or medical signs of a condition that a family member previously had.

Self-diagnosis is prone to error and may be potentially dangerous if inappropriate decisions are made on the basis of a misdiagnosis.[1] Because of the risks, self-diagnosis is officially discouraged by governments,[1] physicians, and patient care organizations. Even physicians are discouraged from engaging in self-diagnosis,[2] because doctors also make mistakes in diagnosing themselves.[3] If the self-diagnosis is wrong, then the misdiagnosis can result in improper health care, including wrong treatments and lack of care for serious conditions.[4]

One of the greatest dangers of self-diagnosis in psychological syndromes is that you may miss a medical disease that masquerades as a psychiatric syndrome. ... Self-diagnosis also undermines the role of the doctor, which is not the best way to start the relationship. ... Then there is the fact that we can know and see ourselves, but sometimes, we need a mirror to see ourselves more clearly. ... By self-diagnosing, you may be missing something that you cannot see. ... Another danger of self-diagnosis is that you may think that there is more wrong with you than there actually is. ... Self-diagnosis is also a problem when you are in a state of denial about your symptoms.[5]
However, self-diagnosis may be appropriate under certain circumstances.[6][7] All over-the-counter (non-prescription) medications are offered on the assumption that people are capable of self-diagnosis,[6] determining first that their condition is unlikely to be serious and then the possible harm caused by incorrect medication minor. Some conditions are more likely to be self-diagnosed, especially simple conditions such as head lice and skin abrasions or familiar conditions such as menstrual cramps, headache or the common cold.

Complex conditions for which medications are heavily advertised, including conditions like ADHD in adults,[8] present a more challenging situation. Direct-to-consumer marketing of medications is widely criticized for promoting inappropriate self-diagnosis.[9][10] One other condition that is commonly self-diagnosed is gluten intolerance.[11]"
state rights,"In American political discourse, states' rights are political powers held for the state governments rather than the federal government according to the United States Constitution, reflecting especially the enumerated powers of Congress and the Tenth Amendment. The enumerated powers that are listed in the Constitution include exclusive federal powers, as well as concurrent powers that are shared with the states, and all of those powers are contrasted with the reserved powers—also called states' rights—that only the states possess.[1][2]"
justice gisnberg,"Joan Ruth Bader Ginsburg (/ˈbeɪdər ˈɡɪnzbɜːrɡ/ BAY-dər GHINZ-burg; née Bader; March 15, 1933 – September 18, 2020)[1] was an American lawyer and jurist who served as an associate justice of the Supreme Court of the United States from 1993 until her death in September 2020.[2] She was nominated by President Bill Clinton, replacing retiring Justice Byron White,[3] and at the time was generally viewed as a moderate consensus-builder. She eventually became part of the liberal wing of the Court as the Court shifted to the right over time. Ginsburg was the first Jewish woman and the second woman to serve on the Court, after Sandra Day O'Connor. During her tenure, Ginsburg wrote notable majority opinions, including United States v. Virginia (1996), Olmstead v. L.C. (1999), Friends of the Earth, Inc. v. Laidlaw Environmental Services, Inc. (2000), and City of Sherrill v. Oneida Indian Nation of New York (2005).

Ginsburg was born and grew up in Brooklyn, New York. Her older sister died when she was a baby, and her mother died shortly before Ginsburg graduated from high school. She earned her bachelor's degree at Cornell University and married Martin D. Ginsburg, becoming a mother before starting law school at Harvard, where she was one of the few women in her class. Ginsburg transferred to Columbia Law School, where she graduated joint first in her class. During the early 1960s she worked with the Columbia Law School Project on International Procedure, learned Swedish and co-authored a book with Swedish jurist Anders Bruzelius; her work in Sweden profoundly influenced her thinking on gender equality. She then became a professor at Rutgers Law School and Columbia Law School, teaching civil procedure as one of the few women in her field.

Ginsburg spent much of her legal career as an advocate for gender equality and women's rights, winning many arguments before the Supreme Court. She advocated as a volunteer attorney for the American Civil Liberties Union and was a member of its board of directors and one of its general counsel in the 1970s. In 1980, President Jimmy Carter appointed her to the U.S. Court of Appeals for the District of Columbia Circuit, where she served until her appointment to the Supreme Court in 1993. Between O'Connor's retirement in 2006 and the appointment of Sonia Sotomayor in 2009, she was the only female justice on the Supreme Court. During that time, Ginsburg became more forceful with her dissents, notably in Ledbetter v. Goodyear Tire & Rubber Co. (2007). Ginsburg's dissenting opinion was credited with inspiring the Lilly Ledbetter Fair Pay Act which was signed into law by President Barack Obama in 2009, making it easier for employees to win pay discrimination claims. Ginsburg received attention in American popular culture for her passionate dissents in numerous cases, widely seen as reflecting paradigmatically liberal views of the law. She was dubbed ""The Notorious R.B.G."", and she later embraced the moniker.[4]

Despite two bouts with cancer and public pleas from liberal law scholars, she decided not to retire in 2013 or 2014 when Democrats could appoint her successor.[5][6][7] Ginsburg died at her home in Washington, D.C., on September 18, 2020, at the age of 87, from complications of metastatic pancreatic cancer. The Republican Senate majority in the 116th Congress confirmed Amy Coney Barrett to the vacancy created by Ginsburg's death on October 27, 2020. The appointment of Barrett was one of three major rightward shifts in the court since 1953, following the appointment of Clarence Thomas to replace Thurgood Marshall in 1991 and the appointment of Warren Burger to replace Earl Warren in 1969.[8]"
judaism,"Judaism (Hebrew: יַהֲדוּת‎, Yahadut; originally from Hebrew יְהוּדָה, Yehudah, ""Judah"", via Greek Ἰουδαϊσμός Ioudaismos;[6][7][8] the term itself is of Anglo-Latin origin c. 1400[9]) is an Abrahamic, monotheistic, and ethnic religion comprising the collective religious, cultural, and legal tradition and civilization of the Jewish people, also sometimes called Israelites.[10][1][11] Judaism is considered by religious Jews to be the expression of the covenant that God established with the Children of Israel.[12] It encompasses a wide body of texts, practices, theological positions, and forms of organization. The Torah is part of the larger text known as the Tanakh or the Hebrew Bible, and supplemental oral tradition represented by later texts such as the Midrash and the Talmud. With between 14.5 and 17.4 million adherents worldwide,[13] Judaism is the tenth largest religion in the world.

Within Judaism there are a variety of religious movements, most of which emerged from Rabbinic Judaism,[14][15] which holds that God revealed his laws and commandments to Moses on Mount Sinai in the form of both the Written and Oral Torah.[16] Historically, all or part of this assertion was challenged by various groups such as the Sadducees and Hellenistic Judaism during the Second Temple period;[14][17] the Karaites during the early and later medieval period; and among segments of the modern non-Orthodox denominations.[18] Some modern branches of Judaism such as Humanistic Judaism may be considered secular or nontheistic.[19][20] Today, the largest Jewish religious movements are Orthodox Judaism (Haredi Judaism and Modern Orthodox Judaism), Conservative Judaism, and Reform Judaism. Major sources of difference between these groups are their approaches to Jewish law, the authority of the Rabbinic tradition, and the significance of the State of Israel.[3][21][22] Orthodox Judaism maintains that the Torah and Jewish law are divine in origin, eternal and unalterable, and that they should be strictly followed. Conservative and Reform Judaism are more liberal, with Conservative Judaism generally promoting a more traditionalist interpretation of Judaism's requirements than Reform Judaism. A typical Reform position is that Jewish law should be viewed as a set of general guidelines rather than as a set of restrictions and obligations whose observance is required of all Jews.[23] Historically, special courts enforced Jewish law; today, these courts still exist but the practice of Judaism is mostly voluntary.[24] Authority on theological and legal matters is not vested in any one person or organization, but in the sacred texts and the rabbis and scholars who interpret them.

Judaism has its roots as an organized religion in the Middle East during the Bronze Age.[25] Modern Judaism evolved from ancient Israelite religion around 500 BCE,[26] and is considered one of the oldest monotheistic religions.[27][28] The Hebrews and Israelites were already referred to as ""Jews"" in later books of the Tanakh such as the Book of Esther, with the term Jews replacing the title ""Children of Israel"".[29] Judaism's texts, traditions and values strongly influenced later Abrahamic religions, including Christianity and Islam.[30][31] Hebraism, like Hellenism, played a seminal role in the formation of Western civilization through its impact as a core background element of Early Christianity.[32]

Jews are an ethnoreligious group[33] including those born Jewish, in addition to converts to Judaism. In 2019, the world Jewish population was estimated at about 14.7 million, or roughly 0.25% of the total world population.[34][35] About 46.9% of all Jews reside in Israel and another 38.8% reside in the United States and Canada, with most of the remainder living in Europe, and other minority groups spread throughout Latin America, Asia, Africa, and Australia.[36]"
healthy eating,"A healthy diet is a diet that helps maintain or improve overall health. A healthy diet provides the body with essential nutrition: fluid, macronutrients, micronutrients, and adequate food energy.[2][3]

A healthy diet may contain fruits, vegetables, and whole grains, and may include little to no processed food or sweetened beverages. The requirements for a healthy diet can be met from a variety of plant-based and animal-based foods, although a non-plant source of vitamin B12 is needed for those following a vegan diet.[4] Various nutrition guides are published by medical and governmental institutions to educate individuals on what they should be eating to be healthy. Nutrition facts labels are also mandatory in some countries to allow consumers to choose between foods based on the components relevant to health.[5][6]"
overweighy,"Being overweight or fat is having more body fat than is optimally healthy. Being overweight is especially common where food supplies are plentiful and lifestyles are sedentary.

As of 2003, excess weight reached epidemic proportions globally, with more than 1 billion adults being either overweight or obese.[1] In 2013, this increased to more than 2 billion.[2] Increases have been observed across all age groups.

A healthy body requires a minimum amount of fat for proper functioning of the hormonal, reproductive, and immune systems, as thermal insulation, as shock absorption for sensitive areas, and as energy for future use; however, the accumulation of too much storage fat can impair movement, flexibility, and alter the appearance of the body."
unhealthy diets,unhealthy diets
mossoff's lack of candidness,mossoff's lack of candidness
prostituiont,"Prostitution is illegal in the vast majority of the United States as a result of state laws rather than federal laws. It is, however, legal in some rural counties within the state of Nevada. Prostitution nevertheless occurs elsewhere in the country.

The regulation of prostitution in the country is not among the enumerated powers of the federal government. It is therefore exclusively the domain of the states to permit, prohibit, or otherwise regulate commercial sex under the Tenth Amendment to the United States Constitution, except insofar as Congress may regulate it as part of interstate commerce with laws such as the Mann Act. In most states, prostitution is considered a misdemeanor in the category of public order crime–crime that disrupts the order of a community. Prostitution was at one time considered a vagrancy crime.

Currently, Nevada is the only U.S. jurisdiction to allow legal prostitution – in the form of regulated brothels – the terms of which are stipulated in the Nevada Revised Statutes. Only eight counties currently contain active brothels. All forms of prostitution are illegal in these counties: Clark (which contains the Las Vegas–Paradise metropolitan area), Washoe (which contains Reno), Carson City, Douglas, Eureka, Lincoln & Pershing. The other counties theoretically allow brothel prostitution, but three of these counties currently have no active brothels. Street prostitution, ""pandering"", and living off of the proceeds of a prostitute remain illegal under Nevada law, as is the case elsewhere in the country.

According to the National Institute of Justice, a study conducted in 2008 alleged that approximately 15-20 percent of men in the country have engaged in commercial sex.[1]

As with other countries, prostitution in the U.S. can be divided into three broad categories: street prostitution, brothel prostitution, and escort prostitution."
unfair restaurant prices,unfair restaurant prices
menu,menu
voting while incarcarated,"Felony disenfranchisement laws bar millions of Americans from voting due to their felony conviction. Among those excluded are persons in prison, those serving felony probation or parole, and, in 11 states, some or all persons who have completed their sentence. While these disenfranchisement laws have been closely documented for years by advocacy organizations, academics, and lawmakers, the de facto disenfranchisement of people legally eligible to vote in jails has received less attention.

In local jails, the vast majority of persons are eligible to vote because they are not currently serving a sentence for a felony conviction. Generally, persons are incarcerated in jail pretrial, sentenced to misdemeanor offenses, or are sentenced and awaiting transfer to state prison. Of the 745,0001) individuals incarcerated in jail as of 2017 nearly two-thirds (64.7%), or 482,000, were being held pretrial because they had not been able to post bail. Of the 263,000 who were serving a sentence, the vast majority had been convicted of a misdemeanor offense that does not result in disenfranchisement.

Despite the fact that most persons detained in jail are eligible to vote, very few actually do. Jail administrators often lack knowledge about voting laws, and bureaucratic obstacles to establishing a voting process within institutions contribute significantly to limited voter participation. Indeed, acquiring voter registration forms or an absentee ballot while incarcerated is challenging when someone cannot use the internet or easily contact the Board of Elections in their community. In addition, many persons in jail do not know they maintain the right to vote while incarcerated, and there are few programs to guarantee voting access.

Problems with voting in jail disproportionately impact communities of color since almost half (48%) of persons in jail nationally are African American or Latino. Other racial groups, including Native Americans and Asians, comprise about 2% of the jail population, or 13,000 persons as of 2017.

In recent years, some jurisdictions have adopted policies and practices to ensure voting access for persons incarcerated in local jails because of initiatives developed by jail leadership and advocacy organizations. This report examines six programs designed to expand voting access for eligible incarcerated citizens. The success and expansion of these efforts will improve democracy."
paying restaurant employees,paying restaurant employees
medical information,medical information
privatized infrastruce,"Privatization (or privatisation in British English) can mean different things including moving something from the public sector into the private sector. It is also sometimes used as a synonym for deregulation when a heavily regulated private company or industry becomes less regulated. Government functions and services may also be privatised (which may also be known as ""franchising"" or ""out-sourcing""); in this case, private entities are tasked with the implementation of government programs or performance of government services that had previously been the purview of state-run agencies. Some examples include revenue collection, law enforcement, water supply, and prison management.[1]

Another definition is the sale of a state-owned enterprise or municipally owned corporation to private investors; in this case shares may be traded in the public market for the first time, or for the first time since an enterprise's previous nationalization. This type of privatization can include the demutualization of a mutual organization, cooperative, or public-private partnership in order to form a joint-stock company.[2]

Separately, privatization can refer to the purchase of all outstanding shares of a publicly-traded company by private equity investors, which is more often called ""going private"". Before and after this process the company is privately owned, but after the buyout its shares are withdrawn from being traded at a public stock exchange.[3][4]"
obestiy,"Obesity is a medical condition in which excess body fat has accumulated to an extent that it may have a negative effect on health.[1] People are generally considered obese when their body mass index (BMI), a measurement obtained by dividing a person's weight by the square of the person's height—despite known allometric inaccuracies[a]—is over 30 kg/m2; the range 25–30 kg/m2 is defined as overweight.[1] Some East Asian countries use lower values.[10] Obesity is correlated with various diseases and conditions, particularly cardiovascular diseases, type 2 diabetes, obstructive sleep apnea, certain types of cancer, and osteoarthritis.[2] High BMI is a marker of risk, but not proven to be a direct cause, for diseases caused by diet, physical activity, and environmental factors.[11] A reciprocal link has been found between obesity and depression, with obesity increasing the risk of clinical depression and also depression leading to a higher chance of developing obesity.[3]

Obesity has individual, socioeconomic, and environmental causes, including diet, physical activity, automation, urbanization, genetic susceptibility, medications, mental disorders, economic policies, endocrine disorders, and exposure to endocrine-disrupting chemicals.[1][4][12][13] While a majority of obese individuals at any given time are attempting to lose weight and are often successful, research shows that maintaining that weight loss over the long term proves to be rare.[14] The reasons for weight cycling are not fully understood but may include decreased energy expenditure combined with increased biological urge to eat during and after caloric restriction.[14] More studies are needed to determine if weight cycling and yo-yo dieting contribute to inflammation and disease risk in obese individuals.[14]

Obesity prevention requires a complex approach, including interventions at community, family, and individual levels.[1][11] Changes to diet and exercising are the main treatments recommended by health professionals.[2] Diet quality can be improved by reducing the consumption of energy-dense foods, such as those high in fat or sugars, and by increasing the intake of dietary fiber.[1] However, large-scale analyses have found an inverse relationship between energy density and energy cost of foods in developed nations.[15] Low-income populations are more likely to live in neighborhoods that are considered ""food deserts"" or ""food swamps"" where nutritional groceries are less available.[16] Medications can be used, along with a suitable diet, to reduce appetite or decrease fat absorption.[5] If diet, exercise, and medication are not effective, a gastric balloon or surgery may be performed to reduce stomach volume or length of the intestines, leading to feeling full earlier or a reduced ability to absorb nutrients from food.[6][17]

Obesity is a leading preventable cause of death worldwide, with increasing rates in adults and children.[1][18] In 2015, 600 million adults (12%) and 100 million children were obese in 195 countries.[7] Obesity is more common in women than in men.[1] Authorities view it as one of the most serious public health problems of the 21st century.[19] Obesity is stigmatized in much of the modern world (particularly in the Western world), though it was seen as a symbol of wealth and fertility at other times in history and still is in some parts of the world.[2][20] In 2013, several medical societies, including the American Medical Association and the American Heart Association, classified obesity as a disease.[21][22][23]"
problems with big banks,problems with big banks
sustainability solution,"The Fermi paradox, named after Italian-American physicist Enrico Fermi, is the apparent contradiction between the lack of evidence for extraterrestrial life and various high estimates for their probability (such as some optimistic estimates for the Drake equation).[1][2]

The following are some of the facts and hypotheses that together serve to highlight the apparent contradiction:

There are billions of stars in the Milky Way similar to the Sun.[3][4]
With high probability, some of these stars have Earth-like planets in a circumstellar habitable zone.[5]
Many of these stars, and hence their planets, are much older than the Sun.[6][7] If the Earth is typical, some may have developed intelligent life long ago.
Some of these civilizations may have developed interstellar travel, a step humans are investigating now.
Even at the slow pace of currently envisioned interstellar travel, the Milky Way galaxy could be completely traversed in a few million years.[8]
And since many of the stars similar to the Sun are billions of years older, Earth should have already been visited by extraterrestrial civilizations, or at least their probes.[9]
However, there is no convincing evidence that this has happened.[8]
There have been many attempts to explain the Fermi paradox,[10][11] primarily suggesting that intelligent extraterrestrial beings are extremely rare, that the lifetime of such civilizations is short, or that they exist but (for various reasons) humans see no evidence.

Although he was not the first to consider this question, Fermi's name is associated with the paradox because of a casual conversation in the summer of 1950 with fellow physicists Edward Teller, Herbert York and Emil Konopinski. While walking to lunch, the men discussed recent UFO reports and the possibility of faster-than-light travel. The conversation moved on to other topics, until during lunch Fermi allegedly said suddenly, ""But where is everybody?"" (although the exact quote is uncertain).[12][13]"
impeding the inspectors,impeding the inspectors
sexism,"Sexism in American political elections refers to sexism which is discrimination, prejudice, or stereotyping based on sexual characteristics or perceived sexual characteristics.[1] Sexism is inherently a product of culture, as culture instills a certain set of beliefs or expectations for what constitutes as appropriate behavior, appearance, or mannerisms for a sex.[2]

Sexism in American political elections is generally cited as a socially-driven obstacle to female political candidates, especially for non-incumbents, raising concerns about the influence of women on politics in the United States.[3] When women are seen as threatening to a man's power or leadership, it can often produce hostile sexism.[4]

Sexism occurs against those who identify as female.[3] Women are undervalued in society because of their sex.[5] This is seen in elections when qualified women are judged based on appearance and lose to men.[3]"
socetial pressures on women,socetial pressures on women
union,union
classroom teachers,classroom teachers
child birih,"Childbirth, also known as labour or delivery, is the ending of pregnancy where one or more babies leaves the uterus by passing through the vagina or by Caesarean section.[7] In 2015, there were about 135 million births globally.[8] About 15 million were born before 37 weeks of gestation,[9] while between 3 and 12% were born after 42 weeks.[10] In the developed world most deliveries occur in hospitals,[11][12] while in the developing world most births take place at home with the support of a traditional birth attendant.[13]

The most common way of childbirth is a vaginal delivery.[6] It involves three stages of labour: the shortening and opening of the cervix during the first stage, descent and birth of the baby during the second stage, and the delivery of the placenta during the third stage.[14][15] The first stage begins with crampy abdominal or back pain that lasts around half a minute and occurs every 10 to 30 minutes.[14] The pain becomes stronger and closer together over time.[15] The second stage ends when the infant is fully expelled. In the third stage, the delivery of the placenta, delayed clamping of the umbilical cord is generally recommended.[16] As of 2014, all major health organisations advise that immediately following vaginal birth, or as soon as the mother is alert and responsive after a Caesarean section, that the infant be placed on the mother's chest, termed skin-to-skin contact, delaying routine procedures for at least one to two hours or until the baby has had its first breastfeeding.[17][18][19]

Most babies are born head first; however about 4% are born feet or buttocks first, known as breech.[15][20] Typically the head enters the pelvis facing to one side, and then rotates to face down.[21] During labour, a woman can generally eat and move around as she likes.[22] A number of methods can help with pain, such as relaxation techniques, opioids, and spinal blocks.[15] While making a cut to the opening of the vagina, known as an episiotomy, is common, it is generally not needed.[15] In 2012, about 23 million deliveries occurred by Caesarean section, an operation on the abdomen.[23][15]

Each year, complications from pregnancy and childbirth result in about 500,000 maternal deaths, seven million women have serious long-term problems, and 50 million women have negative health outcomes following delivery.[5] Most of these occur in the developing world.[5] Specific complications include obstructed labour, postpartum bleeding, eclampsia, and postpartum infection.[5] Complications in the baby may include lack of oxygen at birth, birth trauma, prematurity, and infections.[4][24]"
hospitalization for births,hospitalization for births
breast canser,breast canser
ban,ban
looking at violence carried out in responce to the mere concept,looking at violence carried out in responce to the mere concept
closed minded individuals,having or showing rigid opinions or a narrow outlook.
having closed minds,having or showing rigid opinions or a narrow outlook.
costing the taxpayers,costing the taxpayers
business competition,"In capitalist economics, market competition is the rivalry among sellers trying to achieve such goals as increasing profits, market share and sales volume by varying the elements of the marketing mix: price, product, distribution and promotion. Merriam-Webster defines competition in business as ""the effort of two or more parties acting independently to secure the business of a third party by offering the most favourable terms"".[106] It was described by Adam Smith in The Wealth of Nations (1776) and later economists as allocating productive resources to their most highly valued uses[107] and encouraging efficiency. Smith and other classical economists before Antoine Augustine Cournot were referring to price and non-price rivalry among producers to sell their goods on best terms by bidding of buyers, not necessarily to a large number of sellers nor to a market in final equilibrium.[108] Competition is widespread throughout the market process. It is a condition where ""buyers tend to compete with other buyers, and sellers tend to compete with other sellers"".[109] In offering goods for exchange, buyers competitively bid to purchase specific quantities of specific goods which are available, or might be available if sellers were to choose to offer such goods. Similarly, sellers bid against other sellers in offering goods on the market, competing for the attention and exchange resources of buyers. Competition results from scarcity, as it is not possible to satisfy all conceivable human wants, and occurs as people try to meet the criteria being used to determine allocation.[109]: 105 

In the works of Adam Smith, the idea of capitalism is made possible through competition which creates growth. Although capitalism has not entered mainstream economics at the time of Smith, it is vital to the construction of his ideal society. One of the foundational blocks of capitalism is competition. Smith believed that a prosperous society is one where ""everyone should be free to enter and leave the market and change trades as often as he pleases.""[110] He believed that the freedom to act in one's self-interest is essential for the success of a capitalist society. The fear arises[weasel words] that if all participants focus on their own goals, society's well-being will be water under the bridge. Smith maintains that despite the concerns of intellectuals, ""global trends will hardly be altered if they refrain from pursuing their personal ends.""[111] He insisted that the actions of a few participants cannot alter the course of society. Instead, Smith maintained that they should focus on personal progress instead and that this will result in overall growth to the whole. Competition between participants, ""who are all endeavoring to justle one another out of employment, obliges every man to endeavor to execute his work"" through competition towards growth."
mayo clinic,"The Mayo Clinic (/ˈmeɪjoʊ/) is a nonprofit American academic medical center focused on integrated health care, education, and research.[6] It employs over 4,500 physicians and scientists, along with another 58,400 administrative and allied health staff, across three major campuses: Rochester, Minnesota; Jacksonville, Florida; and Phoenix/Scottsdale, Arizona.[7][8] The practice specializes in treating difficult cases through tertiary care and destination medicine. It is home to the top-15 ranked Mayo Clinic Alix School of Medicine in addition to many of the highest regarded residency education programs in the United States.[9][10][11] It spends over $660 million a year on research and has more than 3,000 full-time research personnel.[12][13]

William Worrall Mayo settled his family in Rochester in 1864 and opened a sole proprietorship medical practice that evolved under his sons, Will and Charlie Mayo, along with practice partners Drs. Stinchfield, Graham, Plummer, Millet, Judd, and Balfour, into Mayo Clinic. Today, in addition to the hospital at Rochester, Mayo Clinic has major campuses in Arizona[14] and Florida.[15] Most recently, in 2020, the Mayo Clinic bought a facility in central London, UK.[16][17] The Mayo Clinic Health System also operates affiliated facilities throughout Minnesota, Wisconsin, and Iowa.[18]

Mayo Clinic is ranked first in the United States in U.S. News & World Report's 2019–20 Best Hospitals Honor Roll,[19] maintaining a position at or near the top for more than 27 years.[20] It has been on the list of ""100 Best Companies to Work For"" published by Fortune magazine for fourteen consecutive years, and has continued to achieve this ranking through 2017.[21][22] Drawing in patients from around the globe, Mayo Clinic performs near the highest number of transplants in the country, including both solid organ and hematologic transplantation."
fitness,fitness
aid from usa,"The American taxpayer has been generous to foreign countries. Between the years 2013 and 2018, nearly $300 billion in U.S. taxpayer money flowed as “aid” to countries outside the United States.

Each year, the U.S. spent about $47 billion. Half the aid went to Africa and the Middle East in FY2018, the latest year available for these statistics. Interestingly, despite President Donald Trump’s “America First” agenda, aid to foreign countries remained virtually unchanged in the first two years of his administration.

This week, our organization at OpenTheBooks.com published an oversight report, U.S. Foreign Aid – How And Where The U.S. Spent $282.6 Billion, Plus Updated COVID-19 Aid & Payments To The UN And Other Agencies. Sinclair Broadcast launched the report on their National Desk and 190 local affiliates with ABC, NBC, CBS, and FOX.

Our auditors found that U.S. foreign aid dwarfs the federal funds spent by 48 out of 50 state governments annually. Only the state governments of California and New York spent more federal funds than what the U.S. sent abroad each year to foreign countries."
israel againt palestinian,"The Israeli–Palestinian conflict is one of the world's most enduring conflicts, with the Israeli occupation of the West Bank and the Gaza Strip reaching 54 years of conflict.[7] Various attempts have been made to resolve the conflict as part of the Israeli–Palestinian peace process.[8][9][10][11]

Public declarations of claims to a Jewish homeland in Palestine, including the 1897 First Zionist Congress and the 1917 Balfour Declaration, created early tension in the region. At the time, the region had a small minority Jewish population, although this was growing via significant Jewish immigration. Following the implementation of the Mandate for Palestine, which included a binding obligation on the British government for the ""establishment in Palestine of a national home for the Jewish people"" the tension grew into sectarian conflict between Jews and Arabs.[12][13] Attempts to solve the early conflict culminated in the 1947 United Nations Partition Plan for Palestine and the 1947–1949 Palestine war, marking the start of the wider Arab–Israeli conflict. The current Israeli-Palestinian status quo began following Israeli military occupation of the Palestinian territories in the 1967 Six-Day War.

Despite a long-term peace process, Israelis and Palestinians have failed to reach a final peace agreement. Progress was made towards a two-state solution with the 1993–1995 Oslo Accords, but today the Palestinians remain subject to Israeli military occupation in the Gaza Strip and in 165 ""islands"" across the West Bank. Key issues that have stalled further progress are security, borders, water rights, control of Jerusalem, Israeli settlements,[14] Palestinian freedom of movement,[15] and Palestinian right of return. The violence of the conflict, in a region rich in sites of historic, cultural and religious interest worldwide, has been the subject of numerous international conferences dealing with historic rights, security issues and human rights, and has been a factor hampering tourism in and general access to areas that are hotly contested.[16] Many attempts have been made to broker a two-state solution, involving the creation of an independent Palestinian state alongside the State of Israel (after Israel's establishment in 1948). In 2007, the majority of both Israelis and Palestinians, according to a number of polls, preferred the two-state solution over any other solution as a means of resolving the conflict.[17]

Within Israeli and Palestinian society, the conflict generates a wide variety of views and opinions. This highlights the deep divisions which exist not only between Israelis and Palestinians, but also within each society. A hallmark of the conflict has been the level of violence witnessed for virtually its entire duration. Fighting has been conducted by regular armies, paramilitary groups, terror cells, and individuals. Casualties have not been restricted to the military, with a large number of civilian fatalities on both sides. There are prominent international actors involved in the conflict. A majority of Jews see the Palestinians' demand for an independent state as just, and think Israel can agree to the establishment of such a state.[18] The majority of Palestinians and Israelis in the West Bank and Gaza Strip have expressed a preference for a two-state solution.[19][20][unreliable source?] Mutual distrust and significant disagreements are deep over basic issues, as is the reciprocal skepticism about the other side's commitment to upholding obligations in an eventual agreement.[21]

The two parties currently engaged in direct negotiation are the Israeli government, led by Naftali Bennett, and the Palestine Liberation Organization (PLO), headed by Mahmoud Abbas. The official negotiations are mediated by an international contingent known as the Quartet on the Middle East (the Quartet) represented by a special envoy, that consists of the United States, Russia, the European Union, and the United Nations. The Arab League is another important actor, which has proposed an alternative peace plan. Egypt, a founding member of the Arab League, has historically been a key participant. Jordan, having relinquished its claim to the West Bank in 1988 and holding a special role in the Muslim Holy shrines in Jerusalem, has also been a key participant.

Since 2006, the Palestinian side has been fractured by conflict between two major factions: Fatah, the traditionally dominant party, and its later electoral challenger, Hamas, which also operates as a militant organization. After Hamas's electoral victory in 2006, the Quartet conditioned future foreign assistance to the Palestinian National Authority (PA) on the future government's commitment to non-violence, recognition of the State of Israel, and acceptance of previous agreements. Hamas rejected these demands,[22] which resulted in the Quartet's suspension of its foreign assistance program, and the imposition of economic sanctions by the Israelis.[23] A year later, following Hamas's seizure of the Gaza Strip in June 2007, the territory officially recognized as the PA was split between Fatah in the West Bank and Hamas in the Gaza Strip. The division of governance between the parties had effectively resulted in the collapse of bipartisan governance of the PA. However, in 2014, a Palestinian Unity Government, composed of both Fatah and Hamas, was formed. The latest round of peace negotiations began in July 2013 and was suspended in 2014.

In May 2021, amidst rising tensions, the 2021 Israel–Palestine crisis began with protests that escalated into rocket attacks from Gaza and airstrikes by Israel."
fee,fee
fired from job,"Unemployment generally falls during periods of economic prosperity and rises during recessions, creating significant pressure on public finances as tax revenue falls and social safety net costs increase. Government spending and taxation decisions (fiscal policy) and U.S. Federal Reserve interest rate adjustments (monetary policy) are important tools for managing the unemployment rate. There may be an economic trade-off between unemployment and inflation, as policies designed to reduce unemployment can create inflationary pressure, and vice versa. The U.S. Federal Reserve (the Fed) has a dual mandate to achieve full employment while maintaining a low rate of inflation. The major political parties debate appropriate solutions for improving the job creation rate, with liberals arguing for more government spending and conservatives arguing for lower taxes and less regulation. Polls indicate that Americans believe job creation is the most important government priority, with not sending jobs overseas the primary solution.[3]

Unemployment can be measured in several ways. A person is defined as unemployed in the United States if they are jobless, but have looked for work in the last four weeks and are available for work. People who are neither employed nor defined as unemployed are not included in the labor force calculation. For example, as of September 2017, the unemployment rate (formally defined as the ""U-3"" rate) in the United States was 4.2% representing 6.8 million unemployed people.[4][5] The unemployment rate was calculated by dividing the number of unemployed by the number in the civilian labor force (age 16+, non-military and not incarcerated) of approximately 159.6 million people,[6] relative to a U.S. population of approximately 326 million people.[7] The historical average unemployment rate (January 1948-September 2020) is 5.8%.[4] The government's broader U-6 unemployment rate, which includes the part-time underemployed was 8.3% in September 2017.[8][9] Both of these rates fell steadily from 2010 to 2019; the U-3 rate was below the November 2007 level that preceded the Great Recession by November 2016, while the U-6 rate did not fully recover until August 2017.[4][8]

The U.S. Bureau of Labor Statistics (BLS) publishes a monthly ""Employment Situation Summary"" with key statistics and commentary.[10] As of June 2018, approximately 128.6 million people in the United States have found full-time work (at least 35 hours a week in total), while 27.0 million worked part-time.[11] There were 4.7 million working part-time for economic reasons, meaning they wanted but could not find full-time work, the lowest level since January 2008.[12]

The vast majority of persons outside the civilian labor force (age 16+) are there by choice. The BLS reported that in July 2018, there were 94.1 million persons age 16+ outside the labor force. Of these, 88.6 million (94%) did not want a job while 5.5 million (6%) wanted a job.[13] Key reasons persons age 16+ are outside the labor force include retired, disabled or illness, attending school, and caregiving.[14] The Congressional Budget Office reported that as of December 2017, the primary reason for men age 25–54 to be outside the labor force was illness/disability (50% or 3.5 million), while the primary reason for women was due to family care-giving (60% or 9.6 million).[15]

The Congressional Budget Office estimated that the U.S. was approximately 2.5 million workers below full employment as of the end of 2015 and 1.6 million at December 31, 2016, mainly due to lower labor force participation. This was very close to full employment, indicating a strong economy.[16] As of May 2018, there were more job openings (6.6 million) than people defined as unemployed (6.0 million) in the U.S.[17][18][19]

In September 2019, the U.S. unemployment rate dropped to 3.5%, near the lowest rate in 50 years.[20] On May 8, 2020, the Bureau of Labor Statistics reported that 20.5 million nonfarm jobs were lost and the unemployment rate rose to 14.7 percent in April, due to the Coronavirus pandemic in the United States.[21]"
tea party,"The Tea Party movement was an American fiscally conservative political movement within the Republican Party. Members of the movement called for lower taxes, and for a reduction of the national debt of the United States and federal budget deficit through decreased government spending.[1][2] The movement supported small-government principles[3][4] and opposes government-sponsored universal healthcare.[5] The Tea Party movement has been described as a popular constitutional movement[6] composed of a mixture of libertarian,[7] right-wing populist,[8] and conservative activism.[9] It has sponsored multiple protests and supported various political candidates since 2009.[10][11][12] According to the American Enterprise Institute, various polls in 2013 estimated that slightly over 10 percent of Americans identified as part of the movement.[13]

The Tea Party movement was popularly launched following a February 19, 2009 call by CNBC reporter Rick Santelli on the floor of the Chicago Mercantile Exchange for a ""tea party"".[14][15] Several conservative activists agreed by conference call to coalesce against President Barack Obama's agenda and scheduled a series of protests.[16][17] Supporters of the movement subsequently had a major impact on the internal politics of the Republican Party. Although the Tea Party is not a political party in the classic sense of the word, some research suggests that members of the Tea Party Caucus vote like a significantly farther right third party in Congress.[18] A major force behind it was Americans for Prosperity (AFP), a conservative political advocacy group founded by businessman and political activist David Koch. It is unclear exactly how much money is donated to AFP by David and his brother Charles Koch.[19] By 2019, it was reported that the conservative wing of the Republican Party ""has basically shed the tea party moniker.""[20]

The movement's name refers to the Boston Tea Party of December 16, 1773, a watershed event in the launch of the American Revolution. The 1773 event demonstrated against taxation by the British government without political representation for the American colonists, and references to the Boston Tea Party and even costumes from the 1770s era are commonly heard and seen in the Tea Party movement.[21]"
"what guys can and should be doing in relationships, families, and the world at large","what guys can and should be doing in relationships, families, and the world at large"
machine labor,"Machine learning (ML) is mostly a predictive enterprise, while the questions of interest to labor economists are mostly causal. In pursuit of causal effects, however, ML may be useful for automated selection of ordinary least squares (OLS) control variables. We illustrate the utility of ML for regression-based causal inference by using lasso to select control variables for estimates of effects of college characteristics on wages. ML also seems relevant for an instrumental variables (IV) first stage, since the bias of two-stage least squares can be said to be due to over-fitting. Our investigation shows, however, that while ML-based instrument selection can improve on conventional 2SLS estimates, split-sample IV, jackknife IV, and LIML estimators do better. In some scenarios, the performance of ML-augmented IV estimators is degraded by pretest bias. In others, nonlinear ML for covariate control creates artificial exclusion restrictions that generate spurious findings. ML does better at choosing control variables for models identified by conditional independence assumptions than at choosing instrumental variables for models identified by exclusion restrictions."
macihines,machines
machine use,machine use
airplane,airplane
airline standardization,"Commercial aviation is the safest and fastest form of transportation in the world. This is made possible by the ICAO which develops best practices and regulations for international air transport.

Civil aviation is working as a catalyst for the world’s largest industry that is “Travel and Tourism”. Therefore standardization is very necessary for a healthy and growing air transport system. It creates and supports a large number of employments around the world.

Here, I’m going to share the real necessity of standards in civil aviation which encourage the safety and quality matters pertaining to aviation and also tell who is the leading organization in international civil aviation, how they work on to keep civil aviation standards up.
"
internet searching,internet searching
real estate criminals,"Real estate is property consisting of land and the buildings on it, along with its natural resources such as crops, minerals or water; immovable property of this nature; an interest vested in this (also) an item of real property, (more generally) buildings or housing in general.[1][2]

Real estate is different from personal property, which is not permanently attached to the land, such as vehicles, boats, jewelry, furniture, tools and the rolling stock of a farm."
financing shcools,financing shcools
training of teachers,"Teacher education or teacher training refers to the policies, procedures, and provision designed to equip (prospective) teachers with the knowledge, attitudes, behaviors, and skills they require to perform their tasks effectively in the classroom, school, and wider community. The professionals who engage in training the prospective teachers are called teacher educators (or, in some contexts, teacher trainers).

There is a longstanding and ongoing debate about the most appropriate term to describe these activities. The term 'teacher training' (which may give the impression that the activity involves training staff to undertake relatively routine tasks) seems to be losing ground, at least in the U.S., to 'teacher education' (with its connotation of preparing staff for a professional role as a reflective practitioner).The two major components of teacher education are in-service teacher education and pre-service teacher education.[1]"
dilluting the college system,dilluting the college system
du pont,"DuPont de Nemours, Inc., commonly known as DuPont, is an American company formed by the merger of Dow Chemical and E. I. du Pont de Nemours and Company on August 31, 2017, and the subsequent spinoffs of Dow Inc. and Corteva. Prior to the spinoffs it was the world's largest chemical company in terms of sales. The merger has been reported to be worth an estimated $130 billion.[2][3][4] With 2018 total revenue of $86 billion, DowDuPont ranked No. 35 on the 2019 Fortune 500 list of the largest United States public corporations.[5] DuPont is headquartered in Wilmington, Delaware, in the state where it is incorporated[6] since the founding of the old DuPont in 1802.

Within 18 months of the merger the DowDupont was split into three publicly traded companies with focuses on agriculture (Corteva), materials science (Dow Inc.), and specialty products (DuPont).[2][3][4][7]"
iphone use,"The iPhone is a line of smartphones designed and marketed by Apple Inc. that use Apple's iOS mobile operating system. The first-generation iPhone was announced by then-Apple CEO Steve Jobs on January 9, 2007. Since then, Apple has annually released new iPhone models and iOS updates. As of November 1, 2018, more than 2.2 billion iPhones had been sold.

The iPhone has a user interface built around a multi-touch screen. It connects to cellular networks or Wi-Fi, and can make calls, browse the web, take pictures, play music and send and receive emails and text messages. Since the iPhone's launch further features have been added, including larger screen sizes, shooting video, waterproofing, the ability to install third-party mobile apps through an app store, and many accessibility features. Up to 2017, iPhones used a layout with a single button on the front panel that returns the user to the home screen. Since 2017, more expensive iPhone models have switched to a nearly bezel-less front screen design with app switching activated by gesture recognition.

The iPhone is one of the two largest smartphone platforms in the world alongside Android, forming a large part of the luxury market. The iPhone has generated large profits for Apple, making it one of the world's most valuable publicly traded companies. The first-generation iPhone was described as ""revolutionary"" and a ""game-changer"" for the mobile phone industry and subsequent models have also garnered praise. The iPhone has been credited with popularizing the smartphone and slate form factor, and with creating a large market for smartphone apps, or ""app economy"". As of January 2017, Apple's App Store contained more than 2.2 million applications for the iPhone."
black market,"A black market, underground economy or shadow economy, is a clandestine market or series of transactions that has some aspect of illegality or is characterized by some form of noncompliant behavior with an institutional set of rules. If the rule defines the set of goods and services whose production and distribution is prohibited by law, non-compliance with the rule constitutes a black market trade since the transaction itself is illegal. Parties engaging in the production or distribution of prohibited goods and services are members of the illegal economy. Examples include the drug trade, prostitution (where prohibited), illegal currency transactions and human trafficking.[1] Violations of the tax code involving income tax evasion constitute membership in the unreported economy.[2][3]

Because tax evasion or participation in a black market activity is illegal, participants will attempt to hide their behavior from the government or regulatory authority.[4] Cash usage is the preferred medium of exchange in illegal transactions since cash usage does not leave a footprint.[5] Common motives for operating in black markets are to trade contraband, avoid taxes and regulations, or skirt price controls or rationing. Typically the totality of such activity is referred to with the definite article as a complement to the official economies, by market for such goods and services, e.g. ""the black market in bush meat"".

The black market is distinct from the grey market, in which commodities are distributed through channels that, while legal, are unofficial, unauthorized, or unintended by the original manufacturer, and the white market, in which trade is legal and official.

Black money is the proceeds of an illegal transaction, on which income and other taxes have not been paid, and which can only be legitimised by some form of money laundering. Because of the clandestine nature of the black economy it is not possible to determine its size and scope.[6]"
rebellion,"Rebellion, uprising, or insurrection is a refusal of obedience or order.[1] It refers to the open resistance against the orders of an established authority.[citation needed]

A rebellion originates from a sentiment of indignation and disapproval of a situation and then manifests itself by the refusal to submit or to obey the authority responsible for this situation.[citation needed] Rebellion can be individual or collective, peaceful (civil disobedience, civil resistance, and nonviolent resistance) or violent (terrorism, sabotage and guerrilla warfare).[citation needed]

In political terms, rebellion and revolt are often distinguished by their different aims. If rebellion generally seeks to evade and/or gain concessions from an oppressive power, a revolt seeks to overthrow and destroy that power, as well as its accompanying laws. The goal of rebellion is resistance while a revolt seeks a revolution.[citation needed] As power shifts relative to the external adversary, or power shifts within a mixed coalition, or positions harden or soften on either side, an insurrection may seesaw between the two forms.[citation needed]"
hand-written print  and cursive,"A manuscript (abbreviated MS for singular and MSS for plural) was, traditionally, any document written by hand – or, once practical typewriters became available, typewritten – as opposed to mechanically printed or reproduced in some indirect or automated way.[1] More recently, the term has come to be understood to further include any written, typed, or word-processed copy of an author's work, as distinguished from its rendition as a printed version of the same.[2] Before the arrival of printing, all documents and books were manuscripts. Manuscripts are not defined by their contents, which may combine writing with mathematical calculations, maps, music notation, explanatory figures or illustrations."
cursive,"Rebellion, uprising, or insurrection is a refusal of obedience or order.[1] It refers to the open resistance against the orders of an established authority.[citation needed]

A rebellion originates from a sentiment of indignation and disapproval of a situation and then manifests itself by the refusal to submit or to obey the authority responsible for this situation.[citation needed] Rebellion can be individual or collective, peaceful (civil disobedience, civil resistance, and nonviolent resistance) or violent (terrorism, sabotage and guerrilla warfare).[citation needed]

In political terms, rebellion and revolt are often distinguished by their different aims. If rebellion generally seeks to evade and/or gain concessions from an oppressive power, a revolt seeks to overthrow and destroy that power, as well as its accompanying laws. The goal of rebellion is resistance while a revolt seeks a revolution.[citation needed] As power shifts relative to the external adversary, or power shifts within a mixed coalition, or positions harden or soften on either side, an insurrection may seesaw between the two forms.[citation needed]"
lebron,"LeBron Raymone James Sr. (/ləˈbrɒn/; born December 30, 1984) is an American professional basketball player for the Los Angeles Lakers of the National Basketball Association (NBA). Nicknamed ""King James"", he is widely considered one of the greatest players in NBA history and is frequently compared to Michael Jordan in debates over the greatest basketball player ever.[1] The only player to have won NBA championships with three franchises (the Cleveland Cavaliers, the Miami Heat, and the Lakers) as NBA Finals MVP,[2] James has competed in ten NBA Finals, eight of them consecutively with the Heat and the Cavaliers from 2011 to 2018. His accomplishments include four NBA championships, four NBA MVP awards, four NBA Finals MVP awards, and two Olympic gold medals. During his 18-year career, James holds the record for all-time playoffs points, is third in all-time points, and eighth in career assists. James has been selected to the All-NBA Team a record 17 times (with a record of 13 First Team selections and 11 consecutive First Team selections, the latter of which is shared with Jordan and Karl Malone), made the NBA All-Defensive First Team five times, and has been named an All-Star 17 times, including three All-Star MVP selections. In 2021, he was selected to the NBA 75th Anniversary Team.

James played basketball for St. Vincent–St. Mary High School in his hometown of Akron, Ohio, where he was heavily touted by the national media as a future NBA superstar. A prep-to-pro, he was selected by Cleveland with the first overall pick of the 2003 NBA draft. Named the 2003–04 Rookie of the Year, he soon established himself as one of the league's premier players, winning the NBA MVP Award in 2009 and 2010. After failing to win a championship with Cleveland, James left in 2010 to sign as a free agent with Miami. This move was announced in an ESPN special titled The Decision, and is one of the most controversial free-agent decisions in sports history. James won his first two NBA championships while playing for the Heat in 2012 and 2013; in both of these years, he also earned league MVP and Finals MVP. After his fourth season with the Heat in 2014, James opted out of his contract to re-sign with the Cavaliers. In 2016, he led the Cavaliers to victory over the Golden State Warriors in the NBA Finals by coming back from a 3–1 deficit, delivering the franchise's first championship and ending Cleveland's 52-year professional sports title drought. In 2018, James exercised his contract option to leave the Cavaliers and signed with the Lakers, where he won the 2020 championship and was awarded his fourth Finals MVP. In 2021, James became the first player in NBA history to accumulate $1 billion in earnings as an active player.[3]

Off the court, James has accumulated more wealth and fame from numerous endorsement contracts. He has been featured in books, documentaries (including winning two Sports Emmy Awards as an executive producer), and television commercials. He has hosted the ESPY Awards and Saturday Night Live, and won 19 ESPY Awards himself. He also appeared in films such as Trainwreck and Space Jam: A New Legacy. James has been a part-owner of Liverpool F.C. since 2011, with the club winning the 2018–2019 UEFA Champions League and 2019–2020 Premier League. Having become more involved in philanthropic and activist pursuits later in his career, James's charitable organization, the LeBron James Family Foundation, helped open an elementary school, housing complex, and community center/retail plaza in his hometown of Akron.[4]"
new york city,"New York, often called New York City to distinguish it from New York State, or NYC for short, is the most populous city in the United States. With a 2020 population of 8,804,190 distributed over 300.46 square miles (778.2 km2), New York City is also the most densely populated major city in the United States. Located at the southern tip of the State of New York, the city is the center of the New York metropolitan area, the largest metropolitan area in the world by urban area.[9] With over 20 million people in its metropolitan statistical area and approximately 23 million in its combined statistical area, it is one of the world's most populous megacities. New York City has been described as the cultural, financial, and media capital of the world, significantly influencing commerce, entertainment, research, technology, education, politics, tourism, art, fashion, and sports, and is the most photographed city in the world.[10] Home to the headquarters of the United Nations, New York is an important center for international diplomacy,[11][12] and has sometimes been called the capital of the world.[13][14]

Situated on one of the world's largest natural harbors, New York City is composed of five boroughs, each of which is coextensive with a respective county of the State of New York. The five boroughs—Brooklyn (Kings County), Queens (Queens County), Manhattan (New York County), the Bronx (Bronx County), and Staten Island (Richmond County)—were created when local governments were consolidated into a single municipal entity in 1898.[15] The city and its metropolitan area constitute the premier gateway for legal immigration to the United States. As many as 800 languages are spoken in New York,[16] making it the most linguistically diverse city in the world. New York is home to more than 3.2 million residents born outside the United States, the largest foreign-born population of any city in the world as of 2016.[17][18] As of 2019, the New York metropolitan area is estimated to produce a gross metropolitan product (GMP) of $2.0 trillion. If the New York metropolitan area were a sovereign state, it would have the eighth-largest economy in the world. New York is home to the highest number of billionaires of any city in the world.[19]

New York City traces its origins to a trading post founded on the southern tip of Manhattan Island by Dutch colonists in approximately 1624. The settlement was named New Amsterdam (Dutch: Nieuw Amsterdam) in 1626 and was chartered as a city in 1653. The city came under English control in 1664 and was renamed New York after King Charles II of England granted the lands to his brother, the Duke of York.[20][21] The city was regained by the Dutch in July 1673 and was renamed New Orange for one year and three months; the city has been continuously named New York since November 1674. New York City was the capital of the United States from 1785 until 1790,[22] and has been the largest U.S. city since 1790. The Statue of Liberty greeted millions of immigrants as they came to the U.S. by ship in the late 19th and early 20th centuries, and is a symbol of the U.S. and its ideals of liberty and peace.[23] In the 21st century, New York has emerged as a global node of creativity, entrepreneurship,[24] and environmental sustainability,[25][26] and as a symbol of freedom and cultural diversity.[27] In 2019, New York was voted the greatest city in the world per a survey of over 30,000 people from 48 cities worldwide, citing its cultural diversity.[28]

Many districts and monuments in New York City are major landmarks, including three of the world's ten most visited tourist attractions in 2013.[29] A record 66.6 million tourists visited New York City in 2019. Times Square is the brightly illuminated hub of the Broadway Theater District,[30] one of the world's busiest pedestrian intersections,[29][31] and a major center of the world's entertainment industry.[32] Many of the city's landmarks, skyscrapers, and parks are known around the world. The Empire State Building has become the global standard of reference to describe the height and length of other structures.[33][34][35] Manhattan's real estate market is among the most expensive in the world.[36][37] Providing continuous 24/7 service and contributing to the nickname The City That Never Sleeps, the New York City Subway is the largest single-operator rapid transit system worldwide, with 472 rail stations. The city has over 120 colleges and universities, including Columbia University, New York University, Rockefeller University, and the City University of New York system, which is the largest urban public university system in the United States. Anchored by Wall Street in the Financial District of Lower Manhattan, New York City has been called both the world's leading financial center and the most financially powerful city in the world, and is home to the world's two largest stock exchanges by total market capitalization, the New York Stock Exchange and NASDAQ.[38][39]"
innovative cars,innovative cars
secular democracy,"Secularism is the principle of seeking to conduct human affairs based on secular, naturalistic considerations. It is most commonly defined as the separation of religion from civic affairs and the state -- which in accordance with religious pluralism defines secularism as neutrality (of the state or non-sectarian institution) on issues of religion as opposed to total opposition of religion in the public square as a whole --, while other views may broaden it to a position concerning the need to remove or minimalize the role of religion in any public sphere.[1] The term has a broad range of meanings, and in the most schematic, may encapsulate any stance that promotes the secular in any given context.[2][3] It may connote anticlericalism, atheism, antitheism, naturalism, non-sectarianism, secularity, neutrality on topics of religion, or the (complete) removal of religious symbols from public institutions.[4]

Freedom of religion
show
Concepts
show
Status by country
show
Religious persecution
Religion portal
vte
As a philosophy, secularism seeks to interpret life based on principles derived solely from the material world, without recourse to religion. It shifts the focus from religion towards ""temporal"" and material concerns.[5]

There are distinct traditions of secularism in the West, like the French, Turkish and Anglo-American models, and beyond, as in India,[4] where the emphasis is more on equality before law and state neutrality rather than blanket separation. The purposes and arguments in support of secularism vary widely, ranging from assertions that it is a crucial element of modernization, or that religion and traditional values are backward and divisive, to the claim that it is the only guarantor of free religious exercise."
islamic,"Islam (/ˈɪslɑːm/;[a] Arabic: اَلْإِسْلَامُ‎, romanized: al-’Islām, [ɪsˈlaːm] (About this soundlisten) ""submission [to God]"")[1] is an Abrahamic monotheistic religion teaching that Muhammad is a messenger of God.[2][3] It is the world's second-largest religion with 1.9 billion followers, or 24.9% of the world's population,[4][5] known as Muslims.[6] Muslims make up a majority of the population in 47 countries.[7][8] Islam teaches that God is merciful, all-powerful, and unique,[9] and has guided humanity through prophets, revealed scriptures, and natural signs.[3][10] The primary scriptures of Islam are the Quran, believed to be the verbatim word of God, as well as the teachings and normative examples (called the sunnah, composed of accounts called hadith) of Muhammad (c. 570 – 632 CE).[11]

Muslims believe that Islam is the complete and universal version of a primordial faith that was revealed many times before through prophets such as Adam, Abraham, Moses, and Jesus.[12] Muslims consider the Quran, in Arabic, to be the unaltered and final revelation of God.[13] Like other Abrahamic religions, Islam also teaches a final judgment with the righteous rewarded in paradise and the unrighteous punished in hell.[14] Religious concepts and practices include the Five Pillars of Islam, which are obligatory acts of worship, as well as following Islamic law (sharia), which touches on virtually every aspect of life and society, from banking and welfare to women and the environment.[15][16] The cities of Mecca, Medina and Jerusalem are home to the three holiest sites in Islam.[17]

From a historical point of view, Islam originated in early 7th century CE in the Arabian Peninsula, in Mecca,[18] and by the 8th century, the Umayyad Caliphate extended from Iberia in the west to the Indus River in the east. The Islamic Golden Age refers to the period traditionally dated from the 8th century to the 13th century, during the Abbasid Caliphate, when much of the historically Muslim world was experiencing a scientific, economic, and cultural flourishing.[19][20][21] The expansion of the Muslim world involved various states and caliphates such as the Ottoman Empire, trade, and conversion to Islam by missionary activities (dawah).[22]

Most Muslims are of one of two denominations: Sunni (85–90%)[23] or Shia (10–15%).[24][25][26] Sunni and Shia differences arose from disagreement over the succession to Muhammad and acquired broader political significance, as well as theological and juridical dimensions.[27] About 12% of Muslims live in Indonesia, the most populous Muslim-majority country;[28] 31% live in South Asia,[29] the largest percentage of Muslims in the world;[30] 20% in the Middle East–North Africa, where it is the dominant religion;[31] and 15% in sub-Saharan Africa.[31] Sizable Muslim communities can also be found in the Americas, China, and Europe.[32][33] Islam is the fastest-growing major religion in the world.[34][35]"
teacjers,teachers
bias,bias
maine,"Maine (/meɪn/ (About this soundlisten)) is a state in the New England region of the United States, bordered by New Hampshire to the west; the Atlantic Ocean to the southeast; and the Canadian provinces of New Brunswick and Quebec to the northeast and northwest, respectively. Maine is the 12th-smallest by area, the 9th-least populous, the 13th-least densely populated, and the most rural[13] of the 50 U.S. states. It is also the northeasternmost among the contiguous United States, the northernmost state east of the Great Lakes, the only state whose name consists of a single syllable, and the only state to border only one other US state. The most populous city in Maine is Portland, while its capital is Augusta.

Maine has traditionally been known for its jagged, rocky Atlantic oceanic and bayshore coastlines; smoothly-contoured mountains; heavily forested interior; picturesque waterways; and its wild lowbush blueberries and seafood cuisine, especially lobster and clams. In more recent years, coastal and Down East Maine, especially in the vicinity of Portland, have emerged as an important center for the creative economy,[14] which is also bringing gentrification.[15]

For thousands of years after the glaciers retreated during the last Ice Age, indigenous peoples were the only inhabitants of the territory that is now Maine. At the time of European arrival, several Algonquian-speaking peoples inhabited the area. The first European settlement in the area was by the French in 1604 on Saint Croix Island, founded by Pierre Dugua, Sieur de Mons. The first English settlement was the short-lived Popham Colony, established by the Plymouth Company in 1607. A number of English settlements were established along the coast of Maine in the 1620s, although the rugged climate and conflict with the local indigenous people caused many to fail.

As Maine entered the 18th century, only a half dozen European settlements had survived. Loyalist and Patriot forces contended for Maine's territory during the American Revolution. During the War of 1812, the largely undefended eastern region of Maine was occupied by British forces with the goal of annexing it to Canada via the Colony of New Ireland, but returned to the United States following failed British offensives on the northern border, mid-Atlantic and south which produced a peace treaty that restored the pre-war boundaries. Maine was part of the Commonwealth of Massachusetts until 1820 when it voted to secede from Massachusetts to become a separate state. On March 15, 1820, under the Missouri Compromise, it was admitted to the Union as the 23rd state."
water access in maine,"Maine (/meɪn/ (About this soundlisten)) is a state in the New England region of the United States, bordered by New Hampshire to the west; the Atlantic Ocean to the southeast; and the Canadian provinces of New Brunswick and Quebec to the northeast and northwest, respectively. Maine is the 12th-smallest by area, the 9th-least populous, the 13th-least densely populated, and the most rural[13] of the 50 U.S. states. It is also the northeasternmost among the contiguous United States, the northernmost state east of the Great Lakes, the only state whose name consists of a single syllable, and the only state to border only one other US state. The most populous city in Maine is Portland, while its capital is Augusta.

Maine has traditionally been known for its jagged, rocky Atlantic oceanic and bayshore coastlines; smoothly-contoured mountains; heavily forested interior; picturesque waterways; and its wild lowbush blueberries and seafood cuisine, especially lobster and clams. In more recent years, coastal and Down East Maine, especially in the vicinity of Portland, have emerged as an important center for the creative economy,[14] which is also bringing gentrification.[15]

For thousands of years after the glaciers retreated during the last Ice Age, indigenous peoples were the only inhabitants of the territory that is now Maine. At the time of European arrival, several Algonquian-speaking peoples inhabited the area. The first European settlement in the area was by the French in 1604 on Saint Croix Island, founded by Pierre Dugua, Sieur de Mons. The first English settlement was the short-lived Popham Colony, established by the Plymouth Company in 1607. A number of English settlements were established along the coast of Maine in the 1620s, although the rugged climate and conflict with the local indigenous people caused many to fail.

As Maine entered the 18th century, only a half dozen European settlements had survived. Loyalist and Patriot forces contended for Maine's territory during the American Revolution. During the War of 1812, the largely undefended eastern region of Maine was occupied by British forces with the goal of annexing it to Canada via the Colony of New Ireland, but returned to the United States following failed British offensives on the northern border, mid-Atlantic and south which produced a peace treaty that restored the pre-war boundaries. Maine was part of the Commonwealth of Massachusetts until 1820 when it voted to secede from Massachusetts to become a separate state. On March 15, 1820, under the Missouri Compromise, it was admitted to the Union as the 23rd state."
motivation,motivation
difficulty of getting a job,"Unemployment generally falls during periods of economic prosperity and rises during recessions, creating significant pressure on public finances as tax revenue falls and social safety net costs increase. Government spending and taxation decisions (fiscal policy) and U.S. Federal Reserve interest rate adjustments (monetary policy) are important tools for managing the unemployment rate. There may be an economic trade-off between unemployment and inflation, as policies designed to reduce unemployment can create inflationary pressure, and vice versa. The U.S. Federal Reserve (the Fed) has a dual mandate to achieve full employment while maintaining a low rate of inflation. The major political parties debate appropriate solutions for improving the job creation rate, with liberals arguing for more government spending and conservatives arguing for lower taxes and less regulation. Polls indicate that Americans believe job creation is the most important government priority, with not sending jobs overseas the primary solution.[3]

Unemployment can be measured in several ways. A person is defined as unemployed in the United States if they are jobless, but have looked for work in the last four weeks and are available for work. People who are neither employed nor defined as unemployed are not included in the labor force calculation. For example, as of September 2017, the unemployment rate (formally defined as the ""U-3"" rate) in the United States was 4.2% representing 6.8 million unemployed people.[4][5] The unemployment rate was calculated by dividing the number of unemployed by the number in the civilian labor force (age 16+, non-military and not incarcerated) of approximately 159.6 million people,[6] relative to a U.S. population of approximately 326 million people.[7] The historical average unemployment rate (January 1948-September 2020) is 5.8%.[4] The government's broader U-6 unemployment rate, which includes the part-time underemployed was 8.3% in September 2017.[8][9] Both of these rates fell steadily from 2010 to 2019; the U-3 rate was below the November 2007 level that preceded the Great Recession by November 2016, while the U-6 rate did not fully recover until August 2017.[4][8]

The U.S. Bureau of Labor Statistics (BLS) publishes a monthly ""Employment Situation Summary"" with key statistics and commentary.[10] As of June 2018, approximately 128.6 million people in the United States have found full-time work (at least 35 hours a week in total), while 27.0 million worked part-time.[11] There were 4.7 million working part-time for economic reasons, meaning they wanted but could not find full-time work, the lowest level since January 2008.[12]

The vast majority of persons outside the civilian labor force (age 16+) are there by choice. The BLS reported that in July 2018, there were 94.1 million persons age 16+ outside the labor force. Of these, 88.6 million (94%) did not want a job while 5.5 million (6%) wanted a job.[13] Key reasons persons age 16+ are outside the labor force include retired, disabled or illness, attending school, and caregiving.[14] The Congressional Budget Office reported that as of December 2017, the primary reason for men age 25–54 to be outside the labor force was illness/disability (50% or 3.5 million), while the primary reason for women was due to family care-giving (60% or 9.6 million).[15]

The Congressional Budget Office estimated that the U.S. was approximately 2.5 million workers below full employment as of the end of 2015 and 1.6 million at December 31, 2016, mainly due to lower labor force participation. This was very close to full employment, indicating a strong economy.[16] As of May 2018, there were more job openings (6.6 million) than people defined as unemployed (6.0 million) in the U.S.[17][18][19]

In September 2019, the U.S. unemployment rate dropped to 3.5%, near the lowest rate in 50 years.[20] On May 8, 2020, the Bureau of Labor Statistics reported that 20.5 million nonfarm jobs were lost and the unemployment rate rose to 14.7 percent in April, due to the Coronavirus pandemic in the United States.[21]"
un,"The United Nations (UN) is an intergovernmental organization aiming to maintain international peace and security, develop friendly relations among nations, achieve international cooperation, and be a centre for harmonizing the actions of nations.[2] It is the world's largest and most familiar international organization.[3] The UN is headquartered on international territory in New York City and has other main offices in Geneva, Nairobi, Vienna, and The Hague.

The UN was established after World War II with the aim of preventing future wars, succeeding the ineffective League of Nations.[4] On 25 April 1945, 50 governments met in San Francisco for a conference and started drafting the UN Charter, which was adopted on 25 June 1945 and took effect on 24 October 1945, when the UN began operations. Pursuant to the Charter, the organization's objectives include maintaining international peace and security, protecting human rights, delivering humanitarian aid, promoting sustainable development, and upholding international law.[5] At its founding, the UN had 51 member states; with the addition of South Sudan in 2011, membership is now 193, representing almost all of the world's sovereign states.[6]

The organization's mission to preserve world peace was complicated in its early decades by the Cold War between the United States and Soviet Union and their respective allies. Its missions have consisted primarily of unarmed military observers and lightly armed troops with primarily monitoring, reporting and confidence-building roles.[7] UN membership grew significantly following widespread decolonization beginning in the 1960s. Since then, 80 former colonies have gained independence, including 11 trust territories that had been monitored by the Trusteeship Council.[8] By the 1970s, the UN's budget for economic and social development programmes far outstripped its spending on peacekeeping. After the end of the Cold War, the UN shifted and expanded its field operations, undertaking a wide variety of complex tasks.[9]

The UN has six principal organs: the General Assembly; the Security Council; the Economic and Social Council (ECOSOC); the Trusteeship Council; the International Court of Justice; and the UN Secretariat. The UN System includes a multitude of specialized agencies, funds and programmes such as the World Bank Group, the World Health Organization, the World Food Programme, UNESCO, and UNICEF. Additionally, non-governmental organizations may be granted consultative status with ECOSOC and other agencies to participate in the UN's work.

The UN's chief administrative officer is the Secretary-General, currently Portuguese politician and diplomat António Guterres, who began his first five year-term on 1 January 2017 and was re-elected on 8 June 2021. The organization is financed by assessed and voluntary contributions from its member states.

The UN, its officers, and its agencies have won many Nobel Peace Prizes, though other evaluations of its effectiveness have been mixed. Some commentators believe the organization to be an important force for peace and human development, while others have called it ineffective, biased, or corrupt."
cons of home schooling,"Homeschooling or home schooling, also known as home education or elective home education (EHE), is the education of school-aged children at home or a variety of places other than school.[1] Usually conducted by a parent, tutor, or an online teacher, many homeschool families use less formal, more personalized and individualized methods of learning that are not always found in schools. The actual practice of homeschooling can look very different. The spectrum ranges from highly structured forms based on traditional school lessons to more open, free forms such as unschooling, which is a lesson- and curriculum-free implementation of homeschooling. Some families who initially attended a school go through a deschool phase to break away from school habits and prepare for homeschooling. While ""homeschooling"" is the term commonly used in North America, ""home education"" is primarily used in Europe and many Commonwealth countries. Homeschooling shouldn't be confused with distance education, which generally refers to the arrangement where the student is educated by and conforms to the requirements of an online school, rather than being educated independently and unrestrictedly by their parents or by themselves.

Before the introduction of compulsory school attendance laws, most childhood education was done by families and local communities. By the early 19th century, attending a school became the most common means of education in the developed world. In the mid to late 20th century, more people began questioning the efficiency and sustainability of school learning, which again led to an increase in the number of homeschoolers, especially in the Americas and some European countries. Today, homeschooling is a relatively widespread form of education and a legal alternative to public and private schools in many countries, which many people believe is due to the rise of the Internet, which enables people to obtain information very quickly. There are also nations in which homeschooling is regulated or illegal, as recorded in the article Homeschooling international status and statistics. During the COVID-19 pandemic, many students from all over the world had to study from home due to the danger posed by the virus. However, this was mostly implemented in the form of distance education rather than traditional homeschooling.

There are many different reasons for homeschooling, ranging from personal interests to dissatisfaction with the public school system. Some parents see better educational opportunities for their child in homeschooling, for example because they know their child more accurately than a teacher and can concentrate fully on educating usually one to a few persons and therefore can respond more precisely to their individual strengths and weaknesses, or because they think that they can better prepare their children for the life outside of school. Some children can also learn better at home, for example, because they are not held back, disturbed or distracted from school matters, do not feel underchallenged or overwhelmed with certain topics, find that certain temperaments are encouraged in school, while others are inhibited, do not cope well with the very predetermined structure in school or are bullied there. Homeschooling is also an option for families living in remote rural areas, those temporarily abroad, those who travel frequently and therefore face the physical impossibility or difficulty of getting their children into school and families who want to spend more and better time with their children. Health reasons and special needs can also play a role in why children cannot attend a school regularly and are at least partially homeschooled.

Critics of homeschooling argue that children may lack social contact at home, possibly resulting in children having poorer social skills. Some are also concerned that some parents may not have the skills required to guide and advise their children in life skills. Critics also say that a child might not encounter people of other cultures, worldviews, and socioeconomic groups if they are not enrolled in a school. Therefore, these critics believe that homeschooling cannot guarantee a comprehensive and neutral education and children can be indoctrinated and manipulated when there is no external influence and surveillance by controlling authorities. There are many studies that show that homeschooled children score better on standardized tests and have equal or higher developed social skills and participate more in cultural and family activities on average than public school students.[2][3] In addition, studies suggest that homeschoolers are generally more likely to have higher self-esteem, deeper friendships, and better relationships with adults, and are less susceptible to peer pressure.[4][3]"
con of home schooling,"Homeschooling or home schooling, also known as home education or elective home education (EHE), is the education of school-aged children at home or a variety of places other than school.[1] Usually conducted by a parent, tutor, or an online teacher, many homeschool families use less formal, more personalized and individualized methods of learning that are not always found in schools. The actual practice of homeschooling can look very different. The spectrum ranges from highly structured forms based on traditional school lessons to more open, free forms such as unschooling, which is a lesson- and curriculum-free implementation of homeschooling. Some families who initially attended a school go through a deschool phase to break away from school habits and prepare for homeschooling. While ""homeschooling"" is the term commonly used in North America, ""home education"" is primarily used in Europe and many Commonwealth countries. Homeschooling shouldn't be confused with distance education, which generally refers to the arrangement where the student is educated by and conforms to the requirements of an online school, rather than being educated independently and unrestrictedly by their parents or by themselves.

Before the introduction of compulsory school attendance laws, most childhood education was done by families and local communities. By the early 19th century, attending a school became the most common means of education in the developed world. In the mid to late 20th century, more people began questioning the efficiency and sustainability of school learning, which again led to an increase in the number of homeschoolers, especially in the Americas and some European countries. Today, homeschooling is a relatively widespread form of education and a legal alternative to public and private schools in many countries, which many people believe is due to the rise of the Internet, which enables people to obtain information very quickly. There are also nations in which homeschooling is regulated or illegal, as recorded in the article Homeschooling international status and statistics. During the COVID-19 pandemic, many students from all over the world had to study from home due to the danger posed by the virus. However, this was mostly implemented in the form of distance education rather than traditional homeschooling.

There are many different reasons for homeschooling, ranging from personal interests to dissatisfaction with the public school system. Some parents see better educational opportunities for their child in homeschooling, for example because they know their child more accurately than a teacher and can concentrate fully on educating usually one to a few persons and therefore can respond more precisely to their individual strengths and weaknesses, or because they think that they can better prepare their children for the life outside of school. Some children can also learn better at home, for example, because they are not held back, disturbed or distracted from school matters, do not feel underchallenged or overwhelmed with certain topics, find that certain temperaments are encouraged in school, while others are inhibited, do not cope well with the very predetermined structure in school or are bullied there. Homeschooling is also an option for families living in remote rural areas, those temporarily abroad, those who travel frequently and therefore face the physical impossibility or difficulty of getting their children into school and families who want to spend more and better time with their children. Health reasons and special needs can also play a role in why children cannot attend a school regularly and are at least partially homeschooled.

Critics of homeschooling argue that children may lack social contact at home, possibly resulting in children having poorer social skills. Some are also concerned that some parents may not have the skills required to guide and advise their children in life skills. Critics also say that a child might not encounter people of other cultures, worldviews, and socioeconomic groups if they are not enrolled in a school. Therefore, these critics believe that homeschooling cannot guarantee a comprehensive and neutral education and children can be indoctrinated and manipulated when there is no external influence and surveillance by controlling authorities. There are many studies that show that homeschooled children score better on standardized tests and have equal or higher developed social skills and participate more in cultural and family activities on average than public school students.[2][3] In addition, studies suggest that homeschoolers are generally more likely to have higher self-esteem, deeper friendships, and better relationships with adults, and are less susceptible to peer pressure.[4][3]"
educational problems,educational problems
fashion,fashion
social media influencing fashion decisions,social media influencing fashion decisions
online medical information,online medical information
prison,prison
favoratism of cities for high earners,"There are wide varieties of economic inequality, most notably measured using the distribution of income (the amount of money people are paid) and the distribution of wealth (the amount of wealth people own). Besides economic inequality between countries or states, there are important types of economic inequality between different groups of people.[2]

Important types of economic measurements focus on wealth, income, and consumption. There are many methods for measuring economic inequality,[3] the Gini coefficient being a widely used one. Another type of measure is the Inequality-adjusted Human Development Index, which is a statistic composite index that takes inequality into account.[4] Important concepts of equality include equity, equality of outcome, and equality of opportunity.

Research suggests that greater inequality hinders economic growth, and that land and human capital inequality reduce growth more than inequality of income.[5] Whereas globalization has reduced global inequality (between nations), it has increased inequality within nations.[6][7][8] Research has generally linked economic inequality to political instability, including revolution,[9] democratic breakdown[10] and civil conflict.[11]

Global income inequality peaked approximately in the 1970s, when world income was distributed bimodally into ""rich"" and ""poor"" countries with little overlap. Since then, inequality has been rapidly decreasing, and this trend seems to be accelerating. Income distribution is now unimodal, with most people living in middle-income countries.[12]"
intercollegiate sports,The term “intercollegiate sport” means a sport played at the collegiate level for which eligibility requirements for participation by a student athlete are established by a national association for the promotion or regulation of college athletics.
editors of the times,"The New York Times (N.Y.T. or N.Y. Times) is an American daily newspaper based in New York City with a worldwide readership.[7][8] Founded in 1851, the Times has since won 132 Pulitzer Prizes (the most of any newspaper),[9] and has long been regarded within the industry as a national ""newspaper of record"".[10] It is ranked 18th in the world by circulation and 3rd in the U.S.[11]

The paper is owned by The New York Times Company, which is publicly traded. It has been governed by the Sulzberger family since 1896, through a dual-class share structure after its shares became publicly traded.[12] A. G. Sulzberger and his father, Arthur Ochs Sulzberger Jr.—the paper's publisher and the company's chairman, respectively—are the fifth and fourth generation of the family to head the paper.[13]

Since the mid-1970s, The New York Times has expanded its layout and organization, adding special weekly sections on various topics supplementing the regular news, editorials, sports, and features. Since 2008,[14] the Times has been organized into the following sections: News, Editorials/Opinions-Columns/Op-Ed, New York (metropolitan), Business, Sports, Arts, Science, Styles, Home, Travel, and other features.[15] On Sundays, the Times is supplemented by the Sunday Review (formerly the Week in Review),[16] The New York Times Book Review,[17] The New York Times Magazine,[18] and T: The New York Times Style Magazine.[19]"
balanced diet,"A healthy diet is a diet that helps maintain or improve overall health. A healthy diet provides the body with essential nutrition: fluid, macronutrients, micronutrients, and adequate food energy.[2][3]

A healthy diet may contain fruits, vegetables, and whole grains, and may include little to no processed food or sweetened beverages. The requirements for a healthy diet can be met from a variety of plant-based and animal-based foods, although a non-plant source of vitamin B12 is needed for those following a vegan diet.[4] Various nutrition guides are published by medical and governmental institutions to educate individuals on what they should be eating to be healthy. Nutrition facts labels are also mandatory in some countries to allow consumers to choose between foods based on the components relevant to health.[5][6]"
healthy diet,"A healthy diet is a diet that helps maintain or improve overall health. A healthy diet provides the body with essential nutrition: fluid, macronutrients, micronutrients, and adequate food energy.[2][3]

A healthy diet may contain fruits, vegetables, and whole grains, and may include little to no processed food or sweetened beverages. The requirements for a healthy diet can be met from a variety of plant-based and animal-based foods, although a non-plant source of vitamin B12 is needed for those following a vegan diet.[4] Various nutrition guides are published by medical and governmental institutions to educate individuals on what they should be eating to be healthy. Nutrition facts labels are also mandatory in some countries to allow consumers to choose between foods based on the components relevant to health.[5][6]"
well balanced diet,"A healthy diet is a diet that helps maintain or improve overall health. A healthy diet provides the body with essential nutrition: fluid, macronutrients, micronutrients, and adequate food energy.[2][3]

A healthy diet may contain fruits, vegetables, and whole grains, and may include little to no processed food or sweetened beverages. The requirements for a healthy diet can be met from a variety of plant-based and animal-based foods, although a non-plant source of vitamin B12 is needed for those following a vegan diet.[4] Various nutrition guides are published by medical and governmental institutions to educate individuals on what they should be eating to be healthy. Nutrition facts labels are also mandatory in some countries to allow consumers to choose between foods based on the components relevant to health.[5][6]"
nuclear agreement,"The Joint Comprehensive Plan of Action (JCPOA; Persian: برنامه جامع اقدام مشترک‎, romanized: barnāmeye jāme'e eqdāme moshtarak (برجام, BARJAM)),[4][5] known commonly as the Iran nuclear deal or Iran deal, is an agreement on the Iranian nuclear program reached in Vienna on 14 July 2015, between Iran and the P5+1 (the five permanent members of the United Nations Security Council—China, France, Russia, United Kingdom, United States—plus Germany)[a] together with the European Union.

Formal negotiations toward JCPOA began with the adoption of the Joint Plan of Action, an interim agreement signed between Iran and the P5+1 countries in November 2013. Iran and the P5+1 countries engaged in negotiations for the next 20 months and, in April 2015, agreed on a framework for the final agreement. In July 2015, Iran and the P5+1 confirmed agreement on the plan, along with the ""Roadmap Agreement"" between Iran and the IAEA.[8]"
getting rid of the humanities,getting rid of the humanities
marjiuana use,"Marijuana—which can also be called cannabis, weed, pot, or dope—refers to the dried flowers, leaves, stems, and seeds of the cannabis plant. The cannabis plant contains more than 100 compounds (or cannabinoids). These compounds include tetrahydrocannabinol (THC), which is impairing or mind-altering, as well as other active compounds, such as cannabidiol (CBD). CBD is not impairing, meaning it does not cause a “high”.1

Marijuana is the most commonly used federally illegal drug in the United States, with an estimated 48.2 million people using it in 2019.2 Marijuana use may have a wide range of health effects on the body and brain. Click on the sections below to learn more about how marijuana use can affect your health."
extreme paretning,"The definition of extreme parenting can be somewhat subjective and open to debate. Generally, parenting tactics and principles that swim upstream and fly in the face of convention qualify for extreme parenting. If you don’t like following the rules, preferring to make your own goals and guidelines, you may be an extreme parent, too. The drive for children to succeed academically can be a high priority for some parents. Toward this end, a parent might exert extreme pressure on the child, with punishments and loss of privileges if the child does not meet expectations. Results of these high academic standards can be discipline, ethics, confidence and intelligence, states C. Cindy Fan, UCLA associate dean of social sciences. However, high academic expectations and rigid scheduling may also cause undue stress for children. Setting more realistic expectations for a child's academic performance may produce better academic results, states Sandra L. Christenson, Ph.D. and Cathryn Peterson, teacher, with the University of Minnesota Extension."
nation,nation
underemployment,"Underemployment is the underuse of a worker because a job does not use the worker's skills, is part-time, or leaves the worker idle.[2] Examples include holding a part-time job despite desiring full-time work, and overqualification, in which the employee has education, experience, or skills beyond the requirements of the job.[3][4]

Underemployment has been studied from a variety of perspectives, including economics, management, psychology, and sociology. In economics, for example, the term underemployment has three different distinct meanings and applications. All of the meanings involve a situation in which a person is working, unlike unemployment, where a person who is searching for work cannot find a job. All meanings involve under-utilization of labor which is missed by most official (governmental agency) definitions and measurements of unemployment.

In economics, underemployment can refer to:

""Overqualification"", or ""overeducation"", or the employment of workers with high education, skill levels, or experience in jobs that do not require such abilities.[5] For example, a trained medical doctor with a foreign credential who works as a taxi driver would experience this type of underemployment.
""Involuntary part-time"" work, where workers who could (and would like to) be working for a full work-week can only find part-time work. By extension, the term is also used in regional planning to describe regions where economic activity rates are unusually low, due to a lack of job opportunities, training opportunities, or due to a lack of services such as childcare and public transportation.
""Overstaffing"" or ""hidden unemployment"" or ""disguised unemployment"" (also called ""labor hoarding""[6]), the practice in which businesses or entire economies employ workers who are not fully occupied; for example, workers currently not being used to produce goods or services due to legal or social restrictions or because the work is highly seasonal.
Underemployment is a significant cause of poverty because although the worker may be able to find part-time work, the part-time pay may not be sufficient for basic needs. Underemployment is a problem particularly in developing countries, where the unemployment rate is often quite low, as most workers are doing subsistence work or occasional part-time jobs. The global average of full-time workers per adult population is only 26%, compared to 30–52% in developed countries and 5–20% in most of Africa.[7]"
myriad's patents should only cover their procedures not chemicals,myriad's patents should only cover their procedures not chemicals
brca chemicals proprietary,"What Is BRCA?

The name “BRCA” is an abbreviation for “BReast CAncer gene.” BRCA1 and BRCA2 are two different genes that have been found to impact a person’s chances of developing breast cancer.

Every human has both the BRCA1 and BRCA2 genes. Despite what their names might suggest, BRCA genes do not cause breast cancer. In fact, these genes normally play a big role in preventing breast cancer. They help repair DNA breaks that can lead to cancer and the uncontrolled growth of tumors. Because of this, the BRCA genes are known as tumor suppressor genes.

However, in some people these tumor suppression genes do not work properly. When a gene becomes altered or broken, it doesn’t function correctly. This is called a gene mutation."
dictator,"A dictator is a political leader who possesses absolute power. A dictatorship is a state ruled by one dictator or by a small clique.[2] The word originated as the title of a Roman dictator elected by the Roman Senate to rule the republic in times of emergency (see Roman dictator and justitium).[3]

Like the term ""tyrant"", and to a lesser degree ""autocrat"", ""dictator"" came to be used almost exclusively as a non-titular term for oppressive rule. In modern usage the term ""dictator"" is generally used to describe a leader who holds or abuses an extraordinary amount of personal power. Dictatorships are often characterised by some of the following: suspension of elections and civil liberties; proclamation of a state of emergency; rule by decree; repression of political opponents; not abiding by the rule of law procedures, and cult of personality. Dictatorships are often one-party or dominant-party states.[4][5]

A wide variety of leaders coming to power in different kinds of regimes, such as one-party states, dominant-party states, and civilian governments under a personal rule, have been described as dictators. They may hold left or right-wing views."
responsibility,responsibility
gay,gay
affirmative action by race,"Affirmative action in the United States is a set of laws, policies, guidelines, and administrative practices ""intended to end and correct the effects of a specific form of discrimination""[1] that include government-mandated, government-approved, and voluntary private programs. The programs tend to focus on access to education and employment, granting special consideration to historically excluded groups, specifically racial minorities or women.[1][2] The impetus toward affirmative action is redressing the disadvantages[3][4][5][6][7] associated with past and present discrimination.[8] Further impetus is a desire to ensure public institutions, such as universities, hospitals, and police forces, are more representative of the populations they serve.[9]

In the United States, affirmative action included the use of racial quotas until the Supreme Court ruled that quotas were unconstitutional.[10] Affirmative action currently tends to emphasize not specific quotas but rather ""targeted goals"" to address past discrimination in a particular institution or in broader society through ""good-faith efforts ... to identify, select, and train potentially qualified minorities and women.""[1][11] For example, many higher education institutions have voluntarily adopted policies which seek to increase recruitment of racial minorities.[12] Outreach campaigns, targeted recruitment, employee and management development, and employee support programs are examples of affirmative action in employment.[13] Nine states in the United States have banned affirmative action: California (1996), Washington (1998), Florida (1999), Michigan (2006), Nebraska (2008), Arizona (2010), New Hampshire (2012), Oklahoma (2012), and Idaho (2020). Florida's ban was via an executive order and New Hampshire and Idaho's bans were passed by the legislature. The other six bans were approved at the ballot.[14] The 1996 Hopwood v. Texas decision effectively barred affirmative action in the three states within the United States Court of Appeals for the Fifth Circuit—Louisiana, Mississippi, and Texas—until Grutter v. Bollinger abrogated it in 2003.[15]

Affirmative action policies were developed to address long histories of discrimination faced by minorities and women, which reports suggest produced corresponding unfair advantages for whites and males.[16][17] They first emerged from debates over non-discrimination policies in the 1940s and during the civil rights movement.[18] These debates led to federal executive orders requiring non-discrimination in the employment policies of some government agencies and contractors in the 1940s and onward, and to Title VII of the Civil Rights Act of 1964 which prohibited racial discrimination in firms with over 25 employees. The first federal policy of race-conscious affirmative action was the Revised Philadelphia Plan, implemented in 1969, which required certain government contractors to set ""goals and timetables"" for integrating and diversifying their workforce. Similar policies emerged through a mix of voluntary practices and federal and state policies in employment and education. Affirmative action as a practice was partially upheld by the Supreme Court in Grutter v. Bollinger (2003), while the use of racial quotas for college admissions was concurrently ruled unconstitutional by the Court in Gratz v. Bollinger (2003).

Affirmative action often gives rise to controversy in American politics. Supporters argue that affirmative action is still needed to counteract continuing bias and prejudice against women and minorities. Opponents argue that these policies constitute reverse racism and/or amount to discrimination against other minorities, such as Asian Americans, which entails favoring one group over another based upon racial preference rather than achievement, and many believe that the diversity of current American society suggests that affirmative action policies succeeded and are no longer required.[19] Supporters point to contemporary examples of conscious and unconscious biases, such as the finding that job-seekers with African American sounding names may be less likely to get a callback than those with white-sounding names, as proof that affirmative action is not obsolete.[11]"
b-12,"Vitamin B12, also known as cobalamin, is a water-soluble vitamin involved in metabolism.[2][3] It is one of eight B vitamins. It is a cofactor in DNA synthesis, in both fatty acid and amino acid metabolism.[4] It is important in the normal functioning of the nervous system via its role in the synthesis of myelin,[3][5] and in the maturation of red blood cells in the bone marrow.[6]

The most common cause of vitamin B12 deficiency in developed countries is impaired absorption due to a loss of gastric intrinsic factor (IF) which must be bound to a food-source of B12 in order for absorption to occur.[7] A second major cause is age-related decline in stomach acid production (achlorhydria), because acid exposure frees protein-bound vitamin.[8] For the same reason, people on long-term antacid therapy, using proton-pump inhibitors, H2 blockers or other antacids are at increased risk.[9] Deficiency may be characterized by limb neuropathy or a blood disorder called pernicious anemia, a type of megaloblastic anemia. Folate levels in the individual may affect the course of pathological changes and symptomatology of vitamin B12 deficiency."
b12 vitamins,"Vitamin B12, also known as cobalamin, is a water-soluble vitamin involved in metabolism.[2][3] It is one of eight B vitamins. It is a cofactor in DNA synthesis, in both fatty acid and amino acid metabolism.[4] It is important in the normal functioning of the nervous system via its role in the synthesis of myelin,[3][5] and in the maturation of red blood cells in the bone marrow.[6]

The most common cause of vitamin B12 deficiency in developed countries is impaired absorption due to a loss of gastric intrinsic factor (IF) which must be bound to a food-source of B12 in order for absorption to occur.[7] A second major cause is age-related decline in stomach acid production (achlorhydria), because acid exposure frees protein-bound vitamin.[8] For the same reason, people on long-term antacid therapy, using proton-pump inhibitors, H2 blockers or other antacids are at increased risk.[9] Deficiency may be characterized by limb neuropathy or a blood disorder called pernicious anemia, a type of megaloblastic anemia. Folate levels in the individual may affect the course of pathological changes and symptomatology of vitamin B12 deficiency."
b-12 vitamins,"Vitamin B12, also known as cobalamin, is a water-soluble vitamin involved in metabolism.[2][3] It is one of eight B vitamins. It is a cofactor in DNA synthesis, in both fatty acid and amino acid metabolism.[4] It is important in the normal functioning of the nervous system via its role in the synthesis of myelin,[3][5] and in the maturation of red blood cells in the bone marrow.[6]

The most common cause of vitamin B12 deficiency in developed countries is impaired absorption due to a loss of gastric intrinsic factor (IF) which must be bound to a food-source of B12 in order for absorption to occur.[7] A second major cause is age-related decline in stomach acid production (achlorhydria), because acid exposure frees protein-bound vitamin.[8] For the same reason, people on long-term antacid therapy, using proton-pump inhibitors, H2 blockers or other antacids are at increased risk.[9] Deficiency may be characterized by limb neuropathy or a blood disorder called pernicious anemia, a type of megaloblastic anemia. Folate levels in the individual may affect the course of pathological changes and symptomatology of vitamin B12 deficiency."
ethical standards,"Ethics or moral philosophy is a branch[1] of philosophy that ""involves systematizing, defending, and recommending concepts of right and wrong behavior"".[2] The field of ethics, along with aesthetics, concerns matters of value; these fields comprise the branch of philosophy called axiology.[3]

Ethics seeks to resolve questions of human morality by defining concepts such as good and evil, right and wrong, virtue and vice, justice and crime. As a field of intellectual inquiry, moral philosophy is related to the fields of moral psychology, descriptive ethics, and value theory.

Three major areas of study within ethics recognized today are:[2]

Meta-ethics, concerning the theoretical meaning and reference of moral propositions, and how their truth values (if any) can be determined;
Normative ethics, concerning the practical means of determining a moral course of action;
Applied ethics, concerning what a person is obligated (or permitted) to do in a specific situation or a particular domain of action.[2]"
animal products,"An animal product is any material derived from the body of an animal.[1] Examples are fat, flesh, blood, milk, eggs, and lesser known products, such as isinglass and rennet.[2]

Animal by-products, as defined by the USDA, are products harvested or manufactured from livestock other than muscle meat.[3] In the EU, animal by-products (ABPs) are defined somewhat more broadly, as materials from animals that people do not consume.[4] Thus, chicken eggs for human consumption are considered by-products in the US but not France; whereas eggs destined for animal feed are classified as animal by-products in both countries. This does not in itself reflect on the condition, safety, or wholesomeness of the product.

Animal by-products are carcasses and parts of carcasses from slaughterhouses, animal shelters, zoos and veterinarians, and products of animal origin not intended for human consumption, including catering waste. These products may go through a process known as rendering to be made into human and non-human foodstuffs, fats, and other material that can be sold to make commercial products such as cosmetics, paint, cleaners, polishes, glue, soap and ink. The sale of animal by-products allows the meat industry to compete economically with industries selling sources of vegetable protein.[5]

The word animals includes all species in the biological kingdom animalia. For example, insects, shrimp, and oysters are animals.

Generally, products made from fossilized or decomposed animals, such as petroleum formed from the ancient remains of marine animals are not considered animal products. Crops grown in soil fertilized with animal remains are rarely characterized as animal products.

Several popular diet patterns prohibit the inclusion of some categories of animal products and may also limit the conditions of when other animal products may be permitted. This includes but not limited to secular diets; like, vegetarian, pescetarian, and paleolithic diets, as well as religious diets, such as kosher, halal, mahayana, macrobiotic, and sattvic diets. Other diets, such as vegan-vegetarian diets and all its subsets exclude any material of animal origin.[6] Scholarly, the term animal source foods (ASFs) has been used to refer to refer to these animal products and byproducts collectively.[7]

In international trade legislation, the terminology products of animal origin (POAO) is used for referring to foods and goods that are derived from animals or have close relation to them.[8]"
children's diet,children's diet
organic farmers,"Organic farming is an agricultural system that uses fertilizers of organic origin such as compost manure, green manure, and bone meal and places emphasis on techniques such as crop rotation and companion planting. It originated early in the 20th century in reaction to rapidly changing farming practices. Certified organic agriculture accounts for 70 million hectares globally, with over half of that total in Australia.[1] Organic farming continues to be developed by various organizations today. Biological pest control, mixed cropping and the fostering of insect predators are encouraged. Organic standards are designed to allow the use of naturally-occurring substances while prohibiting or strictly limiting synthetic substances.[2] For instance, naturally-occurring pesticides such as pyrethrin and rotenone are permitted, while synthetic fertilizers and pesticides are generally prohibited. Synthetic substances that are allowed include, for example, copper sulfate, elemental sulfur and Ivermectin. Genetically modified organisms, nanomaterials, human sewage sludge, plant growth regulators, hormones, and antibiotic use in livestock husbandry are prohibited.[3][4] Organic farming advocates claim advantages in sustainability,[5][6] openness, self-sufficiency, autonomy and independence,[6] health, food security, and food safety.

Organic agricultural methods are internationally regulated and legally enforced by many nations, based in large part on the standards set by the International Federation of Organic Agriculture Movements (IFOAM), an international umbrella organization for organic farming organizations established in 1972.[7] Organic agriculture can be defined as ""an integrated farming system that strives for sustainability, the enhancement of soil fertility and biological diversity while, with rare exceptions, prohibiting synthetic pesticides, antibiotics, synthetic fertilizers, genetically modified organisms, and growth hormones"".[8][9][10][11]

Since 1990, the market for organic food and other products has grown rapidly, reaching $63 billion worldwide in 2012.[12]: 25  This demand has driven a similar increase in organically-managed farmland that grew from 2001 to 2011 at a compounding rate of 8.9% per annum.[13]
As of 2019, approximately 72,300,000 hectares (179,000,000 acres) worldwide were farmed organically, representing approximately 1.5 percent of total world farmland.[14]"
xenphobia,"Xenophobia (from Ancient Greek ξένος (xénos) 'strange, foreign, alien', and φόβος (phóbos) 'fear')[1] is the fear or hatred of that which is perceived to be foreign or strange.[2][3][4] It is an expression of perceived conflict between an ingroup and an outgroup and may manifest in suspicion by the one of the other's activities, a desire to eliminate their presence, and fear of losing national, ethnic, or racial identity.[5][6]"
editorial,editorial
news colunm,news column
maureen dowd's column,"Dowd became a columnist on The New York Times op-ed page in 1995,[8][3] replacing Anna Quindlen.[5] [10] Dowd was named a Woman of the Year by Glamour magazine in 1996,[3] and won the 1999 Pulitzer Prize, for distinguished commentary.[8] She won The Damon Runyon Award for outstanding contributions to journalism in 2000,[11] and became the first Mary Alice Davis Lectureship speaker (sponsored by the School of Journalism and the Center for American History) at the University of Texas at Austin in 2005.[12] In 2010, Dowd was ranked #43 on The Daily Telegraph's list of the 100 most influential liberals in America; in 2007, she was ranked #37 on the same list.[13]

Dowd's columns have been described as letters to her mother, whom friends credit as ""the source, the fountain of Maureen's humor and her Irish sensibilities and her intellectual take.""[4] Dowd herself has said, ""She is in my head in the sense that I want to inform and amuse the reader.""[14] Dowd's columns are distinguished by an acerbic, often polemical writing style.[15] Her columns display a critical and irreverent attitude towards powerful, mostly political, figures such as former Presidents George W. Bush and Bill Clinton. Dowd also tends to refer to her subjects by nicknames. For example, she has often referred to Bush as ""W."" and former Vice President Dick Cheney as ""Big Time"".[16] She has called former President Barack Obama ""Spock""[17] and ""Barry"". Dowd's interest in candidates' personalities earned her criticism early in her career: ""She focuses too much on the person but not enough on policy.""[4]

Dowd, who perceives her columns to be an exploration of politics, Hollywood, and gender-related topics, often uses popular culture to support and metaphorically enhance her political commentary.[15] In a Times video debate, she said of the North Korean government, ""... you could look at a movie like Mean Girls and figure out the way these North Koreans are reacting; you know it's like high school girls with nuclear weapons—they just want some attention from us, you know?""[18]

Dowd's columns have also been described as often being political cartoons that capture a caricatured view of the current political landscape with precision and exaggeration.[4] For example, in the run-up to the 2000 presidential election Dowd wrote that Democratic candidate ""Al Gore is so feminized and diversified and ecologically correct that he's practically lactating,""[19] while referring to the Democratic party as the ""mommy party"".[4] In a Fresh Dialogues interview years later she said,

""I was just teasing him a little bit because he was so earnest and he could be a little righteous and self important. That's not always the most effective way to communicate your ideas, even if the ideas themselves are right. I mean, certainly his ideas were right but he himself was – sometimes – a pompous messenger for them.""[14]
In January 2014, Dowd said she ate about one-quarter of a cannabis-infused chocolate bar, while touring the legalized recreational cannabis industry.[20] She said she was later told she should have only eaten one-sixteenth,[21][22] which was not in the instructions on the label.[23][24] She then described her negative experiences with legal cannabis in a June 3, 2014, The New York Times op-ed.[22][25] In September 2014, Dowd followed up on this story with another New York Times op-ed, this time describing a discussion of using consumable cannabis with her ""marijuana Miyagi"" Willie Nelson.[26]

On March 4, 2014, Dowd published a column about the dominance of men in the film industry; in it, she quoted Amy Pascal, co-chairman of Sony Pictures Entertainment.[27] According to BuzzFeed, ""leaked emails from Sony"" suggested that Dowd had promised to provide the draft column to Pascal's husband, Bernard Weinraub, prior to the column's publication. BuzzFeed said the column ""painted Pascal in such a good light that she engaged in a round of mutual adulation with Dowd over email after its publication.""[28] Both Dowd and Weinraub have denied that Weinraub ever received the column. On December 12, 2014, Times public editor Margaret Sullivan concluded, ""While the tone of the email exchanges is undeniably gushy, I don't think Ms. Dowd did anything unethical here.""[29]

In August 2014, it was announced that Dowd would become a staff writer for The New York Times Magazine.[30] Her first article under the new arrangement was published more than a year later.[31]"
ms. dowd,"Dowd became a columnist on The New York Times op-ed page in 1995,[8][3] replacing Anna Quindlen.[5] [10] Dowd was named a Woman of the Year by Glamour magazine in 1996,[3] and won the 1999 Pulitzer Prize, for distinguished commentary.[8] She won The Damon Runyon Award for outstanding contributions to journalism in 2000,[11] and became the first Mary Alice Davis Lectureship speaker (sponsored by the School of Journalism and the Center for American History) at the University of Texas at Austin in 2005.[12] In 2010, Dowd was ranked #43 on The Daily Telegraph's list of the 100 most influential liberals in America; in 2007, she was ranked #37 on the same list.[13]

Dowd's columns have been described as letters to her mother, whom friends credit as ""the source, the fountain of Maureen's humor and her Irish sensibilities and her intellectual take.""[4] Dowd herself has said, ""She is in my head in the sense that I want to inform and amuse the reader.""[14] Dowd's columns are distinguished by an acerbic, often polemical writing style.[15] Her columns display a critical and irreverent attitude towards powerful, mostly political, figures such as former Presidents George W. Bush and Bill Clinton. Dowd also tends to refer to her subjects by nicknames. For example, she has often referred to Bush as ""W."" and former Vice President Dick Cheney as ""Big Time"".[16] She has called former President Barack Obama ""Spock""[17] and ""Barry"". Dowd's interest in candidates' personalities earned her criticism early in her career: ""She focuses too much on the person but not enough on policy.""[4]

Dowd, who perceives her columns to be an exploration of politics, Hollywood, and gender-related topics, often uses popular culture to support and metaphorically enhance her political commentary.[15] In a Times video debate, she said of the North Korean government, ""... you could look at a movie like Mean Girls and figure out the way these North Koreans are reacting; you know it's like high school girls with nuclear weapons—they just want some attention from us, you know?""[18]

Dowd's columns have also been described as often being political cartoons that capture a caricatured view of the current political landscape with precision and exaggeration.[4] For example, in the run-up to the 2000 presidential election Dowd wrote that Democratic candidate ""Al Gore is so feminized and diversified and ecologically correct that he's practically lactating,""[19] while referring to the Democratic party as the ""mommy party"".[4] In a Fresh Dialogues interview years later she said,

""I was just teasing him a little bit because he was so earnest and he could be a little righteous and self important. That's not always the most effective way to communicate your ideas, even if the ideas themselves are right. I mean, certainly his ideas were right but he himself was – sometimes – a pompous messenger for them.""[14]
In January 2014, Dowd said she ate about one-quarter of a cannabis-infused chocolate bar, while touring the legalized recreational cannabis industry.[20] She said she was later told she should have only eaten one-sixteenth,[21][22] which was not in the instructions on the label.[23][24] She then described her negative experiences with legal cannabis in a June 3, 2014, The New York Times op-ed.[22][25] In September 2014, Dowd followed up on this story with another New York Times op-ed, this time describing a discussion of using consumable cannabis with her ""marijuana Miyagi"" Willie Nelson.[26]

On March 4, 2014, Dowd published a column about the dominance of men in the film industry; in it, she quoted Amy Pascal, co-chairman of Sony Pictures Entertainment.[27] According to BuzzFeed, ""leaked emails from Sony"" suggested that Dowd had promised to provide the draft column to Pascal's husband, Bernard Weinraub, prior to the column's publication. BuzzFeed said the column ""painted Pascal in such a good light that she engaged in a round of mutual adulation with Dowd over email after its publication.""[28] Both Dowd and Weinraub have denied that Weinraub ever received the column. On December 12, 2014, Times public editor Margaret Sullivan concluded, ""While the tone of the email exchanges is undeniably gushy, I don't think Ms. Dowd did anything unethical here.""[29]

In August 2014, it was announced that Dowd would become a staff writer for The New York Times Magazine.[30] Her first article under the new arrangement was published more than a year later.[31]"
united nations,"The United Nations (UN) is an intergovernmental organization aiming to maintain international peace and security, develop friendly relations among nations, achieve international cooperation, and be a centre for harmonizing the actions of nations.[2] It is the world's largest and most familiar international organization.[3] The UN is headquartered on international territory in New York City and has other main offices in Geneva, Nairobi, Vienna, and The Hague.

The UN was established after World War II with the aim of preventing future wars, succeeding the ineffective League of Nations.[4] On 25 April 1945, 50 governments met in San Francisco for a conference and started drafting the UN Charter, which was adopted on 25 June 1945 and took effect on 24 October 1945, when the UN began operations. Pursuant to the Charter, the organization's objectives include maintaining international peace and security, protecting human rights, delivering humanitarian aid, promoting sustainable development, and upholding international law.[5] At its founding, the UN had 51 member states; with the addition of South Sudan in 2011, membership is now 193, representing almost all of the world's sovereign states.[6]

The organization's mission to preserve world peace was complicated in its early decades by the Cold War between the United States and Soviet Union and their respective allies. Its missions have consisted primarily of unarmed military observers and lightly armed troops with primarily monitoring, reporting and confidence-building roles.[7] UN membership grew significantly following widespread decolonization beginning in the 1960s. Since then, 80 former colonies have gained independence, including 11 trust territories that had been monitored by the Trusteeship Council.[8] By the 1970s, the UN's budget for economic and social development programmes far outstripped its spending on peacekeeping. After the end of the Cold War, the UN shifted and expanded its field operations, undertaking a wide variety of complex tasks.[9]

The UN has six principal organs: the General Assembly; the Security Council; the Economic and Social Council (ECOSOC); the Trusteeship Council; the International Court of Justice; and the UN Secretariat. The UN System includes a multitude of specialized agencies, funds and programmes such as the World Bank Group, the World Health Organization, the World Food Programme, UNESCO, and UNICEF. Additionally, non-governmental organizations may be granted consultative status with ECOSOC and other agencies to participate in the UN's work.

The UN's chief administrative officer is the Secretary-General, currently Portuguese politician and diplomat António Guterres, who began his first five year-term on 1 January 2017 and was re-elected on 8 June 2021. The organization is financed by assessed and voluntary contributions from its member states.

The UN, its officers, and its agencies have won many Nobel Peace Prizes, though other evaluations of its effectiveness have been mixed. Some commentators believe the organization to be an important force for peace and human development, while others have called it ineffective, biased, or corrupt."
martijuana is not a gateway drug,"Marijuana—which can also be called cannabis, weed, pot, or dope—refers to the dried flowers, leaves, stems, and seeds of the cannabis plant. The cannabis plant contains more than 100 compounds (or cannabinoids). These compounds include tetrahydrocannabinol (THC), which is impairing or mind-altering, as well as other active compounds, such as cannabidiol (CBD). CBD is not impairing, meaning it does not cause a “high”.1

Marijuana is the most commonly used federally illegal drug in the United States, with an estimated 48.2 million people using it in 2019.2 Marijuana use may have a wide range of health effects on the body and brain. Click on the sections below to learn more about how marijuana use can affect your health."
martijuana,"Marijuana—which can also be called cannabis, weed, pot, or dope—refers to the dried flowers, leaves, stems, and seeds of the cannabis plant. The cannabis plant contains more than 100 compounds (or cannabinoids). These compounds include tetrahydrocannabinol (THC), which is impairing or mind-altering, as well as other active compounds, such as cannabidiol (CBD). CBD is not impairing, meaning it does not cause a “high”.1

Marijuana is the most commonly used federally illegal drug in the United States, with an estimated 48.2 million people using it in 2019.2 Marijuana use may have a wide range of health effects on the body and brain. Click on the sections below to learn more about how marijuana use can affect your health."
amy chua,"Amy Lynn Chua (born October 26, 1962) is an American lawyer, legal scholar, and writer. She is the John M. Duff Jr. Professor of Law at Yale Law School. Her expertise is in international business transactions, law and development, ethnic conflict, and globalization and the law.[2] She joined the Yale faculty in 2001 after teaching at Duke Law School for seven years. Prior to starting her teaching career, she was a corporate law associate at Cleary, Gottlieb, Steen & Hamilton. She is also known for her parenting memoir Battle Hymn of the Tiger Mother. In 2011, she was named one of Time magazine's 100 most influential people, one of The Atlantic's Brave Thinkers, and one of Foreign Policy's Global Thinkers.[3]"
jobs in america,"For purposes of U.S. federal income tax withholding, 26 U.S.C. § 3401(c) provides a definition for the term ""employee"" specific to chapter 24 of the Internal Revenue Code:


Government employment as % of total employment in EU
""For purposes of this chapter, the term “employee” includes an officer, employee, or elected official of the United States, a State, or any political subdivision thereof, or the District of Columbia, or any agency or instrumentality of any one or more of the foregoing. The term “employee” also includes an officer of a corporation.""[42] This definition does not exclude all those who are commonly known as 'employees'. “Similarly, Latham’s instruction which indicated that under 26 U.S.C. § 3401(c) the category of ‘employee’ does not include privately employed wage earners is a preposterous reading of the statute. It is obvious that within the context of both statutes the word ‘includes’ is a term of enlargement not of limitation, and the reference to certain entities or categories is not intended to exclude all others.”[43]

Employees are often contrasted with independent contractors, especially when there is dispute as to the worker's entitlement to have matching taxes paid, workers compensation, and unemployment insurance benefits. However, in September 2009, the court case of Brown v. J. Kaz, Inc. ruled that independent contractors are regarded as employees for the purpose of discrimination laws if they work for the employer on a regular basis, and said employer directs the time, place, and manner of employment.[38]

In non-union work environments, in the United States, unjust termination complaints can be brought to the United States Department of Labor.[44]

Labor unions are legally recognized as representatives of workers in many industries in the United States. Their activity today centers on collective bargaining over wages, benefits, and working conditions for their membership, and on representing their members in disputes with management over violations of contract provisions. Larger unions also typically engage in lobbying activities and electioneering at the state and federal level.[38]

Most unions in America are aligned with one of two larger umbrella organizations: the AFL-CIO created in 1955, and the Change to Win Federation which split from the AFL-CIO in 2005. Both advocate policies and legislation on behalf of workers in the United States and Canada, and take an active role in politics. The AFL-CIO is especially concerned with global trade issues.[24]

American business theorist Jeffrey Pfeffer posits that contemporary employment practices and employer commonalities in the United States, including toxic working environments, job insecurity, long hours and increased performance pressure from management, are responsible for 120,000 excess deaths annually, making the workplace the fifth leading cause of death in the United States.[45]"
black,"African Americans (also referred to as Black Americans or Afro-Americans)[3] are an ethnic group of Americans with partial or total ancestry from any of the Black racial groups of Africa.[4][5] The term African American generally denotes descendants of enslaved Africans who are from the United States,[6][7][8] while some Black immigrants or their children may also come to identify as African-American.[9]

African Americans constitute the second largest racial group in the US, after White Americans and the third largest ethnic group after Hispanic and Latino Americans.[10] Most African Americans are descendants of enslaved people within the boundaries of the present United States.[11][12] On average, African Americans are of West/Central African and European descent, and some also have Native American ancestry.[13] According to U.S. Census Bureau data, African immigrants generally do not self-identify as African American. The overwhelming majority of African immigrants identify instead with their own respective ethnicities (≈95%).[14] Immigrants from some Caribbean, Central American, and South American nations and their descendants may or may not also self-identify with the term.[8]

African-American history began in the 16th century, with Africans from West Africa being sold to European slave traders and transported across the Atlantic to the Thirteen Colonies. After arriving in the Americas, they were sold as slaves to European colonists and put to work on plantations, particularly in the southern colonies. A few were able to achieve freedom through manumission or escape and founded independent communities before and during the American Revolution. After the United States was founded in 1783, most Black people continued to be enslaved, being most concentrated in the American South, with four million enslaved only liberated during and at the end of the Civil War in 1865.[15] During Reconstruction, they gained citizenship and the right to vote, but due to White supremacy, they were largely treated as second-class citizens and found themselves soon disenfranchised in the South. These circumstances changed due to further development of the Black community, participation in the military conflicts of the United States, substantial migration out of the South, the elimination of legal racial segregation, and the civil rights movement which sought political and social freedom. In 2008, Barack Obama became the first African American to be elected President of the United States.[16]"
the a.d.h.d. diagnosis,"Deciding if a child has ADHD is a process with several steps. This page gives you an overview of how ADHD is diagnosed. There is no single test to diagnose ADHD, and many other problems, like sleep disorders, anxiety, depression, and certain types of learning disabilities, can have similar symptoms.

If you are concerned about whether a child might have ADHD, the first step is to talk with a healthcare provider to find out if the symptoms fit the diagnosis. The diagnosis can be made by a mental health professional, like a psychologist or psychiatrist, or by a primary care provider, like a pediatrician.

The American Academy of Pediatrics (AAP) recommends that healthcare providers ask parents, teachers, and other adults who care for the child about the child’s behavior in different settings, like at home, school, or with peers. Read more about the recommendations.

The healthcare provider should also determine whether the child has another condition that can either explain the symptoms better, or that occurs at the same time as ADHD. Healthcare providers use the guidelines in the American Psychiatric Association’s Diagnostic and Statistical Manual, Fifth edition (DSM-5)1, to help diagnose ADHD. This diagnostic standard helps ensure that people are appropriately diagnosed and treated for ADHD. Using the same standard across communities can also help determine how many children have ADHD, and how public health is impacted by this condition."
google as a medical tool,"Google Health is a branch of Google started in 2006. Originally designed as an to attempt to create a repository of health records and data (personal health record services), in order to connect doctors, hospitals and pharmacies directly. The project was introduced in 2008 and discontinued in 2012. Google Health was restarted in 2018 as a new division, but was later reorganized back into Google 2021.[1]

Starting in November 2018, Dr. David Feinberg was appointed lead.[2] In 2019, it was announced they wanted more searchable medical records and to ""improve the quality of health-focused search results across Google and YouTube"".[3] Google Health also appeared to focus on health-related artificial intelligence research, clinical tools, and partnerships for other healthcare tools and services.[4]"
undocumente migrant,"Illegal immigration to the United States is the process of migrating into the United States in violation of federal immigration laws. This can include foreign nationals (aliens) who have entered the United States unlawfully,[1][2] as well as those who lawfully entered but then remained after the expiration of their visas, parole, TPS, etc.[3] Illegal immigration has been a matter of intense debate in the United States since the 1980s.

Research shows that illegal immigrants increase the size of the U.S. economy, contribute to economic growth, enhance the welfare of natives, contribute more in tax revenue than they collect, reduce American firms' incentives to offshore jobs and import foreign-produced goods, and benefit consumers by reducing the prices of goods and services.[4][5][6][7] Economists estimate that legalization of the illegal immigrant population would increase the immigrants' earnings and consumption considerably, and increase U.S. gross domestic product.[8][9][10][11]

There is scholarly consensus that illegal immigrants commit less crime than natives.[12][13] Sanctuary cities—which adopt policies designed to avoid prosecuting people solely for being in the country illegally—have no statistically meaningful impact on crime, and may reduce the crime rate.[14][15] Research suggests that immigration enforcement has no impact on crime rates.[16][17][14] Stricter border controls have been linked to increased levels of undocumented immigrants in the United States, as temporary undocumented workers who used to enter the U.S. for seasonal work increasingly settled permanently in the U.S. when regular travels across the border became harder.[18]

The illegal immigrant population of the United States peaked by 2007, when it was at 12.2 million and 4% of the total U.S. population.[19][20] Estimates in 2016 put the number of unauthorized immigrants at 10.7 million, representing 3.3% of the total U.S. population.[19] Since the Great Recession, more illegal immigrants have left the United States than entered it, and illegal border crossings are at the lowest in decades.[21][22][23][24] Since 2007, visa overstays have accounted for a larger share of the growth in the illegal immigrant population than illegal border crossings,[25] which have declined considerably from 2000 to 2018.[26] In 2012, 52% of unauthorized immigrants were from Mexico, 15% from Central America, 12% from Asia, 6% from South America, 5% from the Caribbean, and another 5% from Europe and Canada.[27] As of 2016, approximately two-thirds of unauthorized adult immigrants had lived in the U.S. for at least a decade.[19]"
google,"Google LLC is an American multinational technology company that specializes in Internet-related services and products, which include online advertising technologies, a search engine, cloud computing, software, and hardware. It is considered one of the Big Five companies in the American information technology industry, along with Amazon, Facebook (Meta), Apple, and Microsoft.[10][11][12]

Google was founded on September 4, 1998, by Larry Page and Sergey Brin while they were Ph.D. students at Stanford University in California. Together they own about 14% of its publicly-listed shares and control 56% of the stockholder voting power through super-voting stock. The company went public via an initial public offering (IPO) in 2004. In 2015, Google was reorganized as a wholly-owned subsidiary of Alphabet Inc.. Google is Alphabet's largest subsidiary and is a holding company for Alphabet's Internet properties and interests. Sundar Pichai was appointed CEO of Google on October 24, 2015, replacing Larry Page, who became the CEO of Alphabet. On December 3, 2019, Pichai also became the CEO of Alphabet.[13]

In 2021, the Alphabet Workers Union was founded, mainly composed of Google employees.[14]

The company's rapid growth since incorporation has included products, acquisitions, and partnerships beyond Google's core search engine, (Google Search). It offers services designed for work and productivity (Google Docs, Google Sheets, and Google Slides), email (Gmail), scheduling and time management (Google Calendar), cloud storage (Google Drive), instant messaging and video chat (Google Duo, Google Chat, and Google Meet), language translation (Google Translate), mapping and navigation (Google Maps, Waze, Google Earth, and Street View), podcast hosting (Google Podcasts), video sharing (YouTube), blog publishing (Blogger), note-taking (Google Keep and Jamboard), and photo organizing and editing (Google Photos). The company leads the development of the Android mobile operating system, the Google Chrome web browser, and Chrome OS (a lightweight, proprietary operating system based on the free and open-source Chromium OS operating system). Google has moved increasingly into hardware; from 2010 to 2015, it partnered with major electronics manufacturers in the production of its Google Nexus devices, and it released multiple hardware products in 2016, including the Google Pixel line of smartphones, Google Home smart speaker, Google Wifi mesh wireless router. Google has also experimented with becoming an Internet carrier (Google Fiber and Google Fi).

Google.com is the most visited website worldwide. Several other Google-owned websites also are on the list of most popular websites, including YouTube and Blogger.[15] On the list of most valuable brands, Google is ranked second by Forbes[16] and fourth by Interbrand.[17] It has received significant criticism involving issues such as privacy concerns, tax avoidance, censorship, search neutrality, antitrust and abuse of its monopoly position."
commodification of everything,"Within a capitalist economic system, commodification is the transformation of goods, services, ideas, nature, personal information or people[1][2][3] into commodities or objects of trade. A commodity at its most basic, according to Arjun Appadurai, is ""anything intended for exchange,"" or any object of economic value.[4]

Commodification is often criticised on the grounds that some things ought not to be treated as commodities—for example water, education, data, information, knowledge, human life, and animal life.[5][6] Human commodity is a term used in case of human organ trade, paid surrogacy also known as commodification of the womb, and human trafficking.[1][2][7] Slave trade as a form of human trafficking is a form of the commodification of people. According to Gøsta Esping-Andersen people are commodified or 'turned into objects' when selling their labour on the market to an employer.[8"
addiction to electronic gadgets,"What is Addiction Of Electronic Gadgets?

The modern era is defined by scientific innovations and gadgets have made life simpler and a comfortable one. Gadgets cover each and every facet of our life and have resulted in an overbearing dependence on them. Consider the smart phones which have revolutionized the way we interact with people and have penetrated into every facet of our lives especially the kids. Kids, today, do not have social circles but indulge in social networking. They don’t go out to play as they interact with everyone from the comfort of their homes. This device seemingly offers them the world and they latch on to it and never let go. Have you ever seen a kid whose phone signal 15 percent battery is left; the anxiety, the sense of despair and hopelessness that comes when the phone’s battery actually dies? This behaviour can’t simply be categorized as dependence but has to be categorized as an addiction. It may seem harsh given the importance of the devices but the behaviour of complete submission to this device shows consistency with that of an addict and hence has to be categorized in that manner. This is particularly common among children especially those born in late 90’s and early 00’s as they have not known a time when these gadgets were not around.

Causes of Addiction Of Electronic Gadgets.

The main reason for this sort of addiction is that these gadgets form a very large part of our lives in the modern day existence. These gadgets are readily available without any restrictions on usage be it in terms of age or time.

Moreover they seem to be very fulfilling at first especially for kids who do not have to make much of an effort to establish connections (however unfulfilling they might be). In fact the causes f the addiction are the same reasons which make these gadgets useful i.e. the increased productivity it ensures and the ease and speed with each it makes the services available to us.

Another reason could be understood by taking into account the safety of anonymity and security for m socially awkward situations that these agents provide. One never has to go through a bad conversation and can easily avoid a difficult conversation if their only medium of contact is internet. But it becomes very difficult to do so in real life and this why it is so addictive. It allows us to project how we want ourselves to be and not who we are."
farm subsisdies,"An agricultural subsidy (also called an agricultural incentive) is a government incentive paid to agribusinesses, agricultural organizations and farms to supplement their income, manage the supply of agricultural commodities, and influence the cost and supply of such commodities. Examples of such commodities include: wheat, feed grains (grain used as fodder, such as maize or corn, sorghum, barley and oats), cotton, milk, rice, peanuts, sugar, tobacco, oilseeds such as soybeans and meat products such as beef, pork, and lamb and mutton.[1] A 2021 study by the UN Food and Agriculture Organization found 87% of the $540bn farmers given every year between 2013 and 2018 in global subsidies are harmful to both people and environment.[2]"
the deaf,the deaf
account,account
racial preference,"Racism in the United States comprises negative attitudes and views on race or ethnicity which are related to each other, are held by various people and groups in the United States and have been reflected in discriminatory laws, practices and actions at various times in the history of the United States (including violence) against racial or ethnic groups. Throughout American history, white Americans have generally enjoyed legally or socially sanctioned privileges and rights which have been denied to members of various ethnic or minority groups at various times. European Americans, particularly affluent white Anglo-Saxon Protestants, are said to have enjoyed advantages in matters of education, immigration, voting rights, citizenship, land acquisition, and criminal procedure.

Racism against various ethnic or minority groups has existed in the United States since the early colonial era. Before 1865, most African Americans were enslaved and even afterwards, they have faced restrictions on their political, social, and economic freedoms. Native Americans have suffered genocide, forced removals, and massacres, and they continue to face discrimination. Non-Protestant immigrants from Europe, particularly Jews, Poles, Italians, and the Irish were often subjected to xenophobic exclusion and other forms of ethnicity-based discrimination. In addition, Hispanics, Middle Eastern and Asian Americans along with Pacific Islanders have also been the victims of discrimination.

Racism in the U.S. has manifested itself in a variety of ways, including genocide, slavery, segregation, Native American reservations and boarding schools, racist immigration and naturalization laws, and internment camps.[a] Formal racial discrimination was largely banned by the mid-20th century and over time, coming to be perceived as being socially and morally unacceptable. Racial politics remains a major phenomenon, and racism continues to be reflected in socioeconomic inequality.[1][b] In recent years research has uncovered extensive evidence of racial discrimination in various sectors of modern U.S. society, including the criminal justice system, business, the economy, housing, health care, the media, and politics. In the view of the United Nations and the U.S. Human Rights Network, ""discrimination in the United States permeates all aspects of life and extends to all communities of color.""[3]"
work ethic,"Work ethic is a belief that work and diligence have a moral benefit and an inherent ability, virtue or value to strengthen character and individual abilities.[1] It is a set of values centered on importance of work and manifested by determination or desire to work hard. Social ingrainment of this value is considered to enhance character through hard work that is respective to an individual's field of work.[2]"
making impact in students lives,making impact in students lives
comment secttion,"The comments section is a feature of online blogs, news websites, and other websites in which the publishers invite the audience to comment on the published content. This is a continuation of the older practice of publishing letters to the editor. Despite this, comments sections can be used for more discussion between readers.[1]"
defund public education,"Education in the United States of America is provided in public, private, and home schools. State governments set overall educational standards, often mandate standardized tests for K–12 public school systems and supervise, usually through a board of regents, state colleges, and universities. The bulk of the $1.3 trillion in funding comes from state and local governments, with federal funding accounting for only about $200 billion.[1] Private schools are generally free to determine their own curriculum and staffing policies, with voluntary accreditation available through independent regional accreditation authorities, although some state regulation can apply.

In 2013, about 87% of school-age children (those below higher education) attended state funded public schools, about 10% attended tuition- and foundation-funded private schools,[8] and roughly 3% were home-schooled.[9]

By state law, education is compulsory over an age range starting between five and eight and ending somewhere between ages sixteen and eighteen, depending on the state.[10] This requirement can be satisfied in public schools, state-certified private schools, or an approved home school program. In most schools, compulsory education is divided into three levels: elementary school, middle or junior high school, and high school. Children are usually divided by age groups into grades, ranging from kindergarten (5- to 6-year-olds) and first grade (6- to 7-year-olds) for the youngest children, up to twelfth grade (17- to 18-year-olds) as the final year of high school.

There is also a large number and wide variety of publicly and privately administered colleges and universities throughout the country. Post-secondary education is divided into college, as the first tertiary degree, and graduate school. Higher education includes extremely wealthy and selective universities, public research universities, private liberal arts colleges, historically-black colleges and universities, community colleges, for-profit colleges, and many other kinds and combinations of institutions. College enrollment rates in the United States have increased over the long term.[11] At the same time, student loan debt has also risen to $1.5 trillion. According to a 2016 report published by the U.S. News & World Report, of the top ten colleges and universities in the world, eight are American (the other two are Oxford and Cambridge, in the United Kingdom).[12]

The United States spends more per student on education than any other country.[13] In 2014, the Pearson/Economist Intelligence Unit rated U.S. education as 14th best in the world. The Programme for International Student Assessment coordinated by the OECD currently ranks the overall knowledge and skills of American 15-year-olds as 31st in the world in reading literacy, mathematics, and science with the average American student scoring 487.7, compared with the OECD average of 493.[14][15] In 2014, the country spent 6.2 percent of its GDP on all levels of education – 1.0 percentage points above the OECD average of 5.2 percent.[16] In 2017, 46.4 percent of Americans aged 25 to 64 attained some form of post-secondary education.[3][4] 48 percent of Americans aged 25 to 34 attained some form of tertiary education, about 4 percent above the OECD average of 44 percent.[17][18][19] 35 percent of Americans aged 25 and over have achieved a bachelor's degree or higher.[20] The United States ranks 3rd from the bottom among OECD nations in terms of its poverty gap, and 4th from the bottom in terms of poverty rate.[21][22]"
puble education funding,"Education in the United States of America is provided in public, private, and home schools. State governments set overall educational standards, often mandate standardized tests for K–12 public school systems and supervise, usually through a board of regents, state colleges, and universities. The bulk of the $1.3 trillion in funding comes from state and local governments, with federal funding accounting for only about $200 billion.[1] Private schools are generally free to determine their own curriculum and staffing policies, with voluntary accreditation available through independent regional accreditation authorities, although some state regulation can apply.

In 2013, about 87% of school-age children (those below higher education) attended state funded public schools, about 10% attended tuition- and foundation-funded private schools,[8] and roughly 3% were home-schooled.[9]

By state law, education is compulsory over an age range starting between five and eight and ending somewhere between ages sixteen and eighteen, depending on the state.[10] This requirement can be satisfied in public schools, state-certified private schools, or an approved home school program. In most schools, compulsory education is divided into three levels: elementary school, middle or junior high school, and high school. Children are usually divided by age groups into grades, ranging from kindergarten (5- to 6-year-olds) and first grade (6- to 7-year-olds) for the youngest children, up to twelfth grade (17- to 18-year-olds) as the final year of high school.

There is also a large number and wide variety of publicly and privately administered colleges and universities throughout the country. Post-secondary education is divided into college, as the first tertiary degree, and graduate school. Higher education includes extremely wealthy and selective universities, public research universities, private liberal arts colleges, historically-black colleges and universities, community colleges, for-profit colleges, and many other kinds and combinations of institutions. College enrollment rates in the United States have increased over the long term.[11] At the same time, student loan debt has also risen to $1.5 trillion. According to a 2016 report published by the U.S. News & World Report, of the top ten colleges and universities in the world, eight are American (the other two are Oxford and Cambridge, in the United Kingdom).[12]

The United States spends more per student on education than any other country.[13] In 2014, the Pearson/Economist Intelligence Unit rated U.S. education as 14th best in the world. The Programme for International Student Assessment coordinated by the OECD currently ranks the overall knowledge and skills of American 15-year-olds as 31st in the world in reading literacy, mathematics, and science with the average American student scoring 487.7, compared with the OECD average of 493.[14][15] In 2014, the country spent 6.2 percent of its GDP on all levels of education – 1.0 percentage points above the OECD average of 5.2 percent.[16] In 2017, 46.4 percent of Americans aged 25 to 64 attained some form of post-secondary education.[3][4] 48 percent of Americans aged 25 to 34 attained some form of tertiary education, about 4 percent above the OECD average of 44 percent.[17][18][19] 35 percent of Americans aged 25 and over have achieved a bachelor's degree or higher.[20] The United States ranks 3rd from the bottom among OECD nations in terms of its poverty gap, and 4th from the bottom in terms of poverty rate.[21][22]"
war in iraq,"The Iraq War[nb 1] was a protracted armed conflict from 2003 to 2011 that began with the invasion of Iraq by the United States–led coalition which overthrew the Iraqi government of Saddam Hussein. The conflict continued for much of the next decade as an insurgency emerged to oppose the coalition forces and the post-invasion Iraqi government.[56] An estimated 151,000 to 1,033,000 Iraqis died in the first three to five years of conflict. US troops were officially withdrawn in 2011. The United States became re-involved in 2014 at the head of a new coalition; the insurgency and many dimensions of the armed conflict continue. The invasion occurred as part of the George W. Bush administration's War on Terror following the September 11 attacks despite no connection of the latter to Iraq.[57]

In October 2002, Congress granted President Bush the power to decide whether to launch any military attack in Iraq.[58] The Iraq War began on 20 March 2003,[59] when the US, joined by the UK, Australia, and Poland launched a ""shock and awe"" bombing campaign. Iraqi forces were quickly overwhelmed as coalition forces swept through the country. The invasion led to the collapse of the Ba'athist government; Saddam Hussein was captured during Operation Red Dawn in December of that same year and executed three years later. The power vacuum following Saddam's demise and mismanagement by the Coalition Provisional Authority led to widespread civil war between Shias and Sunnis, as well as a lengthy insurgency against coalition forces. Many of the violent insurgent groups were supported by Iran and al-Qaeda in Iraq. The United States responded with a build-up of 170,000 troops in 2007.[60] This build-up gave greater control to Iraq's government and military, and was judged a success by many.[61] In 2008, President Bush agreed to a withdrawal of all US combat troops from Iraq. The withdrawal was completed under President Barack Obama in December 2011.[62][63]

The Bush administration based its rationale for the Iraq War on the claim that Iraq had a weapons of mass destruction (WMD) program,[64] and that Iraq posed a threat to the United States and its allies.[65][66] Some US officials falsely accused Saddam of harbouring and supporting al-Qaeda.[67] In 2004, the 9/11 Commission concluded there was no evidence of an relationship between the Saddam Hussein regime and al-Qaeda.[68] No stockpiles of WMDs or an active WMD program were ever found in Iraq.[69] Bush administration officials made numerous claims about a purported Saddam–al-Qaeda relationship and WMDs that were based on sketchy evidence rejected by intelligence officials.[69][70] The rationale for war faced heavy criticism both domestically and internationally.[71] Kofi Annan called the invasion illegal under international law as it violated the UN Charter.[72] The Chilcot Report, a British inquiry into its decision to go to war, was published in 2016 and concluded peaceful alternatives to war had not been exhausted, that the United Kingdom and the United States had undermined the authority of the United Nations Security Council, that the process of identifying the legal basis was ""far from satisfactory"", and that the war was unnecessary.[73][74][75] When interrogated by the FBI, Saddam Hussein confirmed that Iraq did not have weapons of mass destruction prior to the US invasion.[76]

In the aftermath of the invasion, Iraq held multi-party elections in 2005. Nouri al-Maliki became Prime Minister in 2006 and remained in office until 2014. The al-Maliki government enacted policies that alienated the country's previously dominant Sunni minority and worsened sectarian tensions. In the summer of 2014, ISIL launched a military offensive in northern Iraq and declared a worldwide Islamic caliphate, leading to Operation Inherent Resolve, another military response from the United States and its allies. According to a 2019 US Army study, Iran has emerged as ""the only victor"" of the war.[14]

The Iraq War caused at least one hundred thousand civilian deaths, as well as tens of thousands of military deaths (see estimates below). The majority of deaths occurred as a result of the insurgency and civil conflicts between 2004 and 2007. Subsequently, the War in Iraq of 2013 to 2017, which is considered a domino effect of the invasion and occupation, caused at least 155,000 deaths, in addition to the displacement of more than 3.3 million people within the country.[77][78][79]"
thug," a common criminal, who treats others violently and roughly, often for hire"
svalley,svalley
younger generation students,younger generation students
animal product diet,"An animal product is any material derived from the body of an animal.[1] Examples are fat, flesh, blood, milk, eggs, and lesser known products, such as isinglass and rennet.[2]

Animal by-products, as defined by the USDA, are products harvested or manufactured from livestock other than muscle meat.[3] In the EU, animal by-products (ABPs) are defined somewhat more broadly, as materials from animals that people do not consume.[4] Thus, chicken eggs for human consumption are considered by-products in the US but not France; whereas eggs destined for animal feed are classified as animal by-products in both countries. This does not in itself reflect on the condition, safety, or wholesomeness of the product.

Animal by-products are carcasses and parts of carcasses from slaughterhouses, animal shelters, zoos and veterinarians, and products of animal origin not intended for human consumption, including catering waste. These products may go through a process known as rendering to be made into human and non-human foodstuffs, fats, and other material that can be sold to make commercial products such as cosmetics, paint, cleaners, polishes, glue, soap and ink. The sale of animal by-products allows the meat industry to compete economically with industries selling sources of vegetable protein.[5]

The word animals includes all species in the biological kingdom animalia. For example, insects, shrimp, and oysters are animals.

Generally, products made from fossilized or decomposed animals, such as petroleum formed from the ancient remains of marine animals are not considered animal products. Crops grown in soil fertilized with animal remains are rarely characterized as animal products.

Several popular diet patterns prohibit the inclusion of some categories of animal products and may also limit the conditions of when other animal products may be permitted. This includes but not limited to secular diets; like, vegetarian, pescetarian, and paleolithic diets, as well as religious diets, such as kosher, halal, mahayana, macrobiotic, and sattvic diets. Other diets, such as vegan-vegetarian diets and all its subsets exclude any material of animal origin.[6] Scholarly, the term animal source foods (ASFs) has been used to refer to refer to these animal products and byproducts collectively.[7]

In international trade legislation, the terminology products of animal origin (POAO) is used for referring to foods and goods that are derived from animals or have close relation to them.[8]"
public educaitons,"Education in the United States of America is provided in public, private, and home schools. State governments set overall educational standards, often mandate standardized tests for K–12 public school systems and supervise, usually through a board of regents, state colleges, and universities. The bulk of the $1.3 trillion in funding comes from state and local governments, with federal funding accounting for only about $200 billion.[1] Private schools are generally free to determine their own curriculum and staffing policies, with voluntary accreditation available through independent regional accreditation authorities, although some state regulation can apply.

In 2013, about 87% of school-age children (those below higher education) attended state funded public schools, about 10% attended tuition- and foundation-funded private schools,[8] and roughly 3% were home-schooled.[9]

By state law, education is compulsory over an age range starting between five and eight and ending somewhere between ages sixteen and eighteen, depending on the state.[10] This requirement can be satisfied in public schools, state-certified private schools, or an approved home school program. In most schools, compulsory education is divided into three levels: elementary school, middle or junior high school, and high school. Children are usually divided by age groups into grades, ranging from kindergarten (5- to 6-year-olds) and first grade (6- to 7-year-olds) for the youngest children, up to twelfth grade (17- to 18-year-olds) as the final year of high school.

There is also a large number and wide variety of publicly and privately administered colleges and universities throughout the country. Post-secondary education is divided into college, as the first tertiary degree, and graduate school. Higher education includes extremely wealthy and selective universities, public research universities, private liberal arts colleges, historically-black colleges and universities, community colleges, for-profit colleges, and many other kinds and combinations of institutions. College enrollment rates in the United States have increased over the long term.[11] At the same time, student loan debt has also risen to $1.5 trillion. According to a 2016 report published by the U.S. News & World Report, of the top ten colleges and universities in the world, eight are American (the other two are Oxford and Cambridge, in the United Kingdom).[12]

The United States spends more per student on education than any other country.[13] In 2014, the Pearson/Economist Intelligence Unit rated U.S. education as 14th best in the world. The Programme for International Student Assessment coordinated by the OECD currently ranks the overall knowledge and skills of American 15-year-olds as 31st in the world in reading literacy, mathematics, and science with the average American student scoring 487.7, compared with the OECD average of 493.[14][15] In 2014, the country spent 6.2 percent of its GDP on all levels of education – 1.0 percentage points above the OECD average of 5.2 percent.[16] In 2017, 46.4 percent of Americans aged 25 to 64 attained some form of post-secondary education.[3][4] 48 percent of Americans aged 25 to 34 attained some form of tertiary education, about 4 percent above the OECD average of 44 percent.[17][18][19] 35 percent of Americans aged 25 and over have achieved a bachelor's degree or higher.[20] The United States ranks 3rd from the bottom among OECD nations in terms of its poverty gap, and 4th from the bottom in terms of poverty rate.[21][22]"
parole is not given to thse in the afternoon,"Parole is the early release of a prisoner who agrees to abide by certain conditions, originating from the French word parole (""speech, spoken words"" but also ""promise""). The term became associated during the Middle Ages with the release of prisoners who gave their word.

This differs greatly from pardon, amnesty or commutation of sentence in that parolees are still considered to be serving their sentences, and may be returned to prison if they violate the conditions of their parole."
illegal black-market drugs,"A black market, underground economy or shadow economy, is a clandestine market or series of transactions that has some aspect of illegality or is characterized by some form of noncompliant behavior with an institutional set of rules. If the rule defines the set of goods and services whose production and distribution is prohibited by law, non-compliance with the rule constitutes a black market trade since the transaction itself is illegal. Parties engaging in the production or distribution of prohibited goods and services are members of the illegal economy. Examples include the drug trade, prostitution (where prohibited), illegal currency transactions and human trafficking.[1] Violations of the tax code involving income tax evasion constitute membership in the unreported economy.[2][3]

Because tax evasion or participation in a black market activity is illegal, participants will attempt to hide their behavior from the government or regulatory authority.[4] Cash usage is the preferred medium of exchange in illegal transactions since cash usage does not leave a footprint.[5] Common motives for operating in black markets are to trade contraband, avoid taxes and regulations, or skirt price controls or rationing. Typically the totality of such activity is referred to with the definite article as a complement to the official economies, by market for such goods and services, e.g. ""the black market in bush meat"".

The black market is distinct from the grey market, in which commodities are distributed through channels that, while legal, are unofficial, unauthorized, or unintended by the original manufacturer, and the white market, in which trade is legal and official.

Black money is the proceeds of an illegal transaction, on which income and other taxes have not been paid, and which can only be legitimised by some form of money laundering. Because of the clandestine nature of the black economy it is not possible to determine its size and scope.[6]"
black-market drugs,"A black market, underground economy or shadow economy, is a clandestine market or series of transactions that has some aspect of illegality or is characterized by some form of noncompliant behavior with an institutional set of rules. If the rule defines the set of goods and services whose production and distribution is prohibited by law, non-compliance with the rule constitutes a black market trade since the transaction itself is illegal. Parties engaging in the production or distribution of prohibited goods and services are members of the illegal economy. Examples include the drug trade, prostitution (where prohibited), illegal currency transactions and human trafficking.[1] Violations of the tax code involving income tax evasion constitute membership in the unreported economy.[2][3]

Because tax evasion or participation in a black market activity is illegal, participants will attempt to hide their behavior from the government or regulatory authority.[4] Cash usage is the preferred medium of exchange in illegal transactions since cash usage does not leave a footprint.[5] Common motives for operating in black markets are to trade contraband, avoid taxes and regulations, or skirt price controls or rationing. Typically the totality of such activity is referred to with the definite article as a complement to the official economies, by market for such goods and services, e.g. ""the black market in bush meat"".

The black market is distinct from the grey market, in which commodities are distributed through channels that, while legal, are unofficial, unauthorized, or unintended by the original manufacturer, and the white market, in which trade is legal and official.

Black money is the proceeds of an illegal transaction, on which income and other taxes have not been paid, and which can only be legitimised by some form of money laundering. Because of the clandestine nature of the black economy it is not possible to determine its size and scope.[6]"
hidden contract fees,"Have you ever noticed that your phone or cable bill or the price you paid for your dream vacation may be higher than you expected? That may be due to extra fees tacked on to the original charge. While most consumers expect to pay specific fees for the services they use, there may be additional charges added on that they may not necessarily be aware of at the time they signed up. These are called hidden or undisclosed fees, which may be a one-time charge and may appear in fine print on a contract. These are charged by a variety of companies such as banks, credit cards, cellphone, cable and Internet providers, brokers and insurance firms, and those in the travel industry.

Hidden fees can cost consumers billions of dollars a year (and, in turn, make big profits for corporations) and are usually regulated at the state and federal level. According to a 2016 report from the National Economic Council, these fees can often be deceptive because they muddy the purchase price for consumers. The report states that fees have steadily increased in the airline, hotel, and related industries."
raw starches,raw starches
people fighting isis,"Islamic State (official name since June 2014;[98] abbreviated IS), at times known as the Islamic State of Iraq and the Levant (ISIL; /ˈaɪsəl, ˈaɪsɪl/), and as the Islamic State of Iraq and Syria (ISIS; /ˈaɪsɪs/),[99] or by its Arabic acronym, Daesh (داعش, Dāʿish, IPA: [ˈdaːʕɪʃ]),[100] is a militant Sunni Islamist group and former unrecognized quasi-state[101] that follows a Salafi jihadist doctrine.[102] Islamic State was founded by Abu Musab al-Zarqawi and gained global prominence in 2014 when it drove Iraqi security forces out of key cities in its Western Iraq offensive,[103] followed by its capture of Mosul[104] and the Sinjar massacre.[105]

IS originated in 1999, pledged allegiance to Al-Qaeda and participated in the Iraqi insurgency following the 2003 invasion of Iraq by Western forces. In June 2014, the group proclaimed itself a worldwide caliphate[106][107] and began referring to itself as the Islamic State (الدولة الإسلامية ad-Dawlah al-Islāmiyah; IS).[1] As a caliphate, it claimed religious, political, and military authority over all Muslims worldwide.[108] Its adoption of the name Islamic State and its idea of a caliphate have been criticised, with the United Nations, various governments, and mainstream Muslim groups rejecting its statehood.[109] In Syria, the group conducted ground attacks on both government forces and opposition factions, and by December 2015, it held an area extending from western Iraq to eastern Syria, containing an estimated eight to twelve million people,[54][55][110] where it enforced its interpretation of sharia law. They were estimated at the time to have an annual budget of more than US$1 billion and more than 30,000 fighters.[111]

In mid-2014, an international coalition led by the United States intervened against ISIL in Syria and Iraq with an airstrike campaign, in addition to supplying advisors, weapons, training, and supplies to ISIL's enemies in the Iraqi Security Forces and Syrian Democratic Forces. This campaign reinvigorated the latter two forces and damaged ISIL, killing tens of thousands of its troops[112] and reducing its financial and military infrastructure.[113] This was followed by a smaller-scale Russian intervention exclusively in Syria, in which ISIL lost thousands more fighters to airstrikes, cruise missile attacks, and other Russian military activities and had its financial base further degraded.[114] In July 2017, the group lost control of its largest city, Mosul, to the Iraqi army, followed by the loss of its de facto political capital of Raqqa to the Syrian Democratic Forces.[115] By December 2017, the Islamic State controlled just 2% of its maximum territory (in May 2015).[116] In December 2017, Iraqi forces had driven the last remnants of the Islamic State underground, three years after the group captured about a third of Iraq's territory.[117] By March 2019, ISIL lost one of their last significant territories in the Middle East in the Deir ez-Zor campaign, surrendering their ""tent city"" and pockets in Al-Baghuz Fawqani to the Syrian Democratic Forces after the Battle of Baghuz Fawqani.[30]

The group has been designated as a terrorist organisation by the United Nations. The IS is known for its videos of beheadings and other types of executions[118] of both soldiers and civilians, including journalists and aid workers, and its destruction of cultural heritage sites.[119] The United Nations holds ISIL responsible for committing human rights abuses, genocide, war crimes, and crimes against humanity.[120] The Islamic State committed genocide and ethnic cleansing on a historic scale in northern Iraq.[121][122] In October 2019, IS media announced that Abu Ibrahim al-Hashimi al-Qurashi was the new leader of the Islamic State,[123] after Abu Bakr al-Baghdadi killed himself by detonating a suicide vest during the US Barisha raid in Syria.[124][125][126] The Islamic State has also had a presence outside the Middle East through its affiliates and various ""provinces"",[127] and has been militarily active in countries including notably in Nigeria, Afghanistan and the Philippines.[128]"
ways for kids to express themselves,ways for kids to express themselves
teacbhratings system,teacher rating system. The Teacher Development and Evaluation System is a multiple-measure system that allows for a holistic assessment of a teacher's contribution to the progress students make. Measures of student learning are combined with measures of teacher practice (observations made by evaluators) to create an overall rating.
law enfoursement,law enforcement
hepatitis vaccine after birth,"Hepatitis means inflammation of the liver. The liver is a vital organ that processes nutrients, filters the blood, and fights infections. When the liver is inflamed or damaged, its function can be affected. Heavy alcohol use, toxins, some medications, and certain medical conditions can cause hepatitis. However, hepatitis is often caused by a virus. In the United States, the most common types of viral hepatitis are hepatitis A, hepatitis B, and hepatitis C."
bib,"Benjamin Netanyahu (/ˌnɛtɑːnˈjɑːhuː/;[2] Hebrew: About this soundבִּנְיָמִין נְתַנְיָהוּ‎ (help·info); born 21 October 1949) is an Israeli politician who served as the ninth prime minister of Israel from 1996 to 1999 and from 2009 to 2021. Netanyahu currently serves as Leader of the Opposition and as the chairman of the Likud – National Liberal Movement. He served in office for a total of 15 years, making him the longest-serving Israeli prime minister in history. He was also the first prime minister to be born in Israel after its Declaration of Independence.[3][4]

Born in Tel Aviv to secular Jewish parents, Netanyahu was raised both in Jerusalem, and for a time in Philadelphia, Pennsylvania in the United States. He returned to Israel in 1967 to join the Israel Defense Forces. He became a team leader in the Sayeret Matkal special forces and took part in several missions, achieving the rank of captain before being honorably discharged. After graduating from the Massachusetts Institute of Technology, Netanyahu became an economic consultant for the Boston Consulting Group. He moved back to Israel in 1978 to found the Yonatan Netanyahu Anti-Terror Institute.

From 1984–1988, Netanyahu was Permanent Representative of Israel to the United Nations. In 1993, he was elected as the Chairman of Likud, becoming Leader of the Opposition. He went on to defeat the incumbent Prime Minister Shimon Peres at the 1996 election, and was appointed as Israel's youngest-ever Prime Minister. After serving a single term, Netanyahu and Likud were heavily defeated in the 1999 election by Ehud Barak's One Israel party; Netanyahu chose to retire from politics entirely, and entered the private sector. However several years later, after his successor as Likud Chairman, Ariel Sharon, became Prime Minister, Netanyahu was convinced to return to politics, and served as Minister of Foreign Affairs and Minister of Finance. As Finance Minister, Netanyahu initiated major reforms of the Israeli economy that were credited by commentators as having significantly improved Israel's subsequent economic performance.[5] Netanyahu later clashed with Sharon, eventually resigning over disagreements regarding the Gaza disengagement plan.

Netanyahu returned to the leadership of Likud in December 2005 after Sharon stepped down to form a new party, Kadima.[6] He was the Leader of the Opposition from 2006 to 2009. Although Likud finished second in the 2009 election to Kadima, Netanyahu was able to form a coalition government with other right-wing parties and was sworn in as Prime Minister for a second time.[7][8][9] He went on to lead Likud to victory in the 2013 and 2015 elections.[10] After the April 2019 election resulted in no party being able to form a government, a second election in 2019 took place. In the September 2019 election, the centrist Blue and White alliance, led by Benny Gantz, emerged slightly ahead of Netanyahu's Likud; however, neither Netanyahu nor Gantz was able to form a government.[11] After continued political deadlock, this was resolved when Likud and Blue and White reached a coalition agreement following the 2020 election. Under the terms of the agreement, the premiership would rotate between Netanyahu and Benny Gantz, in which Gantz was scheduled to succeed Netanyahu in November 2021.[12] In December 2020, this coalition collapsed and a new election was held in March 2021. In his final government, Netanyahu led Israel's response to the COVID-19 pandemic and the 2021 Israel–Palestine crisis. In June 2021, after Naftali Bennett formed a government with Yair Lapid, Netanyahu was removed from the premiership, becoming Leader of the Opposition for the third time.

Netanyahu made his closeness to Donald Trump, a personal friend since the 1980s, central to his political appeal in Israel from 2016.[13] During Trump's presidency, the United States recognized Jerusalem as the capital of Israel, recognized Israeli sovereignty over the Golan Heights, and brokered the Abraham Accords, a series of normalization agreements between Israel and various Arab states. Since December 2016, Netanyahu has been under investigation for corruption by Israeli police and prosecutors.[14] On 21 November 2019, he was indicted on charges of breach of trust, bribery and fraud.[15] Due to the indictment, Netanyahu was legally required to relinquish all of his ministry posts other than the prime minister position prior to his ousting.[16][17]"
driving priviliges,"Under the laws of the United States, it is unlawful to drive a motor vehicle when the ability to do so is materially impaired by the consumption of alcohol or other drugs, including prescription medications.[73] For impaired driving charges involving the consumption of alcohol, the blood alcohol level at which impairment is presumed is 0.08, although it is possible to be convicted of impaired driving with a lower blood alcohol level.[74]

For example, the state of California has two basic drunk driving laws with nearly identical criminal penalties:[75]

V.C. Sec. 23152(a) - it is a misdemeanor to drive under the influence of alcohol or other drugs.
V.C. Sec. 23152(b) - it is a misdemeanor to drive with .08% or more of alcohol in one's blood.
Under the first law, a driver may be convicted of impaired driving based upon their inability to safely operate a motor vehicle, no matter what their blood alcohol level. Under the second law, it is per se unlawful to drive with a blood alcohol level of .08 or greater.

For commercial drivers, a BAC of 0.04 can result in a DUI or DWI charge. In most states, individuals under 21 years of age are subject to a zero tolerance limit and even a small amount of alcohol can lead to a DUI arrest.

In some states, an intoxicated person may be convicted of a DUI in a parked car if the individual is sitting behind the wheel of the car.[76] In some jurisdictions, the occupant of a vehicle might be charged with impaired driving even if sleeping in the back seat based on proof of risk that the occupant would put the vehicle in motion while intoxicated.[77] Some states allow for a charge of attempted DUI if an officer can reasonably infer that the defendant intended to drive a vehicle while impaired.[78]

Repeated impaired driving offenses or an impaired driving incident that results in bodily injury to another may trigger more significant penalties, and potentially trigger a felony charge.[79]

Many states in the US have adopted truth in sentencing laws that enforce strict guidelines on sentencing, differing from previous practice where prison time was reduced or suspended after sentencing had been issued.[80]

Some states allow for conviction for impaired driving based upon a measurement of THC, through blood test or urine testing. For example, in Colorado and Washington, driving with a blood level of THC in excess of 5 nanograms can result in a DUI conviction. In Nevada, the legal THC limit is 2 nanograms. It is also possible for a driver to be convicted of impaired driving based upon the officer's observations of impairment, even if the driver is under the legal limit. In states that have not yet established a THC blood level that triggers a presumption of impaired driving, a driver may similarly be convicted of impaired driving based upon the officer's observations and performance on other sobriety tests."
liberal art degress,liberal art degress
superstitious folklore,superstitious folklore
cold war,"The Cold War was a period of geopolitical tension between the United States and the Soviet Union and their respective allies, the Western Bloc and the Eastern Bloc, which began following World War II. Historians do not fully agree on its starting and ending points, but the period is generally considered to span the 1947 Truman Doctrine (12 March 1947) to the 1991 Dissolution of the Soviet Union (26 December 1991).[1] The term cold war is used because there was no large-scale fighting directly between the two superpowers, but they each supported major regional conflicts known as proxy wars. The conflict was based around the ideological and geopolitical struggle for global influence by these two superpowers, following their temporary alliance and victory against Nazi Germany in 1945.[2] Aside from the nuclear arsenal development and conventional military deployment, the struggle for dominance was expressed via indirect means such as psychological warfare, propaganda campaigns, espionage, far-reaching embargoes, rivalry at sports events and technological competitions such as the Space Race.

The Western Bloc was led by the United States as well as the other First World nations of the Western Bloc that were generally liberal democratic but tied to a network of the authoritarian states, most of which were their former colonies.[3][A] The Eastern Bloc was led by the Soviet Union and its Communist Party, which had an influence across the Second World. The US government supported right-wing governments and uprisings across the world, while the Soviet government funded communist parties and revolutions around the world. As nearly all the colonial states achieved independence in the period 1945–1960, they became Third World battlefields in the Cold War.

The first phase of the Cold War began shortly after the end of the Second World War in 1945. The United States and its allies created the NATO military alliance in 1949 in the apprehension of a Soviet attack and termed their global policy against Soviet influence containment. The Soviet Union formed the Warsaw Pact in 1955 in response to NATO. Major crises of this phase included the 1948–49 Berlin Blockade, the 1927–1949 Chinese Civil War, the 1950–1953 Korean War, the 1956 Hungarian Revolution, the 1956 Suez Crisis, the Berlin Crisis of 1961 and the 1962 Cuban Missile Crisis. The US and the USSR competed for influence in Latin America, the Middle East, and the decolonizing states of Africa and Asia.

Following the Cuban Missile Crisis, a new phase began that saw the Sino-Soviet split between China and the Soviet Union complicate relations within the Communist sphere, while France, a Western Bloc state, began to demand greater autonomy of action. The USSR invaded Czechoslovakia to suppress the 1968 Prague Spring, while the US experienced internal turmoil from the civil rights movement and opposition to the Vietnam War. In the 1960s–70s, an international peace movement took root among citizens around the world. Movements against nuclear arms testing and for nuclear disarmament took place, with large anti-war protests. By the 1970s, both sides had started making allowances for peace and security, ushering in a period of détente that saw the Strategic Arms Limitation Talks and the US opening relations with the People's Republic of China as a strategic counterweight to the USSR. A number of self-proclaimed Marxist regimes were formed in the second half of the 1970s in the Third World, including Angola, Mozambique, Ethiopia, Cambodia, Afghanistan and Nicaragua.

Détente collapsed at the end of the decade with the beginning of the Soviet–Afghan War in 1979. The early 1980s was another period of elevated tension. The United States increased diplomatic, military, and economic pressures on the Soviet Union, at a time when it was already suffering from economic stagnation. In the mid-1980s, the new Soviet leader Mikhail Gorbachev introduced the liberalizing reforms of glasnost (""openness"", c. 1985) and perestroika (""reorganization"", 1987) and ended Soviet involvement in Afghanistan. Pressures for national sovereignty grew stronger in Eastern Europe, and Gorbachev refused to militarily support their governments any longer.

In 1989, the fall of the Iron Curtain after the Pan-European Picnic and a peaceful wave of revolutions (with the exception of Romania and Afghanistan) overthrew almost all communist governments of the Eastern Bloc. The Communist Party of the Soviet Union itself lost control in the Soviet Union and was banned following an abortive coup attempt in August 1991. This in turn led to the formal dissolution of the USSR in December 1991, the declaration of independence of its constituent republics and the collapse of communist governments across much of Africa and Asia. The United States was left as the world's only superpower.

The Cold War and its events have left a significant legacy. It is often referred to in popular culture, especially with themes of espionage and the threat of nuclear warfare."
bartenders during happy hour,"A bartender (also known as a barkeep, barman, barmaid, or a mixologist) is a person who formulates and serves alcoholic or soft drink beverages behind the bar, usually in a licensed establishment. Bartenders also usually maintain the supplies and inventory for the bar. A bartender can generally mix classic cocktails such as a Cosmopolitan, Manhattan, Old Fashioned, and Mojito.

Bartenders are also usually responsible for confirming that customers meet the legal drinking age requirements before serving them alcoholic beverages. In certain countries, such as Canada, the United Kingdom, and Sweden, bartenders are legally required to refuse more alcohol to drunk customers.[1]"
school data,school data
castro brothers,"Joaquin Castro (born September 16, 1974)[1] is an American lawyer and Democratic politician who has represented Texas's 20th congressional district in the United States House of Representatives since 2013. The district includes just over half of his native San Antonio, as well as some of its nearby suburbs. From 2003 to 2013, Castro represented the 125th district in the Texas House of Representatives.[2] While in the state legislature, he served as vice-chair of the Higher Education Committee and was a member of the Judiciary & Civil Jurisprudence Committee. He also previously served on other committees, such as County Affairs, Border & International Affairs, and Juvenile Justice & Family Issues.[2]

Joaquin served as campaign chair for his identical twin brother, Julián Castro, during his 2020 presidential campaign.[3][4] Fidel Alejandro Castro Ruz (/ˈkæstroʊ/;[1] American Spanish: [fiˈðel aleˈxandɾo ˈkastɾo ˈrus]; 13 August 1926 – 25 November 2016) was a Cuban revolutionary, lawyer, and politician who was the leader of Cuba from 1959 to 2008, serving as the prime minister of Cuba from 1959 to 1976 and president from 1976 to 2008. Ideologically a Marxist–Leninist and Cuban nationalist, he also served as the first secretary of the Communist Party of Cuba from 1961 until 2011. Under his administration, Cuba became a one-party communist state; industry and business were nationalized, and state socialist reforms were implemented throughout society."
bargohouti,"Marwan Hasib Ibrahim Barghouti (also transliterated al-Barghuthi; Arabic: مروان حسيب ابراهيم البرغوثي‎; born 6 June 1959) is a Palestinian political figure convicted and imprisoned for murder by an Israeli court.[1] He is regarded as a leader of the First and Second Intifadas. Barghouti at one time supported the peace process, but later became disillusioned, and after 2000 went on to become a leader of the Second Intifada from the West Bank.[1][2] Barghouti was a leader of Tanzim, a paramilitary offshoot of Fatah.[3]

Israeli authorities have called Barghouti a terrorist, accusing him of directing numerous attacks, including suicide bombings, against civilian and military targets alike.[4] Barghouti was arrested by Israel Defense Forces in 2002 in Ramallah.[1] He was tried and convicted on charges of murder, and sentenced to five life sentences. Marwan Barghouti refused to present a defense to the charges brought against him, maintaining throughout that the trial was illegal and illegitimate.

Barghouti still exerts great influence in Fatah from within prison.[5] With popularity reaching further than that, there has been some speculation whether he could be a unifying candidate in a bid to succeed Mahmud Abbas.[6]

In the negotiations over the exchange of Palestinian prisoners for the captured Israeli soldier Gilad Shalit, Hamas insisted on including Barghouti in the deal with Israel.[7][8] However, Israel was unwilling to concede to that demand and despite initial reports that he indeed was to be released in the 11 October 2011 deal between Israel and Hamas, it was soon denied by Israeli sources.[9][10]

In November 2014, Barghouti urged the Palestinian Authority to immediately end security cooperation with Israel and called for a Third Intifada against Israel.[11]"
homeshooling,"Homeschooling or home schooling, also known as home education or elective home education (EHE), is the education of school-aged children at home or a variety of places other than school.[1] Usually conducted by a parent, tutor, or an online teacher, many homeschool families use less formal, more personalized and individualized methods of learning that are not always found in schools. The actual practice of homeschooling can look very different. The spectrum ranges from highly structured forms based on traditional school lessons to more open, free forms such as unschooling, which is a lesson- and curriculum-free implementation of homeschooling. Some families who initially attended a school go through a deschool phase to break away from school habits and prepare for homeschooling. While ""homeschooling"" is the term commonly used in North America, ""home education"" is primarily used in Europe and many Commonwealth countries. Homeschooling shouldn't be confused with distance education, which generally refers to the arrangement where the student is educated by and conforms to the requirements of an online school, rather than being educated independently and unrestrictedly by their parents or by themselves.

Before the introduction of compulsory school attendance laws, most childhood education was done by families and local communities. By the early 19th century, attending a school became the most common means of education in the developed world. In the mid to late 20th century, more people began questioning the efficiency and sustainability of school learning, which again led to an increase in the number of homeschoolers, especially in the Americas and some European countries. Today, homeschooling is a relatively widespread form of education and a legal alternative to public and private schools in many countries, which many people believe is due to the rise of the Internet, which enables people to obtain information very quickly. There are also nations in which homeschooling is regulated or illegal, as recorded in the article Homeschooling international status and statistics. During the COVID-19 pandemic, many students from all over the world had to study from home due to the danger posed by the virus. However, this was mostly implemented in the form of distance education rather than traditional homeschooling.

There are many different reasons for homeschooling, ranging from personal interests to dissatisfaction with the public school system. Some parents see better educational opportunities for their child in homeschooling, for example because they know their child more accurately than a teacher and can concentrate fully on educating usually one to a few persons and therefore can respond more precisely to their individual strengths and weaknesses, or because they think that they can better prepare their children for the life outside of school. Some children can also learn better at home, for example, because they are not held back, disturbed or distracted from school matters, do not feel underchallenged or overwhelmed with certain topics, find that certain temperaments are encouraged in school, while others are inhibited, do not cope well with the very predetermined structure in school or are bullied there. Homeschooling is also an option for families living in remote rural areas, those temporarily abroad, those who travel frequently and therefore face the physical impossibility or difficulty of getting their children into school and families who want to spend more and better time with their children. Health reasons and special needs can also play a role in why children cannot attend a school regularly and are at least partially homeschooled.

Critics of homeschooling argue that children may lack social contact at home, possibly resulting in children having poorer social skills. Some are also concerned that some parents may not have the skills required to guide and advise their children in life skills. Critics also say that a child might not encounter people of other cultures, worldviews, and socioeconomic groups if they are not enrolled in a school. Therefore, these critics believe that homeschooling cannot guarantee a comprehensive and neutral education and children can be indoctrinated and manipulated when there is no external influence and surveillance by controlling authorities. There are many studies that show that homeschooled children score better on standardized tests and have equal or higher developed social skills and participate more in cultural and family activities on average than public school students.[2][3] In addition, studies suggest that homeschoolers are generally more likely to have higher self-esteem, deeper friendships, and better relationships with adults, and are less susceptible to peer pressure.[4][3]"
tax increases,tax increases
not adopting cats that you cn not take care of.,not adopting cats that you can not take care of
eating habits,eating habits
ultra-conservatism,"Conservatism in the United States is a political and social philosophy which characteristically prioritizes American traditions, republicanism, and limited federal governmental power in relation to the states, referred to more simply as limited government and states' rights. It typically supports Christian values,[1] moral absolutism,[2] traditional family values,[3] American exceptionalism,[4] and individualism.[5] It is generally pro-capitalist[6] and pro-business while opposing trade unions. It often advocates for a strong national defense, gun rights, free trade,[7] and a defense of Western culture from perceived threats posed by communism,[8] socialism, and moral relativism.[9]

American conservatives generally consider individual liberty—within the bounds of conservative values—as the fundamental trait of democracy.[10][11] They typically believe in a balance between federal government and states' rights. Apart from some right-libertarians, American conservatives tend to favor strong action in areas they believe to be within government's legitimate jurisdiction, particularly national defense and law enforcement. Social conservatives—many of them religious—often oppose abortion, civil unions, and same-sex marriage. They often favor Christian prayer in public schools and government funding for private Christian schools.[12][13][3][14]

Like most political ideologies in the United States, conservatism originates from republicanism, which rejects aristocratic and monarchical government and upholds the principles of the 1776 U.S. Declaration of Independence (""that all men are created equal, that they are endowed by their Creator with certain unalienable Rights, that among these are Life, Liberty and the Pursuit of Happiness"") and of the U.S. Constitution (which established a federal republic under the rule of law). Conservative philosophy also derives in part from the classical liberal tradition of the 18th and 19th centuries, which advocated for laissez-faire economics (i.e. economic freedom and deregulation).[15][16]

While historians such as Patrick Allitt (1956–) and political theorists such as Russell Kirk (1918–1994) assert that conservative principles have played a major role in U.S. politics and culture since 1776, they also argue that an organized conservative movement—with beliefs that differ from those of other American political parties—did not emerge in the U.S. until the 1950s.[17][18][19] The recent movement conservatism has its base in the Republican Party, which has adopted conservative policies since the 1950s; Southern Democrats also became important early figures in the movement's history.[20][21][22][23] In 1937, conservative Republicans and Southern Democrats formed the congressional conservative coalition, which played an influential role in Congress from the late 1930s to the mid 1960s. In recent decades Southern conservatives vote heavily Republican."
conservativism on education,"Conservatism in the United States is a political and social philosophy which characteristically prioritizes American traditions, republicanism, and limited federal governmental power in relation to the states, referred to more simply as limited government and states' rights. It typically supports Christian values,[1] moral absolutism,[2] traditional family values,[3] American exceptionalism,[4] and individualism.[5] It is generally pro-capitalist[6] and pro-business while opposing trade unions. It often advocates for a strong national defense, gun rights, free trade,[7] and a defense of Western culture from perceived threats posed by communism,[8] socialism, and moral relativism.[9]

American conservatives generally consider individual liberty—within the bounds of conservative values—as the fundamental trait of democracy.[10][11] They typically believe in a balance between federal government and states' rights. Apart from some right-libertarians, American conservatives tend to favor strong action in areas they believe to be within government's legitimate jurisdiction, particularly national defense and law enforcement. Social conservatives—many of them religious—often oppose abortion, civil unions, and same-sex marriage. They often favor Christian prayer in public schools and government funding for private Christian schools.[12][13][3][14]

Like most political ideologies in the United States, conservatism originates from republicanism, which rejects aristocratic and monarchical government and upholds the principles of the 1776 U.S. Declaration of Independence (""that all men are created equal, that they are endowed by their Creator with certain unalienable Rights, that among these are Life, Liberty and the Pursuit of Happiness"") and of the U.S. Constitution (which established a federal republic under the rule of law). Conservative philosophy also derives in part from the classical liberal tradition of the 18th and 19th centuries, which advocated for laissez-faire economics (i.e. economic freedom and deregulation).[15][16]

While historians such as Patrick Allitt (1956–) and political theorists such as Russell Kirk (1918–1994) assert that conservative principles have played a major role in U.S. politics and culture since 1776, they also argue that an organized conservative movement—with beliefs that differ from those of other American political parties—did not emerge in the U.S. until the 1950s.[17][18][19] The recent movement conservatism has its base in the Republican Party, which has adopted conservative policies since the 1950s; Southern Democrats also became important early figures in the movement's history.[20][21][22][23] In 1937, conservative Republicans and Southern Democrats formed the congressional conservative coalition, which played an influential role in Congress from the late 1930s to the mid 1960s. In recent decades Southern conservatives vote heavily Republican."
fairness,fairness
cat adoption,cat adoption
distraction,distraction
peace in the middke east,"The Middle East (Arabic: الشرق الأوسط‎, ISO 233: ash-Sharq al-Awsat) is a geopolitical term[2] that commonly refers to the region spanning the Levant, Arabian Peninsula, Anatolia (including modern Turkey and Cyprus), Egypt, Iran and Iraq. The term came into widespread usage as a replacement of the term Near East (as opposed to the Far East) beginning in the early 20th century. The term ""Middle East"" has led to some confusion over its changing definitions, and has been viewed by some to be discriminatory[3] or too Eurocentric.[4] The region includes the vast majority of the territories included in the closely associated definition of Western Asia, but without the Caucasus and including all of Egypt, and not just the Sinai Peninsula.

Most Middle Eastern countries (13 out of 18) are part of the Arab world. The most populous countries in the region are Egypt, Iran, and Turkey, while Saudi Arabia is the largest Middle Eastern country by area. The history of the Middle East dates back to ancient times, with the geopolitical importance of the region being recognized for millennia.[5][6][7] Several major religions have their origins in the Middle East, including Judaism, Christianity, and Islam. Arabs constitute the main socioethnic grouping in the region,[8] followed by Turks, Persians, Kurds, Azeris, Copts, Jews, Assyrians, Iraqi Turkmen, and Greek Cypriots.

The Middle East generally has a hot, arid climate, especially in the Peninsula and Egyptian regions. Several major rivers providing irrigation to support agriculture in limited areas here such as the Nile Delta in Egypt, the Tigris and Euphrates watersheds of Mesopotamia (Iraq, Kuwait, and eastern Syria), and most of what is known as the Fertile Crescent. Conversely the Levantine coast and most of Turkey have more temperate, oceanic and wetter climates. Most of the countries that border the Persian Gulf have vast reserves of petroleum, with monarchs of the Arabian Peninsula in particular benefiting economically from petroleum exports. Because of the arid climate and heavy reliance on the fossil fuel industry, the Middle East is both a heavy contributor to climate change and a region expected to be severely negatively impacted by it.

Other concepts of the region exist including the broader the Middle East and North Africa (MENA), which includes states of the Maghreb and Sudan, or the ""Greater Middle East"" which additionally also includes parts of East Africa, Afghanistan, Pakistan, and sometimes Transcaucasia and Central Asia."
national independence,national independence
new york times,"The New York Times (N.Y.T. or N.Y. Times) is an American daily newspaper based in New York City with a worldwide readership.[7][8] Founded in 1851, the Times has since won 132 Pulitzer Prizes (the most of any newspaper),[9] and has long been regarded within the industry as a national ""newspaper of record"".[10] It is ranked 18th in the world by circulation and 3rd in the U.S.[11]

The paper is owned by The New York Times Company, which is publicly traded. It has been governed by the Sulzberger family since 1896, through a dual-class share structure after its shares became publicly traded.[12] A. G. Sulzberger and his father, Arthur Ochs Sulzberger Jr.—the paper's publisher and the company's chairman, respectively—are the fifth and fourth generation of the family to head the paper.[13]

Since the mid-1970s, The New York Times has expanded its layout and organization, adding special weekly sections on various topics supplementing the regular news, editorials, sports, and features. Since 2008,[14] the Times has been organized into the following sections: News, Editorials/Opinions-Columns/Op-Ed, New York (metropolitan), Business, Sports, Arts, Science, Styles, Home, Travel, and other features.[15] On Sundays, the Times is supplemented by the Sunday Review (formerly the Week in Review),[16] The New York Times Book Review,[17] The New York Times Magazine,[18] and T: The New York Times Style Magazine.[19]"
against declawing your cat if your kid has autism,"Autism is a neurodevelopmental disorder characterized by difficulties with social interaction and communication, and by restricted and repetitive behavior.[3] Parents often notice signs during the first three years of their child's life.[1][3] These signs often develop gradually, though some autistic children experience regression in their communication and social skills after reaching developmental milestones at a normal pace.[13]

Autism is associated with a combination of genetic and environmental factors.[4] Risk factors during pregnancy include certain infections, such as rubella, toxins including valproic acid, alcohol, cocaine, pesticides, lead, and air pollution, fetal growth restriction, and autoimmune diseases.[14][15][16] Controversies surround other proposed environmental causes; for example, the vaccine hypothesis, which has been disproven.[17][18] Autism affects information processing in the brain and how nerve cells and their synapses connect and organize; how this occurs is not well understood.[19] The Diagnostic and Statistical Manual of Mental Disorders (DSM-5) combines forms of the condition, including Asperger syndrome and pervasive developmental disorder not otherwise specified (PDD-NOS) into the diagnosis of autism spectrum disorder (ASD).[3][20]

Several interventions have been shown to reduce symptoms and improve the ability of autistic people to function and participate independently in the community.[21] Behavioral, psychological, education, and/or skill-building interventions may be used to assist autistic people to learn life skills necessary for living independently, as well as other social, communication, and language skills. Therapy also aims to reduce challenging behaviors and build upon strengths.[22] Some autistic adults are unable to live independently.[23] An autistic culture has developed, with some individuals seeking a cure and others believing autism should be accepted as a difference to be accommodated instead of cured.[24]

Globally, autism is estimated to affect 24.8 million people as of 2015.[12] In the 2000s, the number of autistic people worldwide was estimated at 1–2 per 1,000 people.[25] In the developed countries, about 1.5% of children are diagnosed with ASD as of 2017,[26] from 0.7% in 2000 in the United States.[27] It is diagnosed four-to-five times more often in males than females.[27] The number of people diagnosed has increased considerably since the 1990s, which may be partly due to increased recognition of the condition.[28]"
animal rights.,"Animal rights is the philosophy according to which all animals are entitled to the possession of their own existence and that their most basic interests—such as the need to avoid suffering—should be afforded the same consideration as similar interests of human beings.[2] That is, all species of animals have the right to be treated as individuals, with their own desires and needs, rather than as unfeeling property.[3]

Advocates for animal rights oppose the assignment of moral value and fundamental protections on the basis of species membership alone—an idea known as speciesism since 1970, when Richard D. Ryder adopted the term[4]—arguing that it is a prejudice as irrational as any other.[5] They maintain that animals should no longer be viewed as property or used as food, clothing, research subjects, entertainment, or beasts of burden.[6] Multiple cultural traditions around the world such as Jainism, Taoism, Hinduism, Buddhism, Shintoism and Animism also espouse some forms of animal rights.

In parallel to the debate about moral rights, law schools in North America now often teach animal law,[7] and several legal scholars, such as Steven M. Wise and Gary L. Francione, support the extension of basic legal rights and personhood to non-human animals. The animals most often considered in arguments for personhood are hominids. Some animal-rights academics support this because it would break through the species barrier, but others oppose it because it predicates moral value on mental complexity, rather than on sentience alone.[8] As of November  2019, 29 countries had enacted bans on hominoid experimentation; Argentina has granted a captive orangutan basic human rights since 2014.[9]

Outside the order of primates, animal-rights discussions most often address the status of mammals (compare charismatic megafauna). Other animals (considered less sentient) have gained less attention; insects relatively little[10] (outside Jainism), and animal-like bacteria (despite their overwhelming numbers) hardly any.[11]

Critics of animal rights argue that nonhuman animals are unable to enter into a social contract, and thus cannot be possessors of rights, a view summed up by the philosopher Roger Scruton (1944–2020), who writes that only humans have duties, and therefore only humans have rights.[12] Another argument, associated with the utilitarian tradition, maintains that animals may be used as resources so long as there is no unnecessary suffering;[13] animals may have some moral standing, but they are inferior in status to human beings, and any interests they have may be overridden, though what counts as ""necessary"" suffering or a legitimate sacrifice of interests can vary considerably.[14] Certain forms of animal-rights activism, such as the destruction of fur farms and of animal laboratories by the Animal Liberation Front, have attracted criticism, including from within the animal-rights movement itself,[15] and have prompted reaction from the U.S. Congress with the enactment of laws, including the Animal Enterprise Terrorism Act, allowing the prosecution of this sort of activity as terrorism.[16]"
flat-fee dining,"A flat fee, also referred to as a flat rate or a linear rate refers to a pricing structure that charges a single fixed fee for a service, regardless of usage.[1] Less commonly, the term may refer to a rate that does not vary with usage or time of use."
gambling casino,"Gambling (also known as betting) is the wagering something of value (""the stakes"") on an event with an uncertain outcome with the intent of winning something else of value. Gambling thus requires three elements to be present: consideration (an amount wagered), risk (chance), and a prize.[1] The outcome of the wager is often immediate, such as a single roll of dice, a spin of a roulette wheel, or a horse crossing the finish line, but longer time frames are also common, allowing wagers on the outcome of a future sports contest or even an entire sports season.

The term ""gaming""[2] in this context typically refers to instances in which the activity has been specifically permitted by law. The two words are not mutually exclusive; i.e., a ""gaming"" company offers (legal) ""gambling"" activities to the public[3] and may be regulated by one of many gaming control boards, for example, the Nevada Gaming Control Board. However, this distinction is not universally observed in the English-speaking world. For instance, in the United Kingdom, the regulator of gambling activities is called the Gambling Commission (not the Gaming Commission).[4] The word gaming is used more frequently since the rise of computer and video games to describe activities that do not necessarily involve wagering, especially online gaming, with the new usage still not having displaced the old usage as the primary definition in common dictionaries. ""Gaming"" has also been used to circumvent laws against ""gambling"". The media and others have used one term or the other to frame conversations around the subjects, resulting in a shift of perceptions among their audiences.[5]

Gambling is also a major international commercial activity, with the legal gambling market totaling an estimated $335 billion in 2009.[6] In other forms, gambling can be conducted with materials that have a value, but are not real money. For example, players of marbles games might wager marbles, and likewise games of Pogs or Magic: The Gathering can be played with the collectible game pieces (respectively, small discs and trading cards) as stakes, resulting in a meta-game regarding the value of a player's collection of pieces."
length of time it took linda rondstat to be inducted,"Linda Maria Ronstadt (born July 15, 1946) is a retired American singer who performed and recorded in diverse genres including rock, country, light opera, and Latin. She has earned 11 Grammy Awards,[2] three American Music Awards, two Academy of Country Music awards, an Emmy Award, and an ALMA Award. Many of her albums have been certified gold, platinum or multiplatinum in the United States and internationally. She has also earned nominations for a Tony Award and a Golden Globe award. She was awarded the Latin Grammy Lifetime Achievement Award by the Latin Recording Academy in 2011 and also awarded the Grammy Lifetime Achievement Award by the Recording Academy in 2016. She was inducted into the Rock and Roll Hall of Fame in April 2014.[3] On July 28, 2014, she was awarded the National Medal of Arts and Humanities.[4][15] In 2019, she received a star jointly with Dolly Parton and Emmylou Harris on the Hollywood Walk of Fame for their work as the group Trio.[16][17] Ronstadt was among five honorees who received the 2019 Kennedy Center Honors for lifetime artistic achievements.

Ronstadt has released 24 studio albums and 15 compilation or greatest hits albums. She charted 38 US Billboard Hot 100 singles. Twenty-one of those singles reached the top 40, ten reached the top 10, and one reached number one (""You're No Good""). Her success, however, did not translate across the Atlantic to the UK. Although Ronstadt's duets, ""Somewhere Out There"" with James Ingram and ""Don't Know Much"" with Aaron Neville, peaked at numbers 8 and 2 respectively in 1987 and 1989, the single ""Blue Bayou"" was her only solo single to reach the UK Top 40.[18][19] She has charted 36 albums, ten top-10 albums, and three number 1 albums on the US Billboard Pop Album Chart.

Ronstadt has collaborated with artists in diverse genres, including Bette Midler, Billy Eckstine,[20] Frank Zappa, Carla Bley (Escalator Over the Hill), Rosemary Clooney, Flaco Jiménez, Philip Glass, Warren Zevon, Emmylou Harris, Gram Parsons, Dolly Parton, Neil Young, Paul Simon, Earl Scruggs, Johnny Cash, and Nelson Riddle. She has lent her voice to over 120 albums and has sold more than 100 million records, making her one of the world's best-selling artists of all time.[21][22] Christopher Loudon, of Jazz Times, wrote in 2004 that Ronstadt is ""blessed with arguably the most sterling set of pipes of her generation.""[23]

Ronstadt reduced her activity after 2000 when she felt her singing voice deteriorating,[24] releasing her last full-length album in 2004 and performing her last live concert in 2009. She announced her retirement in 2011 and revealed shortly afterwards that she is no longer able to sing as a result of a degenerative condition later determined to be progressive supranuclear palsy.[24][a] Since then, Ronstadt has continued to make public appearances, going on a number of public speaking tours in the 2010s. She published an autobiography, Simple Dreams: A Musical Memoir,[25] in September 2013. A documentary based on her memoirs, Linda Ronstadt: The Sound of My Voice, was released in 2019."
linda rondstadt induction in 2014,"Linda Maria Ronstadt (born July 15, 1946) is a retired American singer who performed and recorded in diverse genres including rock, country, light opera, and Latin. She has earned 11 Grammy Awards,[2] three American Music Awards, two Academy of Country Music awards, an Emmy Award, and an ALMA Award. Many of her albums have been certified gold, platinum or multiplatinum in the United States and internationally. She has also earned nominations for a Tony Award and a Golden Globe award. She was awarded the Latin Grammy Lifetime Achievement Award by the Latin Recording Academy in 2011 and also awarded the Grammy Lifetime Achievement Award by the Recording Academy in 2016. She was inducted into the Rock and Roll Hall of Fame in April 2014.[3] On July 28, 2014, she was awarded the National Medal of Arts and Humanities.[4][15] In 2019, she received a star jointly with Dolly Parton and Emmylou Harris on the Hollywood Walk of Fame for their work as the group Trio.[16][17] Ronstadt was among five honorees who received the 2019 Kennedy Center Honors for lifetime artistic achievements.

Ronstadt has released 24 studio albums and 15 compilation or greatest hits albums. She charted 38 US Billboard Hot 100 singles. Twenty-one of those singles reached the top 40, ten reached the top 10, and one reached number one (""You're No Good""). Her success, however, did not translate across the Atlantic to the UK. Although Ronstadt's duets, ""Somewhere Out There"" with James Ingram and ""Don't Know Much"" with Aaron Neville, peaked at numbers 8 and 2 respectively in 1987 and 1989, the single ""Blue Bayou"" was her only solo single to reach the UK Top 40.[18][19] She has charted 36 albums, ten top-10 albums, and three number 1 albums on the US Billboard Pop Album Chart.

Ronstadt has collaborated with artists in diverse genres, including Bette Midler, Billy Eckstine,[20] Frank Zappa, Carla Bley (Escalator Over the Hill), Rosemary Clooney, Flaco Jiménez, Philip Glass, Warren Zevon, Emmylou Harris, Gram Parsons, Dolly Parton, Neil Young, Paul Simon, Earl Scruggs, Johnny Cash, and Nelson Riddle. She has lent her voice to over 120 albums and has sold more than 100 million records, making her one of the world's best-selling artists of all time.[21][22] Christopher Loudon, of Jazz Times, wrote in 2004 that Ronstadt is ""blessed with arguably the most sterling set of pipes of her generation.""[23]

Ronstadt reduced her activity after 2000 when she felt her singing voice deteriorating,[24] releasing her last full-length album in 2004 and performing her last live concert in 2009. She announced her retirement in 2011 and revealed shortly afterwards that she is no longer able to sing as a result of a degenerative condition later determined to be progressive supranuclear palsy.[24][a] Since then, Ronstadt has continued to make public appearances, going on a number of public speaking tours in the 2010s. She published an autobiography, Simple Dreams: A Musical Memoir,[25] in September 2013. A documentary based on her memoirs, Linda Ronstadt: The Sound of My Voice, was released in 2019."
linda rondstadt,"Linda Maria Ronstadt (born July 15, 1946) is a retired American singer who performed and recorded in diverse genres including rock, country, light opera, and Latin. She has earned 11 Grammy Awards,[2] three American Music Awards, two Academy of Country Music awards, an Emmy Award, and an ALMA Award. Many of her albums have been certified gold, platinum or multiplatinum in the United States and internationally. She has also earned nominations for a Tony Award and a Golden Globe award. She was awarded the Latin Grammy Lifetime Achievement Award by the Latin Recording Academy in 2011 and also awarded the Grammy Lifetime Achievement Award by the Recording Academy in 2016. She was inducted into the Rock and Roll Hall of Fame in April 2014.[3] On July 28, 2014, she was awarded the National Medal of Arts and Humanities.[4][15] In 2019, she received a star jointly with Dolly Parton and Emmylou Harris on the Hollywood Walk of Fame for their work as the group Trio.[16][17] Ronstadt was among five honorees who received the 2019 Kennedy Center Honors for lifetime artistic achievements.

Ronstadt has released 24 studio albums and 15 compilation or greatest hits albums. She charted 38 US Billboard Hot 100 singles. Twenty-one of those singles reached the top 40, ten reached the top 10, and one reached number one (""You're No Good""). Her success, however, did not translate across the Atlantic to the UK. Although Ronstadt's duets, ""Somewhere Out There"" with James Ingram and ""Don't Know Much"" with Aaron Neville, peaked at numbers 8 and 2 respectively in 1987 and 1989, the single ""Blue Bayou"" was her only solo single to reach the UK Top 40.[18][19] She has charted 36 albums, ten top-10 albums, and three number 1 albums on the US Billboard Pop Album Chart.

Ronstadt has collaborated with artists in diverse genres, including Bette Midler, Billy Eckstine,[20] Frank Zappa, Carla Bley (Escalator Over the Hill), Rosemary Clooney, Flaco Jiménez, Philip Glass, Warren Zevon, Emmylou Harris, Gram Parsons, Dolly Parton, Neil Young, Paul Simon, Earl Scruggs, Johnny Cash, and Nelson Riddle. She has lent her voice to over 120 albums and has sold more than 100 million records, making her one of the world's best-selling artists of all time.[21][22] Christopher Loudon, of Jazz Times, wrote in 2004 that Ronstadt is ""blessed with arguably the most sterling set of pipes of her generation.""[23]

Ronstadt reduced her activity after 2000 when she felt her singing voice deteriorating,[24] releasing her last full-length album in 2004 and performing her last live concert in 2009. She announced her retirement in 2011 and revealed shortly afterwards that she is no longer able to sing as a result of a degenerative condition later determined to be progressive supranuclear palsy.[24][a] Since then, Ronstadt has continued to make public appearances, going on a number of public speaking tours in the 2010s. She published an autobiography, Simple Dreams: A Musical Memoir,[25] in September 2013. A documentary based on her memoirs, Linda Ronstadt: The Sound of My Voice, was released in 2019."
induction of linda rondstadt,"Linda Maria Ronstadt (born July 15, 1946) is a retired American singer who performed and recorded in diverse genres including rock, country, light opera, and Latin. She has earned 11 Grammy Awards,[2] three American Music Awards, two Academy of Country Music awards, an Emmy Award, and an ALMA Award. Many of her albums have been certified gold, platinum or multiplatinum in the United States and internationally. She has also earned nominations for a Tony Award and a Golden Globe award. She was awarded the Latin Grammy Lifetime Achievement Award by the Latin Recording Academy in 2011 and also awarded the Grammy Lifetime Achievement Award by the Recording Academy in 2016. She was inducted into the Rock and Roll Hall of Fame in April 2014.[3] On July 28, 2014, she was awarded the National Medal of Arts and Humanities.[4][15] In 2019, she received a star jointly with Dolly Parton and Emmylou Harris on the Hollywood Walk of Fame for their work as the group Trio.[16][17] Ronstadt was among five honorees who received the 2019 Kennedy Center Honors for lifetime artistic achievements.

Ronstadt has released 24 studio albums and 15 compilation or greatest hits albums. She charted 38 US Billboard Hot 100 singles. Twenty-one of those singles reached the top 40, ten reached the top 10, and one reached number one (""You're No Good""). Her success, however, did not translate across the Atlantic to the UK. Although Ronstadt's duets, ""Somewhere Out There"" with James Ingram and ""Don't Know Much"" with Aaron Neville, peaked at numbers 8 and 2 respectively in 1987 and 1989, the single ""Blue Bayou"" was her only solo single to reach the UK Top 40.[18][19] She has charted 36 albums, ten top-10 albums, and three number 1 albums on the US Billboard Pop Album Chart.

Ronstadt has collaborated with artists in diverse genres, including Bette Midler, Billy Eckstine,[20] Frank Zappa, Carla Bley (Escalator Over the Hill), Rosemary Clooney, Flaco Jiménez, Philip Glass, Warren Zevon, Emmylou Harris, Gram Parsons, Dolly Parton, Neil Young, Paul Simon, Earl Scruggs, Johnny Cash, and Nelson Riddle. She has lent her voice to over 120 albums and has sold more than 100 million records, making her one of the world's best-selling artists of all time.[21][22] Christopher Loudon, of Jazz Times, wrote in 2004 that Ronstadt is ""blessed with arguably the most sterling set of pipes of her generation.""[23]

Ronstadt reduced her activity after 2000 when she felt her singing voice deteriorating,[24] releasing her last full-length album in 2004 and performing her last live concert in 2009. She announced her retirement in 2011 and revealed shortly afterwards that she is no longer able to sing as a result of a degenerative condition later determined to be progressive supranuclear palsy.[24][a] Since then, Ronstadt has continued to make public appearances, going on a number of public speaking tours in the 2010s. She published an autobiography, Simple Dreams: A Musical Memoir,[25] in September 2013. A documentary based on her memoirs, Linda Ronstadt: The Sound of My Voice, was released in 2019."
non-isolated electricity,"A non-isolated power supply is usually >95 % efficient and more compact than an isolated power supply. The downside is, there is no isolation between the input and output. Non-isolated power supplies are commonly board mounted near the load they are driving and are referred to as point of load (POL) power supplies. They are usually downstream from a power supply that is isolated which helps with safety concerns."
electricity,electricity
model t ford-,"The Ford Model T (colloquially known as the ""tin Lizzie,"" ""leaping Lena,"" ""jitney"" or ""flivver"") is an automobile produced by Ford Motor Company from October 1, 1908, to May 26, 1927.[10][11] It is generally regarded as the first affordable automobile, which made car travel available to middle-class Americans. The relatively low price was partly the result of Ford's efficient fabrication, including assembly line production instead of individual handcrafting.[12]

The Ford Model T was named the most influential car of the 20th century in the 1999 Car of the Century competition, ahead of the BMC Mini, Citroën DS, and Volkswagen Beetle.[13] Ford's Model T was successful not only because it provided inexpensive transportation on a massive scale, but also because the car signified innovation for the rising middle class and became a powerful symbol of the United States' age of modernization.[14] With 15 million sold, it was the most sold car in history before being surpassed by the Volkswagen Beetle in 1972,[15] and still stood eighth on the top-ten list, as of 2012.[16]"
king george vi,"George VI (Albert Frederick Arthur George; 14 December 1895 – 6 February 1952) was King of the United Kingdom and the Dominions of the British Commonwealth from 11 December 1936 until his death in 1952. He was concurrently the last Emperor of India until August 1947, when the British Raj was dissolved.

Known as ""Bertie"" among his family and close friends, George VI was born in the reign of his great-grandmother Queen Victoria and was named after his great-grandfather Albert, Prince Consort. As the second son of King George V, he was not expected to inherit the throne and spent his early life in the shadow of his elder brother, Edward. He attended naval college as a teenager and served in the Royal Navy and Royal Air Force during the First World War. In 1920, he was made Duke of York. He married Lady Elizabeth Bowes-Lyon in 1923, and they had two daughters, Elizabeth and Margaret. In the mid-1920s, he had speech therapy for a stammer, which he learned to manage to some degree. George's elder brother ascended the throne as Edward VIII after their father died in 1936. Later that year, Edward abdicated to marry the twice-divorced American socialite Wallis Simpson, and George became the third monarch of the House of Windsor.

In September 1939, the British Empire and Commonwealth—but not Ireland—declared war on Nazi Germany. War with the Kingdom of Italy and the Empire of Japan followed in 1940 and 1941, respectively. George was seen as sharing the hardships of the common people and his popularity soared. Buckingham Palace was bombed during the Blitz while the King and Queen were there, and his younger brother, the Duke of Kent, was killed on active service. George became known as a symbol of British determination to win the war. Britain and its allies were victorious in 1945, but the British Empire declined. Ireland had largely broken away, followed by the independence of India and Pakistan in 1947. George relinquished the title of Emperor of India in June 1948 and instead adopted the new title of Head of the Commonwealth. He was beset by smoking-related health problems in the later years of his reign and died of a coronary thrombosis in 1952. He was succeeded by his daughter, Elizabeth II."
duel nationals,"Multiple/dual citizenship (or multiple/dual nationality) is a legal status in which a person is concurrently regarded as a national or citizen of more than one country under the laws of those countries.[1] Conceptually, citizenship is focused on the internal political life of the country and nationality is a matter of international dealings.[2] There is no international convention which determines the nationality or citizenship status of a person. This is defined exclusively by national laws, which can vary and conflict with each other. Multiple citizenship arises because different countries use different, and not necessarily mutually exclusive, criteria for citizenship. Colloquially, people may ""hold"" multiple citizenship but, technically, each nation makes a claim that a particular person is considered its national.

A person holding multiple citizenship is, generally, entitled to the rights of citizenship in each country whose citizenship they are holding (such as right to a passport, right to enter the country, right to residence and work, right to vote, etc.), but may also be subject to obligations of citizenship (such as a potential obligation for national service, becoming subject to taxation on worldwide income, etc.).

Some countries do not permit dual citizenship or only do in certain cases (e.g. inheriting multiple nationalities at birth). This may be by requiring an applicant for naturalization to renounce all existing citizenship, or by withdrawing its citizenship from someone who voluntarily acquires another citizenship, or by other devices. Some countries permit a renunciation of citizenship, while others do not. Some countries permit a general dual citizenship while others permit dual citizenship but only of a limited number of countries.

A country that allows dual citizenship may still not recognize the other citizenship of its nationals within its own territory (for example, in relation to entry into the country, national service, duty to vote, etc.). Similarly, it may not permit consular access by another country for a person who is also its national. Some countries prohibit dual citizenship holders from serving in their armed forces or on police forces or holding certain public offices.[3]"
how trump is addicted to chaos and attention,"Donald John Trump (born June 14, 1946) is an American politician, media personality, and businessman who served as the 45th president of the United States from 2017 to 2021.

Born and raised in Queens, New York City, Trump graduated from the Wharton School of the University of Pennsylvania with a bachelor's degree in 1968. He became the president of his father Fred Trump's real estate business in 1971 and renamed it The Trump Organization. Trump expanded the company's operations to building and renovating skyscrapers, hotels, casinos, and golf courses. He later started various side ventures, mostly by licensing his name. Trump and his businesses have been involved in more than 4,000 state and federal legal actions, including six bankruptcies. He owned the Miss Universe brand of beauty pageants from 1996 to 2015. From 2004 to 2015, he co-produced and hosted the reality television series The Apprentice.

Trump's political positions have been described as populist, protectionist, isolationist, and nationalist. He entered the 2016 presidential race as a Republican and was elected in an upset victory over Democratic nominee Hillary Clinton while losing the popular vote,[a] becoming the first U.S. president without prior military or government service. His election and policies sparked numerous protests. Trump made many false and misleading statements during his campaigns and presidency, to a degree unprecedented in American politics, and promoted conspiracy theories. Many of his comments and actions have been characterized as racially charged or racist, and many as misogynistic.

Trump ordered a travel ban on citizens from several Muslim-majority countries, diverted funding towards building a wall on the U.S.–Mexico border, and implemented a policy of family separations for apprehended migrants. He signed the Tax Cuts and Jobs Act of 2017, which cut taxes for individuals and businesses and rescinded the individual health insurance mandate penalty of the Affordable Care Act. He appointed more than 200 federal judges, including three to the Supreme Court: Neil Gorsuch, Brett Kavanaugh and Amy Coney Barrett. In foreign policy, Trump pursued an America First agenda: he renegotiated the North American Free Trade Agreement as the United States–Mexico–Canada Agreement and withdrew the U.S. from the Trans-Pacific Partnership trade negotiations, the Paris Agreement on climate change and the Iran nuclear deal. He imposed import tariffs that triggered a trade war with China. Trump met three times with North Korean leader Kim Jong-un but made no progress on denuclearization. He reacted slowly to the COVID-19 pandemic, ignored or contradicted many recommendations from health officials in his messaging, and promoted misinformation about unproven treatments and the availability of testing.

The special counsel investigation led by Robert Mueller established that Russia interfered in the 2016 election to benefit the Trump campaign but not that members of the Trump campaign conspired or coordinated with Russian election interference activities. Mueller also investigated Trump for obstruction of justice and neither indicted nor exonerated him. After Trump pressured Ukraine to investigate his political rival Joe Biden, the House of Representatives impeached him for abuse of power and obstruction of Congress in December 2019. The Senate acquitted him of both charges in February 2020.

Trump lost the 2020 presidential election to Biden but refused to concede. He falsely claimed that there was widespread electoral fraud and attempted to overturn the results, pressuring government officials, mounting scores of unsuccessful legal challenges, and obstructing the presidential transition. On January 6, 2021, Trump urged his supporters to march to the Capitol, which hundreds then attacked, resulting in multiple deaths and interrupting the electoral vote count. On January 13, the House of Representatives impeached Trump a second time, for incitement of insurrection, making him the only federal officeholder in American history to be impeached twice. The Senate acquitted Trump again on February 13, after he had already left office. Scholars and historians rank Trump as one of the worst presidents in American history."
prostituition,"Prostitution is illegal in the vast majority of the United States as a result of state laws rather than federal laws. It is, however, legal in some rural counties within the state of Nevada. Prostitution nevertheless occurs elsewhere in the country.

The regulation of prostitution in the country is not among the enumerated powers of the federal government. It is therefore exclusively the domain of the states to permit, prohibit, or otherwise regulate commercial sex under the Tenth Amendment to the United States Constitution, except insofar as Congress may regulate it as part of interstate commerce with laws such as the Mann Act. In most states, prostitution is considered a misdemeanor in the category of public order crime–crime that disrupts the order of a community. Prostitution was at one time considered a vagrancy crime.

Currently, Nevada is the only U.S. jurisdiction to allow legal prostitution – in the form of regulated brothels – the terms of which are stipulated in the Nevada Revised Statutes. Only eight counties currently contain active brothels. All forms of prostitution are illegal in these counties: Clark (which contains the Las Vegas–Paradise metropolitan area), Washoe (which contains Reno), Carson City, Douglas, Eureka, Lincoln & Pershing. The other counties theoretically allow brothel prostitution, but three of these counties currently have no active brothels. Street prostitution, ""pandering"", and living off of the proceeds of a prostitute remain illegal under Nevada law, as is the case elsewhere in the country.

According to the National Institute of Justice, a study conducted in 2008 alleged that approximately 15-20 percent of men in the country have engaged in commercial sex.[1]

As with other countries, prostitution in the U.S. can be divided into three broad categories: street prostitution, brothel prostitution, and escort prostitution."
suvsidies,"A subsidy or government incentive is a form of financial aid or support extended to an economic sector (business, or individual) generally with the aim of promoting economic and social policy.[1] Although commonly extended from the government, the term subsidy can relate to any type of support – for example from NGOs or as implicit subsidies. Subsidies come in various forms including: direct (cash grants, interest-free loans) and indirect (tax breaks, insurance, low-interest loans, accelerated depreciation, rent rebates).[2][3]

Furthermore, they can be broad or narrow, legal or illegal, ethical or unethical. The most common forms of subsidies are those to the producer or the consumer. Producer/production subsidies ensure producers are better off by either supplying market price support, direct support, or payments to factors of production.[3] Consumer/consumption subsidies commonly reduce the price of goods and services to the consumer. For example, in the US at one time it was cheaper to buy gasoline than bottled water.[1]"
delayed fatherhood,"Birth data from developed countries indicates that the average paternal age is increasing. As the trend to older fatherhood has become established, concerns have been raised that this may be linked to adverse outcomes, such as pregnancy complications, congenital anomalies, and long-term health implications for the child. Since the sperm of older fathers may be impaired due to the general effects of ageing, their offspring may be at risk due to defects in sperm quality at conception. A literature search was performed to identify pregnancy complications, fetal anomalies and health issues for the child when the father is in an older age bracket. Evidence for impairment in the sperm and genetic material of older fathers was reviewed. With an older father, there is evidence of an increase in stillbirths and a slightly increased risk of autism, bipolar disorder and schizophrenia in the offspring later in life. The increased risk of achondroplasia has long been recognised. For the mother, there is an increased rate of Caesarean section. Investigations of other possible adverse outcomes have produced mixed findings. Further robust and longitudinal studies are needed to clarify these issues.
"
young motherhood,yound motherhood
israeli national boundaries,"The modern borders of Israel exist as the result both of past wars and of diplomatic agreements between the State of Israel and its neighbours as well as colonial powers. Only two of Israel's five total potential land borders are internationally recognized and uncontested, while the other three remain disputed; the majority of its border disputes are rooted in territorial changes that came about as a result of the 1967 Arab–Israeli War, which saw Israel occupy large swathes of territory from its rivals.[1] Israel's two formally recognized and confirmed borders exist with Egypt and Jordan since the 1979 Egypt–Israel peace treaty and the 1994 Israel–Jordan peace treaty, while its borders with Syria (via the Israeli-occupied Golan Heights), Lebanon (via the Blue Line; see Shebaa Farms dispute) and the Palestinian territories (Israeli-occupied land largely recognized as part of the de jure State of Palestine) remain internationally recognized as contested.[2]

According to the Green Line agreed upon in the 1949 Armistice Agreements, Israel is bordered by Lebanon to the north, the Golan Heights under Syrian sovereignty as well as the rest of Syria to the northeast, the Palestinian West Bank and Jordan to the east, and by the Palestinian Gaza Strip and Egypt to the southwest. The Israeli border with Egypt is the international border demarcated in 1906 between the United Kingdom and the Ottoman Empire, and confirmed in the 1979 Egypt–Israel peace treaty; the Israeli border with Jordan is based on the border defined in the 1922 Trans-Jordan memorandum, and confirmed in the 1994 Israel–Jordan peace treaty."
rise in nationalisim,"The rise of nationalism in Europe initiated with the Spring of Nations in 1848.[citation needed] American political science professor Leon Baradat has argued that “nationalism calls on people to identify with the interests of their national group and to support the creation of a state – a nation-state – to support those interests.” Nationalism was the ideological impetus that, in a few decades, transformed Europe. Rule by monarchies and foreign control of territory was replaced by self-determination and newly formed national governments.[1] Some countries, such as Germany and Italy were formed by uniting various regional states with a common ""national identity"". Others, such as Greece, Serbia, Poland and Bulgaria, were formed by uprisings against the Ottoman Empire and the Russian Empire.[2] Romania is a special case, formed by the unification of the principalities of Moldavia and Wallachia on 1859 and later gaining independence from the Ottoman Empire in 1878."
waiters and bartendeers get tips,waiters and bartendeers get tips
american customs,american customs
npr,"National Public Radio (NPR) is an American privately and publicly funded non-profit media organization headquartered in Washington, D.C. (often called the ""mothership"" of NPR), with its NPR West headquarters in Culver City, California.[2] It differs from other non-profit membership media organizations such as Associated Press, in that it was established by an act of Congress,[3] and most of its member stations are owned by government entities (often public universities). It serves as a national syndicator to a network of over 1,000 public radio stations in the United States.[4]

NPR produces and distributes news and cultural programming. The organization's flagship shows are two drive-time news broadcasts: Morning Edition and the afternoon All Things Considered, both carried by most NPR member stations, and among the most popular radio programs in the country.[5][6] As of March 2018, the drive-time programs attract an audience of 14.9 million and 14.7 million per week, respectively.[7]

NPR manages the Public Radio Satellite System, which distributes its programs and other programming from independent producers and networks such as American Public Media and Public Radio International. Its content is also available on-demand online, on mobile networks, and in many cases, as podcasts.[8] Several NPR stations also carry programs from British public broadcaster BBC World Service."
npr interview,"National Public Radio (NPR) is an American privately and publicly funded non-profit media organization headquartered in Washington, D.C. (often called the ""mothership"" of NPR), with its NPR West headquarters in Culver City, California.[2] It differs from other non-profit membership media organizations such as Associated Press, in that it was established by an act of Congress,[3] and most of its member stations are owned by government entities (often public universities). It serves as a national syndicator to a network of over 1,000 public radio stations in the United States.[4]

NPR produces and distributes news and cultural programming. The organization's flagship shows are two drive-time news broadcasts: Morning Edition and the afternoon All Things Considered, both carried by most NPR member stations, and among the most popular radio programs in the country.[5][6] As of March 2018, the drive-time programs attract an audience of 14.9 million and 14.7 million per week, respectively.[7]

NPR manages the Public Radio Satellite System, which distributes its programs and other programming from independent producers and networks such as American Public Media and Public Radio International. Its content is also available on-demand online, on mobile networks, and in many cases, as podcasts.[8] Several NPR stations also carry programs from British public broadcaster BBC World Service."
musical instruments,musical instruments
responsibile marijuana use,"Marijuana—which can also be called cannabis, weed, pot, or dope—refers to the dried flowers, leaves, stems, and seeds of the cannabis plant. The cannabis plant contains more than 100 compounds (or cannabinoids). These compounds include tetrahydrocannabinol (THC), which is impairing or mind-altering, as well as other active compounds, such as cannabidiol (CBD). CBD is not impairing, meaning it does not cause a “high”.1

Marijuana is the most commonly used federally illegal drug in the United States, with an estimated 48.2 million people using it in 2019.2 Marijuana use may have a wide range of health effects on the body and brain. Click on the sections below to learn more about how marijuana use can affect your health."
homeopathy,"Homeopathy or homoeopathy is a pseudoscientific[1][2][3][4] system of alternative medicine. It was conceived in 1796 by the German physician Samuel Hahnemann. Its practitioners, called homeopaths, believe that a substance that causes symptoms of a disease in healthy people can cure similar symptoms in sick people; this doctrine is called similia similibus curentur, or ""like cures like"".[5] Homeopathic preparations are termed remedies and are made using homeopathic dilution. In this process, the selected substance is repeatedly diluted until the final product is chemically indistinguishable from the diluent. Often not even a single molecule of the original substance can be expected to remain in the product.[6] Between each dilution homeopaths may hit and/or shake the product, claiming this makes the diluent remember the original substance after its removal. Practitioners claim that such preparations, upon oral intake, can treat or cure disease.[7]

All relevant scientific knowledge about physics, chemistry, biochemistry and biology[8][9][10][11][12][13] gained since at least the mid-19th century[14] contradicts homeopathy. Homeopathic remedies are biochemically inert, and have no effect on any known disease.[8][15][16] Hahnemann's theory of disease, centered around principles he termed miasms, is inconsistent with subsequent identification of viruses and bacteria as causes of disease. Clinical trials have been conducted and generally demonstrated no objective effect from homeopathic preparations.[17][18][19]: 206 [20] The fundamental implausibility of homeopathy as well as a lack of demonstrable effectiveness has led to it being characterized within the scientific and medical communities as quackery and fraud.[3][21][22]

Homeopathy achieved its greatest popularity in the 19th century. It was introduced to the United States in 1825 with the first homeopathic school opening in 1835. Throughout the 19th century, dozens of homeopathic institutions appeared in Europe and the United States. During this period, homeopathy was able to appear relatively successful, as other forms of treatment could be harmful and ineffective. By the end of the century the practice began to wane, with the last exclusively homeopathic medical school in the US closing in 1920.

In the 1970s, homeopathy made a significant comeback, with sales of some homeopathic products increasing tenfold. The trend corresponded with the rise of the New Age movement, and may be in part due to an irrational preference for ""natural"" products, and the longer consultation times homeopathic practitioners provided.

In the 21st century a series of meta-analyses have shown that the therapeutic claims of homeopathy lack scientific justification. As a result, national and international bodies have recommended the withdrawal of government funding for homeopathy in healthcare. National bodies from Australia, the United Kingdom, Switzerland and France, as well as the European Academies' Science Advisory Council and the Russian Academy of Sciences have all concluded that homeopathy is ineffective, and recommended against the practice receiving any further funding.[23][24][25][26] The National Health Service in England no longer provides funding for homeopathic remedies and asked the Department of Health to add homeopathic remedies to the list of forbidden prescription items.[27][28][29] France removed funding in 2021,[30][31] while Spain has also announced moves to ban homeopathy and other pseudotherapies from health centers."
athletic competitions of college students,athletic competitions of college students
spanish,spanish
working in a warehouse without ac,working in a warehouse without ac
drug ban,drug ban
export-import bank,"The Export–Import Bank of the United States (abbreviated as EXIM or known as the Bank) is the official export credit agency (ECA) of the United States federal government.[1][2] Operating as a wholly owned federal government corporation,[1] the Bank ""assists in financing and facilitating U.S. exports of goods and services"".[1] EXIM intervenes when private sector lenders are unable or unwilling to provide financing, equipping American businesses with the financing tools necessary to compete for global sales. EXIM's aim is to promote U.S. goods and services at no cost to U.S. taxpayers, protecting “made in America” products against foreign competition in overseas markets and encouraging the creation of American jobs. Its most recent chairman and president, Kimberly A. Reed, was nominated by President Donald J. Trump on January 16, 2019,[3] and sworn in on May 9, 2019.[4] Her term expired on January 20, 2021, and her replacement has yet to be appointed.

Founded in 1934, the Export–Import Bank was established by an executive order organized by President Franklin D. Roosevelt under the name Export–Import Bank of Washington. The stated goal was ""to aid in financing and to facilitate exports and imports and the exchange of commodities between the United States and other Nations or the agencies or nationals thereof."" The Bank's first transaction was a $3.8 million loan to Cuba in 1935 for the purchase of U.S. silver ingots. In 1945, it was made an independent agency in the Executive Branch by Congress. It was last chartered for a three-year term in 2012 and in September 2014 was extended through June 30, 2015.[5][6] Congressional authorization for the bank lapsed as of July 1, 2015. As a result, the bank could not engage in new business, but it continued to manage its existing loan portfolio.[7] Five months later, after the successful employment of the rarely used discharge petition procedure in the House of Representatives, the U.S. Congress reauthorized the bank until September 2019 via the Fixing America's Surface Transportation Act signed into law on December 4, 2015, by President Barack Obama.[8] In December of 2019, President Donald Trump signed the Export-Import Bank Extension into law as part of the Further Consolidated Appropriations Act, 2020 (P.L. 116-94) which authorized the bank until December 31st, 2026.

Over the years, the Export–Import Bank helped finance several historic projects including the Pan-American Highway, the Burma Road, and post-WWII reconstruction."
large farms,large farms
len penzo's salary,Electrical engineer and personal finance blogger who is the founder and editor-in-chief of Len Penzo dot Com. Don't worry: no blue check here.
i agree with the commenter above -- let's have len penzo's salary be determined by our tips,Electrical engineer and personal finance blogger who is the founder and editor-in-chief of Len Penzo dot Com. Don't worry: no blue check here.
fear,fear
anthropocentricsm,"Anthropocentrism (/ˌænθroʊpoʊˈsɛntrɪzəm/;[1] from Ancient Greek: ἄνθρωπος, ánthrōpos, ""human being""; and Ancient Greek: κέντρον, kéntron, ""center"") is the belief that human beings are the central or most important entity in the universe.[2] The term can be used interchangeably with humanocentrism, and some refer to the concept as human supremacy or human exceptionalism. From an anthropocentric perspective, humankind is seen as separate from nature and superior to it, and other entities (animals, plants, minerals, etc) are viewed as resources for humans to use.[2]

Anthropocentrism interprets or regards the world in terms of human values and experiences.[3] It is considered to be profoundly embedded in many modern human cultures and conscious acts. It is a major concept in the field of environmental ethics and environmental philosophy, where it is often considered to be the root cause of problems created by human action within the ecosphere.[4] However, many proponents of anthropocentrism state that this is not necessarily the case: they argue that a sound long-term view acknowledges that the global environment must be made continually suitable for humans and that the real issue is shallow anthropocentrism.[5][6]"
palestinian soveriegnty,"Palestine (Arabic: فلسطين‎, romanized: Filasṭīn), officially recognized as the State of Palestine[i] (Arabic: دولة فلسطين‎, romanized: Dawlat Filasṭīn) by the United Nations and other entities, is a de jure sovereign state[20][21] in Western Asia officially governed by the Palestine Liberation Organization (PLO) and claiming the West Bank and Gaza Strip[2] with Jerusalem as the designated capital; in practice, however, only partial administrative control is held over the 167 ""islands"" in the West Bank, and Gaza is ruled by a rival government (Hamas).[iii] The entirety of territory claimed by the State of Palestine has been occupied since 1948, first by Egypt (Gaza Strip) and Jordan (West Bank) and then by Israel after the Six-Day War in 1967.[7][22] Palestine has a population of 5,051,953 as of February 2020, ranked 121st in the world.[23]

After World War II, in 1947, the UN adopted a Partition Plan for Mandatory Palestine recommending the creation of independent Arab and Jewish states and an internationalized Jerusalem.[24] This partition plan was accepted by the Jews but rejected by the Arabs. The day after the establishment of a Jewish state in Eretz Israel, to be known as the State of Israel on 14 May 1948,[25][26][27] neighboring Arab armies invaded the former British mandate and fought the Israeli forces.[28][29] Later, the All-Palestine Government was established by the Arab League on 22 September 1948 to govern the Egyptian-controlled enclave in Gaza. It was soon recognized by all Arab League members except Transjordan. Though jurisdiction of the Government was declared to cover the whole of the former Mandatory Palestine, its effective jurisdiction was limited to the Gaza Strip.[30] Israel later captured the Gaza Strip and the Sinai Peninsula from Egypt, the West Bank (including East Jerusalem) from Jordan, and the Golan Heights from Syria in June 1967 during the Six-Day War.

On 15 November 1988 in Algiers, Yasser Arafat, Chairman of the PLO, proclaimed the establishment of the State of Palestine. A year after the signing of the Oslo Accords in 1993, the Palestinian National Authority was formed to govern (in varying degrees) areas A and B in the West Bank, comprising 165 ""islands"", and the Gaza Strip. After Hamas became the PNA parliament's leading party in the most recent elections (2006), a conflict broke out between it and the Fatah party, leading to Gaza being taken over by Hamas in 2007 (two years after the Israeli disengagement).

The State of Palestine has been recognized by 138 of the 193 UN members and since 2012 has had a status of a non-member observer state in the United Nations.[31][32][33] Palestine is a member of the Arab League, the Organisation of Islamic Cooperation, the G77, the International Olympic Committee, and other international bodies."
rising ecological problems,"Environmental issues are harmful effects of human activity on the biophysical environment.[citation needed][dubious – discuss] Environmental protection is a practice of protecting the natural environment on the individual, organizational or governmental levels, for the benefit of both the environment and humans. Environmentalism, a social and environmental movement, addresses environmental issues through advocacy, education and activism.[1]

The carbon dioxide equivalent of greenhouse gases (GHG) in the atmosphere has already exceeded 400 parts per million (NOAA) (with total ""long-term"" GHG exceeding 455 parts per million) (Intergovernmental Panel on Climate Report). The amount of greenhouse gas in the atmosphere is possibly above the threshold that can potentially cause climate change. The UN Office for the Coordination of Humanitarian Affairs (OCHA) has stated ""Climate change is not just a distant future threat. It is the main driver behind rising humanitarian needs and we are seeing its impact. The number of people affected and the damages inflicted by extreme weather has been unprecedented.""[2] Further, OCHA has stated:

Climate disasters are on the rise. Around 70 percent of disasters are now climate-related – up from around 50 percent from two decades ago.
These disasters take a heavier human toll and come with a higher price tag. In the last decade, 2.4 billion people were affected by climate-related disasters, compared to 1.7 billion in the previous decade. The cost of responding to disasters has risen tenfold between 1992 and 2008.
Destructive sudden heavy rains, intense tropical storms, repeated flooding, and droughts are likely to increase, as will the vulnerability of local communities in the absence of strong concerted action.[3]
Environment destruction caused by humans is a global, ongoing problem. By the year 2050, the global human population is expected to grow by 2 billion people, thereby reaching a level of 9.6 billion people.[4] The human effects on Earth can be seen in many different ways. A main effect, is an increase in global temperature. According to the report ”Our Changing Climate”, the global warming that has been going on for the past 50 years is primarily due to human activities.[5] Since 1895, the average temperature in the United States has increased by between 0.7 °C and 1.1 °C (1.3 °F and 1.9 °F), with most of the increase taking place since around 1970.[5][6][7]"
condso,condso
myhappiness,my happiness
new lawyers,new lawyers
superstitious people,superstitious people
lentil,"The lentil (Lens culinaris or Lens esculenta) is an edible legume. It is an annual plant known for its lens-shaped seeds. It is about 40 cm (16 in) tall, and the seeds grow in pods, usually with two seeds in each. As a food crop, the majority of world production comes from Canada and India, producing 58% combined of the world total.

In cuisines of the Indian subcontinent, where lentils are a staple, split lentils (often with their hulls removed) known as daal are often cooked into a thick curry/gravy that is usually eaten with rice or rotis."
veganism and non veganism and the need to have moderation,"Veganism is the practice of abstaining from the use of animal products, particularly in diet, and an associated philosophy that rejects the commodity status of animals.[c] An individual who follows the diet or philosophy is known as a vegan. Distinctions may be made between several categories of veganism. Dietary vegans, also known as ""strict vegetarians"", refrain from consuming meat, eggs, dairy products, and any other animal-derived substances.[d] An ethical vegan is someone who not only follows a plant-based diet but extends the philosophy into other areas of their lives, opposes the use of animals for any purpose,[e] and tries to avoid any cruelty and exploitation of all animals including humans.[23] Another term is ""environmental veganism"", which refers to the avoidance of animal products on the premise that the industrial farming of animals is environmentally damaging and unsustainable.[24]

Well-planned vegan diets are regarded as appropriate for all stages of life, including infancy and pregnancy, by the American Academy of Nutrition and Dietetics,[f] the Australian National Health and Medical Research Council,[26] the British Dietetic Association,[27] Dietitians of Canada,[28] and the New Zealand Ministry of Health.[29] The German Society for Nutrition—which is a non-profit organisation and not an official health agency—does not recommend vegan diets for children or adolescents, or during pregnancy and breastfeeding.[g] There is inconsistent evidence for vegan diets providing a protective effect against metabolic syndrome, but some evidence suggests that a vegan diet can help with weight loss, especially in the short term.[31][32] Vegan diets tend to be higher in dietary fiber, magnesium, folic acid, vitamin C, vitamin E, iron, and phytochemicals, and lower in dietary energy, saturated fat, cholesterol, omega-3 fatty acid, vitamin D, calcium, zinc, and vitamin B12.[h] A poorly-planned vegan diet may lead to nutritional deficiencies that nullify any beneficial effects and may cause serious health issues,[33][34][35] some of which can only be prevented with fortified foods or dietary supplements.[33][36] Vitamin B12 supplementation is important because its deficiency can cause blood disorders and potentially irreversible neurological damage; this danger is also one of the most common in poorly-planned non-vegan diets.[35][37][38]

The word 'vegan' was coined by Donald Watson and his then-future wife Dorothy Morgan in 1944.[39][40] It was derived from 'Allvega' and 'Allvegan' which had been used and suggested beforehand by original members and future officers of the society George A. Henderson and his wife Fay,[41] the latter of whom wrote the first vegan recipe book.[3][4][39] At first, they used it to mean ""non-dairy vegetarian"",[42][43] however, by May 1945, vegans explicitly abstained from ""eggs, honey; and animals' milk, butter and cheese"". From 1951, the Society defined it as ""the doctrine that man should live without exploiting animals"".[44] Interest in veganism increased significantly in the 2010s,[45][46] especially in the latter half, with more vegan stores opening and more vegan options becoming increasingly available in supermarkets and restaurants worldwide.[46]"
health clinics,health clinics
isalm,"Islam (/ˈɪslɑːm/;[a] Arabic: اَلْإِسْلَامُ‎, romanized: al-’Islām, [ɪsˈlaːm] (About this soundlisten) ""submission [to God]"")[1] is an Abrahamic monotheistic religion teaching that Muhammad is a messenger of God.[2][3] It is the world's second-largest religion with 1.9 billion followers, or 24.9% of the world's population,[4][5] known as Muslims.[6] Muslims make up a majority of the population in 47 countries.[7][8] Islam teaches that God is merciful, all-powerful, and unique,[9] and has guided humanity through prophets, revealed scriptures, and natural signs.[3][10] The primary scriptures of Islam are the Quran, believed to be the verbatim word of God, as well as the teachings and normative examples (called the sunnah, composed of accounts called hadith) of Muhammad (c. 570 – 632 CE).[11]

Muslims believe that Islam is the complete and universal version of a primordial faith that was revealed many times before through prophets such as Adam, Abraham, Moses, and Jesus.[12] Muslims consider the Quran, in Arabic, to be the unaltered and final revelation of God.[13] Like other Abrahamic religions, Islam also teaches a final judgment with the righteous rewarded in paradise and the unrighteous punished in hell.[14] Religious concepts and practices include the Five Pillars of Islam, which are obligatory acts of worship, as well as following Islamic law (sharia), which touches on virtually every aspect of life and society, from banking and welfare to women and the environment.[15][16] The cities of Mecca, Medina and Jerusalem are home to the three holiest sites in Islam.[17]

From a historical point of view, Islam originated in early 7th century CE in the Arabian Peninsula, in Mecca,[18] and by the 8th century, the Umayyad Caliphate extended from Iberia in the west to the Indus River in the east. The Islamic Golden Age refers to the period traditionally dated from the 8th century to the 13th century, during the Abbasid Caliphate, when much of the historically Muslim world was experiencing a scientific, economic, and cultural flourishing.[19][20][21] The expansion of the Muslim world involved various states and caliphates such as the Ottoman Empire, trade, and conversion to Islam by missionary activities (dawah).[22]

Most Muslims are of one of two denominations: Sunni (85–90%)[23] or Shia (10–15%).[24][25][26] Sunni and Shia differences arose from disagreement over the succession to Muhammad and acquired broader political significance, as well as theological and juridical dimensions.[27] About 12% of Muslims live in Indonesia, the most populous Muslim-majority country;[28] 31% live in South Asia,[29] the largest percentage of Muslims in the world;[30] 20% in the Middle East–North Africa, where it is the dominant religion;[31] and 15% in sub-Saharan Africa.[31] Sizable Muslim communities can also be found in the Americas, China, and Europe.[32][33] Islam is the fastest-growing major religion in the world.[34][35]"
corporate banks and tbeir ties to politica and economy,corporate banks and their ties to politica and economy
time warner,"Warner Media, LLC, doing business as WarnerMedia, is an American multinational mass media and entertainment conglomerate corporation owned by AT&T and headquartered at the 30 Hudson Yards complex in New York City, United States. It was originally formed in 1990 by Steve Ross from the merger of Time Inc. and the original Warner Communications, and was formerly known as Time Warner from 1990 to 2001 and again from 2003 to 2018. The company has film, television and cable operations, with its assets including WarnerMedia Studios & Networks (consisting of the entertainment assets of Turner Broadcasting, HBO, and Cinemax as well as Warner Bros., which itself consists of the film, animation, television studios, the company's home entertainment division and Studio Distribution Services, its joint venture with Universal Pictures Home Entertainment, DC Comics, New Line Cinema, and, together with ViacomCBS, a 50% interest in The CW television network); WarnerMedia News & Sports (consisting of the news and sports assets of Turner Broadcasting, including CNN, Turner Sports, and AT&T SportsNet); WarnerMedia Sales & Distribution (consisting of digital media company Otter Media); and WarnerMedia Direct (consisting of the HBO Max streaming service).

Despite spinning off Time Inc. in 2014, the company retained the Time Warner name until AT&T's acquisition in 2018, after which it became WarnerMedia.[5] On October 22, 2016, AT&T announced an offer to acquire Time Warner for $85 billion (including assumed Time Warner debt).[6][7] The proposed merger was confirmed on June 12, 2018,[8] after AT&T won an antitrust lawsuit that the U.S. Justice Department filed in 2017 to attempt to block the acquisition.[9] The merger closed two days later, with the company becoming a subsidiary of AT&T.[10] Under AT&T, the company moved to launch a streaming service built around the company's content, known as HBO Max.

In May 2021, nearly three years after the acquisition, AT&T announced that it had proposed to spin-off WarnerMedia and merge it with Discovery, Inc. to form a new publicly-traded company, Warner Bros. Discovery, under its CEO David Zaslav.

The company's previous assets included Time Inc., TW Telecom, AOL, Time Warner Cable, AOL Time Warner Book Group, and Warner Music Group; these operations were either sold to others or spun off as independent companies. The company was ranked No. 98 in the 2018 Fortune 500 list of the largest United States corporations by total revenue.[11]"
womrn in bikini and high heels,womrn in bikini and high heels
modern airplanes,modern airplanes
uncontrolled collaboration,uncontrolled collaboration
employmen,employment
isis and the middle east,"Islamic State (official name since June 2014;[98] abbreviated IS), at times known as the Islamic State of Iraq and the Levant (ISIL; /ˈaɪsəl, ˈaɪsɪl/), and as the Islamic State of Iraq and Syria (ISIS; /ˈaɪsɪs/),[99] or by its Arabic acronym, Daesh (داعش, Dāʿish, IPA: [ˈdaːʕɪʃ]),[100] is a militant Sunni Islamist group and former unrecognized quasi-state[101] that follows a Salafi jihadist doctrine.[102] Islamic State was founded by Abu Musab al-Zarqawi and gained global prominence in 2014 when it drove Iraqi security forces out of key cities in its Western Iraq offensive,[103] followed by its capture of Mosul[104] and the Sinjar massacre.[105]

IS originated in 1999, pledged allegiance to Al-Qaeda and participated in the Iraqi insurgency following the 2003 invasion of Iraq by Western forces. In June 2014, the group proclaimed itself a worldwide caliphate[106][107] and began referring to itself as the Islamic State (الدولة الإسلامية ad-Dawlah al-Islāmiyah; IS).[1] As a caliphate, it claimed religious, political, and military authority over all Muslims worldwide.[108] Its adoption of the name Islamic State and its idea of a caliphate have been criticised, with the United Nations, various governments, and mainstream Muslim groups rejecting its statehood.[109] In Syria, the group conducted ground attacks on both government forces and opposition factions, and by December 2015, it held an area extending from western Iraq to eastern Syria, containing an estimated eight to twelve million people,[54][55][110] where it enforced its interpretation of sharia law. They were estimated at the time to have an annual budget of more than US$1 billion and more than 30,000 fighters.[111]

In mid-2014, an international coalition led by the United States intervened against ISIL in Syria and Iraq with an airstrike campaign, in addition to supplying advisors, weapons, training, and supplies to ISIL's enemies in the Iraqi Security Forces and Syrian Democratic Forces. This campaign reinvigorated the latter two forces and damaged ISIL, killing tens of thousands of its troops[112] and reducing its financial and military infrastructure.[113] This was followed by a smaller-scale Russian intervention exclusively in Syria, in which ISIL lost thousands more fighters to airstrikes, cruise missile attacks, and other Russian military activities and had its financial base further degraded.[114] In July 2017, the group lost control of its largest city, Mosul, to the Iraqi army, followed by the loss of its de facto political capital of Raqqa to the Syrian Democratic Forces.[115] By December 2017, the Islamic State controlled just 2% of its maximum territory (in May 2015).[116] In December 2017, Iraqi forces had driven the last remnants of the Islamic State underground, three years after the group captured about a third of Iraq's territory.[117] By March 2019, ISIL lost one of their last significant territories in the Middle East in the Deir ez-Zor campaign, surrendering their ""tent city"" and pockets in Al-Baghuz Fawqani to the Syrian Democratic Forces after the Battle of Baghuz Fawqani.[30]

The group has been designated as a terrorist organisation by the United Nations. The IS is known for its videos of beheadings and other types of executions[118] of both soldiers and civilians, including journalists and aid workers, and its destruction of cultural heritage sites.[119] The United Nations holds ISIL responsible for committing human rights abuses, genocide, war crimes, and crimes against humanity.[120] The Islamic State committed genocide and ethnic cleansing on a historic scale in northern Iraq.[121][122] In October 2019, IS media announced that Abu Ibrahim al-Hashimi al-Qurashi was the new leader of the Islamic State,[123] after Abu Bakr al-Baghdadi killed himself by detonating a suicide vest during the US Barisha raid in Syria.[124][125][126] The Islamic State has also had a presence outside the Middle East through its affiliates and various ""provinces"",[127] and has been militarily active in countries including notably in Nigeria, Afghanistan and the Philippines.[128]"
hiring obese employees,hiring obese employees
uc admittance policy,"The University of California (UC) is a public land-grant research university system in the U.S. state of California. The system is composed of the campuses at Berkeley, Davis, Irvine, Los Angeles, Merced, Riverside, San Diego, San Francisco, Santa Barbara, and Santa Cruz, along with numerous research centers and academic abroad centers.[5] The system is the state's land-grant university.[6]

The University of California was founded on March 23, 1868, and operated in Oakland before moving to Berkeley in 1873.[7][8] Over time, several branch locations and satellite programs were established. In March 1951, the University of California began to reorganize itself into something distinct from its campus in Berkeley, with UC President Robert Gordon Sproul staying in place as chief executive of the UC system, while Clark Kerr became the first chancellor of UC Berkeley[9][10][11][12] and Raymond B. Allen became the first chancellor of UCLA.[13] However, the 1951 reorganization was stalled by resistance from Sproul and his allies,[14] and it was not until Kerr succeeded Sproul as UC President that UC was able to evolve into a university system from 1957 to 1960.[15] At that time, chancellors were appointed for additional campuses and each was granted some degree of greater autonomy.[16]

The University of California currently has 10 campuses, a combined student body of 285,862 students, 24,400 faculty members, 143,200 staff members and over 2.0 million living alumni.[2] Its newest campus in Merced opened in fall 2005. Nine campuses enroll both undergraduate and graduate students; one campus, UC San Francisco, enrolls only graduate and professional students in the medical and health sciences. In addition, the UC Hastings College of the Law, located in San Francisco, is legally affiliated with UC, but other than sharing its name is entirely autonomous from the rest of the system. Under the California Master Plan for Higher Education, the University of California is a part of the state's three-system public higher education plan, which also includes the California State University system and the California Community Colleges system. UC is governed by a Board of Regents whose autonomy from the rest of the state government is protected by the state constitution.[17] The University of California also manages or co-manages three national laboratories for the U.S. Department of Energy: Lawrence Berkeley National Laboratory (LBNL), Lawrence Livermore National Laboratory (LLNL), and Los Alamos National Laboratory (LANL).[18]

Collectively, the colleges, institutions, and alumni of the University of California make it the most comprehensive and advanced postsecondary educational system in the world, responsible for nearly $50 billion per year of economic impact.[failed verification][19] Major publications generally rank most UC campuses as being among the best universities in the world. Eight of the campuses, Berkeley, Davis, Irvine, Los Angeles, Santa Barbara, San Diego, Santa Cruz, and Riverside, are considered Public Ivies, making California the state with the most universities in the nation to hold the title.[20][21] UC campuses have large numbers of distinguished faculty in almost every academic discipline, with UC faculty and researchers having won 71 Nobel Prizes as of 2021.[22]"
jewish beliefs,jewish beliefs
rich criminals,rich criminals
new yorkers,A resident of New York City
race-based affirmative action,"Affirmative action in the United States is a set of laws, policies, guidelines, and administrative practices ""intended to end and correct the effects of a specific form of discrimination""[1] that include government-mandated, government-approved, and voluntary private programs. The programs tend to focus on access to education and employment, granting special consideration to historically excluded groups, specifically racial minorities or women.[1][2] The impetus toward affirmative action is redressing the disadvantages[3][4][5][6][7] associated with past and present discrimination.[8] Further impetus is a desire to ensure public institutions, such as universities, hospitals, and police forces, are more representative of the populations they serve.[9]

In the United States, affirmative action included the use of racial quotas until the Supreme Court ruled that quotas were unconstitutional.[10] Affirmative action currently tends to emphasize not specific quotas but rather ""targeted goals"" to address past discrimination in a particular institution or in broader society through ""good-faith efforts ... to identify, select, and train potentially qualified minorities and women.""[1][11] For example, many higher education institutions have voluntarily adopted policies which seek to increase recruitment of racial minorities.[12] Outreach campaigns, targeted recruitment, employee and management development, and employee support programs are examples of affirmative action in employment.[13] Nine states in the United States have banned affirmative action: California (1996), Washington (1998), Florida (1999), Michigan (2006), Nebraska (2008), Arizona (2010), New Hampshire (2012), Oklahoma (2012), and Idaho (2020). Florida's ban was via an executive order and New Hampshire and Idaho's bans were passed by the legislature. The other six bans were approved at the ballot.[14] The 1996 Hopwood v. Texas decision effectively barred affirmative action in the three states within the United States Court of Appeals for the Fifth Circuit—Louisiana, Mississippi, and Texas—until Grutter v. Bollinger abrogated it in 2003.[15]

Affirmative action policies were developed to address long histories of discrimination faced by minorities and women, which reports suggest produced corresponding unfair advantages for whites and males.[16][17] They first emerged from debates over non-discrimination policies in the 1940s and during the civil rights movement.[18] These debates led to federal executive orders requiring non-discrimination in the employment policies of some government agencies and contractors in the 1940s and onward, and to Title VII of the Civil Rights Act of 1964 which prohibited racial discrimination in firms with over 25 employees. The first federal policy of race-conscious affirmative action was the Revised Philadelphia Plan, implemented in 1969, which required certain government contractors to set ""goals and timetables"" for integrating and diversifying their workforce. Similar policies emerged through a mix of voluntary practices and federal and state policies in employment and education. Affirmative action as a practice was partially upheld by the Supreme Court in Grutter v. Bollinger (2003), while the use of racial quotas for college admissions was concurrently ruled unconstitutional by the Court in Gratz v. Bollinger (2003).

Affirmative action often gives rise to controversy in American politics. Supporters argue that affirmative action is still needed to counteract continuing bias and prejudice against women and minorities. Opponents argue that these policies constitute reverse racism and/or amount to discrimination against other minorities, such as Asian Americans, which entails favoring one group over another based upon racial preference rather than achievement, and many believe that the diversity of current American society suggests that affirmative action policies succeeded and are no longer required.[19] Supporters point to contemporary examples of conscious and unconscious biases, such as the finding that job-seekers with African American sounding names may be less likely to get a callback than those with white-sounding names, as proof that affirmative action is not obsolete.[11]"
primitive muslim beliefs,"Muslims (Arabic: مسلم‎, romanized: Muslim) are people who follow or practice Islam, an Abrahamic monotheistic religion. The word ""Muslim"" derives from Arabic and means ""submitter (to God)"".[30] Muslims consider the Quran, their holy book, to be the verbatim word of God as revealed to the Islamic prophet and messenger Muhammad.[31] The majority of Muslims also follow the teachings and practices of Muhammad (sunnah) as recorded in traditional accounts (hadith).[32]

The beliefs of Muslims include: that God (Arabic: الله‎ Allah) is eternal, transcendent and absolutely one (tawhid); that God is incomparable, self-sustaining and neither begets nor was begotten; that Islam is the complete and universal version of a primordial faith that has been revealed before through many prophets including Abraham, Ishmael, Isaac, Moses, and Jesus;[33] that these previous messages and revelations have been partially changed or corrupted over time (tahrif)[34] and that the Quran is the final unaltered revelation from God.[35]

As of 2015, 1.8 billion or about 24.1% of the world population are Muslims.[36] By the percentage of the total population in a region considering themselves Muslim, 91% in the Middle East–North Africa (MENA),[37] 81% in Central Asia,[38][39] 65% in the Caucasus,[40][41][42][43][44][45] 40% in Southeast Asia,[46][47] 31% in South Asia,[48][49] 30% in Sub-Saharan Africa,[50] 25% in Asia and Oceania collectively,[51] around 6% in Europe,[52] and 1% in the Americas.[53][54][55][56]

Most Muslims are of one of two denominations; Sunni (75–90%)[57] and Shia (12-17%).[18] About 12% of Muslims live in Indonesia, the largest Muslim-majority country;[58][59] 31% of Muslims live in South Asia,[60] the largest population of Muslims in the world;[61] 20% in the Middle East–North Africa,[62] where it is the dominant religion;[63] and 15% in Sub-Saharan Africa.[64] Muslims are the overwhelming majority in Central Asia,[65] the majority in the Caucasus[40][41] and widespread in Southeast Asia.[47] India is the country with the largest Muslim population outside Muslim-majority countries.[66] Sizeable Muslim communities are also found in the Americas, China, and Europe.[67][68][69] Islam is the fastest-growing major religion in the world.[70][71][72]"
dr. evins wants the pot business,dr. evins wants the pot business
nida,"The National Institute on Drug Abuse (NIDA) is a United States federal-government research institute whose mission is to ""advance science on the causes and consequences of drug use and addiction and to apply that knowledge to improve individual and public health.""

The institute has conducted an in-depth study of addiction according to its biological, behavioral and social components. It has also supported many treatments such as nicotine patches and gums, and performed research into AIDS and other drug-related diseases. Its monopoly on the supply of research-grade marijuana has proved controversial."
making yourself happy,making yourself happy
balanced diets,"A healthy diet is a diet that helps maintain or improve overall health. A healthy diet provides the body with essential nutrition: fluid, macronutrients, micronutrients, and adequate food energy.[2][3]

A healthy diet may contain fruits, vegetables, and whole grains, and may include little to no processed food or sweetened beverages. The requirements for a healthy diet can be met from a variety of plant-based and animal-based foods, although a non-plant source of vitamin B12 is needed for those following a vegan diet.[4] Various nutrition guides are published by medical and governmental institutions to educate individuals on what they should be eating to be healthy. Nutrition facts labels are also mandatory in some countries to allow consumers to choose between foods based on the components relevant to health.[5][6]"
miliatayr intervention,"The American-led intervention in the Syrian Civil War refers to the American-led support of Syrian rebels and the Syrian Democratic Forces (SDF) during the course of the Syrian civil war, including Operation Inherent Resolve, the active military operation led by the United States, and involving the militaries of the United Kingdom, France, Jordan, Turkey, Canada, Australia, and others against the Islamic State of Iraq and the Levant (ISIL) and al-Nusra Front since 2014. Beginning in 2017–18, the U.S. and its partners have also targeted the Syrian government and its allies via airstrikes and aircraft shoot-downs, mainly in defense of either the SDF or the Revolutionary Commando Army in al-Tanf.

Shortly after the civil war broke out in 2011, the U.S. initially supplied the rebels of the Free Syrian Army with non-lethal aid (e.g. food rations and pickup trucks), but quickly began providing training, money, and intelligence to selected Syrian rebel commanders. At least two U.S. programs attempted to assist the Syrian rebels, including a 2014 Pentagon program that planned to train and equip 15,000 rebels to fight ISIL, which was canceled in 2015 after spending $500 million and producing only a few dozen fighters.[142] A simultaneous $1 billion covert program called Timber Sycamore conducted by the Central Intelligence Agency (CIA) aimed at fighting Syrian President Bashar al-Assad was more successful, but was decimated by Russian bombing, and canceled in mid-2017 by the Trump administration.[142] The Obama administration began surveillance missions on Islamic State positions in Syria in September 2014.[143] On 22 September 2014, the U.S., Bahrain, Jordan, Qatar, Saudi Arabia, and the United Arab Emirates (UAE) began to attack ISIL forces inside Syria,[18][144] as well as the Khorasan group in the Idlib Governorate west of Aleppo, and the al-Nusra Front around Raqqa,[24][145] as part of the international military intervention against ISIL.

The U.S. missile strike on Shayrat Airbase on 7 April 2017 was the first time the U.S. deliberately attacked Syrian government forces,[146] and marked the start of a series of direct military actions by U.S. forces against the Syrian government and its allies that occurred during the periods of May–June 2017 and February 2018. In mid-January 2018, the Trump administration indicated its intention to maintain an open-ended military presence in Syria to counter Iran's influence and oust Syrian president Bashar al-Assad.[147] In early September 2018, the U.S. began implementing a new strategy that sought to indefinitely extend its military effort, launching a major diplomatic push to achieve American objectives in Syria.[148] However, on 19 December, President Trump unilaterally ordered the withdrawal of the 2,000–2,500 American ground troops in Syria, which was initially set to take place in a 90-day period, and to be completed in 2019.[149][150][151] With proliferating concerns over a potential power vacuum, the U.S. announced on 22 February 2019 that instead of a total withdrawal, a contingency force of around 400 American troops would remain garrisoned in Syria indefinitely, and that their withdrawal would be gradual and conditions-based, marking a return to a policy of open-ended American military presence in the country.[152][153]

In 2019, the coalition saw decisive results in its intervention against ISIL; the terror group lost its last remaining territory in Syria during the Battle of Baghuz Fawqani[154] and its leader Abu Bakr al-Baghdadi died during a U.S. special forces raid in Barisha, Idlib in October 2019.[155] The Trump administration ordered all U.S. forces to withdraw from Rojava in early October ahead of a Turkish incursion into the region, a controversial move widely seen as a reneging of the U.S.'s alliance with the SDF in favor of NATO ally Turkey.[156] The decision was however partially reversed by November 2019 as U.S. troops instead repositioned to eastern Syria, reinforcing their presence in the al-Hasakah and Deir ez-Zor governorates, with the subordinate mission of securing SDF-controlled oil and gas infrastructure from the ISIL insurgency and the Syrian government.[157]

On 23 November 2019, the head of U.S. Central Command stated there was no ""end date"" on the U.S.'s intervention in Syria.[158]"
nonmbers,nonmember
alcohol in certain establishments,alcohol in certain establishments
attact on cuba,"attack on cuba. The Bay of Pigs Invasion (Spanish: invasión de bahía de Cochinos; sometimes called invasión de playa Girón or batalla de Girón, after the Playa Girón) was a failed landing operation on the southwestern coast of Cuba in 1961 by Cuban exiles who opposed Fidel Castro's Cuban Revolution. Covertly financed and directed by the U.S. government, the operation took place at the height of the Cold War, and its failure led to major shifts in international relations between Cuba, the United States, and the Soviet Union.

In 1952, American ally General Fulgencio Batista led a coup against President Carlos Prio and forced Prio into exile in Miami, Florida. Prio's exile inspired the creation of the 26th of July Movement against Batista by Castro. The movement successfully completed the Cuban Revolution in December 1958. Castro nationalized American businesses—including banks, oil refineries, and sugar and coffee plantations—then severed Cuba's formerly close relations with the United States and reached out to its Cold War rival, the Soviet Union. In response, U.S. President Dwight D. Eisenhower allocated $13.1 million to the Central Intelligence Agency (CIA) in March 1960, for use against Castro. With the aid of Cuban counter-revolutionaries, the CIA proceeded to organize an invasion operation.

After Castro's victory, Cuban exiles who had traveled to the U.S. had formed the counter-revolutionary military unit Brigade 2506. The brigade fronted the armed wing of the Democratic Revolutionary Front (DRF), and its purpose was to overthrow Castro's government. The CIA funded the brigade, which also included some U.S. military[7] personnel, and trained the unit in Guatemala.

Over 1,400 paramilitaries, divided into five infantry battalions and one paratrooper battalion, assembled and launched from Guatemala and Nicaragua by boat on 17 April 1961. Two days earlier, eight CIA-supplied B-26 bombers had attacked Cuban airfields and then returned to the U.S. On the night of 17 April, the main invasion force landed on the beach at Playa Girón in the Bay of Pigs, where it overwhelmed a local revolutionary militia. Initially, José Ramón Fernández led the Cuban Army counter-offensive; later, Castro took personal control. As the invaders lost the strategic initiative, the international community found out about the invasion, and U.S. President John F. Kennedy decided to withhold further air support.[8] The plan devised during Eisenhower's presidency had required involvement of both air and naval forces. Without air support, the invasion was being conducted with fewer forces than the CIA had deemed necessary. The invading force was defeated within three days by the Cuban Revolutionary Armed Forces (Spanish: Fuerzas Armadas Revolucionarias – FAR) and the invaders surrendered on 20 April. Most of the invading counter-revolutionary troops were publicly interrogated and put into Cuban prisons.

The invasion was a U.S. foreign policy failure. The invasion's defeat solidified Castro's role as a national hero and widened the political division between the two formerly-allied countries. It also pushed Cuba closer to the Soviet Union, setting the stage for the Cuban Missile Crisis in 1962."
economic conditions of black populations,"Over the past half-century, Black Americans have made substantial social and economic progress, gaining political rights that long had been denied to them, entering professions from which they had been blocked and largely overcoming centuries of overt racism and oppression.
While there were only five Black Members of Congress when the Civil Rights Act became law in 1964, there currently are 56 Black Members of Congress, including 12% of the House of Representatives. Black activists, scholars, and social commentators have raised awareness about the importance of diversity and shaped the national conversation around race and inequality. There has been a proliferation of Black writers, screenwriters, artists, poets, athletes and musicians who have become superstars in their respective fields. And the 21st century saw the election of the first Black American, Barack Obama, as president of the United States.
Millions of Black Americans also have benefited from the opportunities created by the de jure end of Jim Crow, entering the middle class for the first time, earning undergraduate and advanced degrees, receiving higher wages, achieving professional success and raising children who will build on their achievements. Leading indicators of economic prosperity and other measures of well-being also have trended upward for most of this period, with increased life expectancy, increased household incomes and substantial gains in educational attainment.
However, these very visible signs of improvement mask deep inequities that relegate tens of millions of Black Americans to second-class status, with far fewer opportunities to achieve good health, political influence, prosperity and security than other Americans. The majority of Americans fail to recognize the magnitude of these problems. For example, a 2019 study found that over 97% of respondents vastly underestimated the huge gap between the median wealth held by Black families ($17,000) and White families ($171,000)—a ratio of 10 to one. Respondents estimated the gap to be 80 percentage points smaller than the actual divide.
The data reveal a much different story, with leading indicators of social and economic well-being showing that, on average, Black Americans face much more difficult circumstances than their White counterparts. For example, Black Americans take home less income, are far less likely to own their homes and live shorter lives than White Americans.
Evaluating the economic state of Black America requires acknowledging that while the United States has made some progress, very large disparities continue to exist. Recognizing both the progress and the challenges is essential to ensuring that all Americans, including Black Americans, have a realistic chance to achieve success."
wal-mart groceries,"Walmart Inc. ( /ˈwɔːlmɑːrt/; formerly Wal-Mart Stores, Inc.) is an American multinational retail corporation that operates a chain of hypermarkets (also called supercenters), discount department stores, and grocery stores from the United States, headquartered in Bentonville, Arkansas.[9] The company was founded by Sam Walton in nearby Rogers, Arkansas in 1962 and incorporated under Delaware General Corporation Law on October 31, 1969. It also owns and operates Sam's Club retail warehouses.[10][11] As of July 31, 2021, Walmart has 10,524 stores and clubs in 24 countries, operating under 48 different names.[2][3][12] The company operates under the name Walmart in the United States and Canada, as Walmart de México y Centroamérica in Mexico and Central America, and as Flipkart Wholesale in India. It has wholly owned operations in Chile, Canada, and South Africa. Since August 2018, Walmart holds only a minority stake in Walmart Brasil, which was renamed Grupo Big in August 2019, with 20 percent of the company's shares, and private equity firm Advent International holding 80 percent ownership of the company.

Walmart is the world's largest company by revenue, with US$548.743 billion, according to the Fortune Global 500 list in 2020. It is also the largest private employer in the world with 2.2 million employees. It is a publicly traded family-owned business, as the company is controlled by the Walton family. Sam Walton's heirs own over 50 percent of Walmart through both their holding company Walton Enterprises and their individual holdings.[13] Walmart was the largest United States grocery retailer in 2019, and 65 percent of Walmart's US$510.329 billion sales came from U.S. operations.[14][15]

Walmart was listed on the New York Stock Exchange in 1972. By 1988, it was the most profitable retailer in the U.S.,[16] and it had become the largest in terms of revenue by October 1989.[17] The company was originally geographically limited to the South and lower Midwest, but it had stores from coast to coast by the early 1990s. Sam's Club opened in New Jersey in November 1989, and the first California outlet opened in Lancaster, in July 1990. A Walmart in York, Pennsylvania, opened in October 1990, the first main store in the Northeast.[18]

Walmart's investments outside the U.S. have seen mixed results. Its operations and subsidiaries in Canada,[19] the United Kingdom,[20] Central America, South America, and China are successful, but its ventures failed in Germany, Japan, and South Korea.[21][22][23]"
auditing algebra,auditing algebra
wilt chamberlain,"Wilton Norman Chamberlain (/ˈtʃeɪmbərlɪn/; August 21, 1936 – October 12, 1999) was an American professional basketball player who played as a center, and is widely regarded as one of the greatest players in the sport's history.[2][3] He played for the Philadelphia/San Francisco Warriors, the Philadelphia 76ers, and the Los Angeles Lakers of the National Basketball Association (NBA). He played for the University of Kansas and for the Harlem Globetrotters before playing in the NBA. Chamberlain stood 7 ft 1 in (2.16 m) tall, and weighed 250 pounds (110 kg) as a rookie[4] before gaining up to 275 pounds (125 kg) and later to over 300 pounds (140 kg) with the Lakers.

Chamberlain holds numerous NBA regular season records in scoring, rebounding, and durability categories. He is the only player to score 100 points in a single NBA game or average more than 40 and 50 points in a season. He won seven scoring, eleven rebounding, nine durability, and nine field goal percentage titles, and led the league in assists once.[3] Chamberlain is the only player in NBA history to average at least 30 points and 20 rebounds per game in a season, which he accomplished seven times. He is also the only player to average at least 30 points and 20 rebounds per game over the entire course of his NBA career. Although Chamberlain suffered a long string of NBA Finals losses during his career,[5] he had a successful career, winning two NBA championships, earning four regular-season NBA Most Valuable Player (MVP) awards, the NBA Rookie of the Year award, one NBA Finals MVP award, and was selected to 13 NBA All-Star Game and ten All-NBA First and Second teams.[2][6] Chamberlain was subsequently enshrined in the Naismith Memorial Basketball Hall of Fame in 1978, elected into the NBA's 35th Anniversary Team of 1980, and was chosen as one of the 50 Greatest Players in NBA History in 1996.[6] In October 2021, as part of the NBA’s 75th Anniversary, Chamberlain was honored as one of the 75 greatest players of all time, by being named to the NBA’s 75th Anniversary All-Time Team.[7]

Chamberlain was known by several nicknames during his basketball playing career. He disliked the ones that called attention to his height, such as ""Goliath"" and ""Wilt the Stilt"". A Philadelphia sportswriter coined the nicknames during Chamberlain's high school days. He preferred ""The Big Dipper"", which was inspired by his friends who saw him dip his head as he walked through doorways.[8] After his professional basketball career ended, Chamberlain played volleyball in the short-lived International Volleyball Association, was president of that organization, and is enshrined in the IVA Hall of Fame for his contributions.[9] He was a successful businessman, authored several books, and appeared in the movie Conan the Destroyer. Chamberlain was also a lifelong bachelor and became notorious for his statement of having had sexual relations with as many as 20,000 women.[10]"
developing brains,developing brains
dual citizanship,"Multiple/dual citizenship (or multiple/dual nationality) is a legal status in which a person is concurrently regarded as a national or citizen of more than one country under the laws of those countries.[1] Conceptually, citizenship is focused on the internal political life of the country and nationality is a matter of international dealings.[2] There is no international convention which determines the nationality or citizenship status of a person. This is defined exclusively by national laws, which can vary and conflict with each other. Multiple citizenship arises because different countries use different, and not necessarily mutually exclusive, criteria for citizenship. Colloquially, people may ""hold"" multiple citizenship but, technically, each nation makes a claim that a particular person is considered its national.

A person holding multiple citizenship is, generally, entitled to the rights of citizenship in each country whose citizenship they are holding (such as right to a passport, right to enter the country, right to residence and work, right to vote, etc.), but may also be subject to obligations of citizenship (such as a potential obligation for national service, becoming subject to taxation on worldwide income, etc.).

Some countries do not permit dual citizenship or only do in certain cases (e.g. inheriting multiple nationalities at birth). This may be by requiring an applicant for naturalization to renounce all existing citizenship, or by withdrawing its citizenship from someone who voluntarily acquires another citizenship, or by other devices. Some countries permit a renunciation of citizenship, while others do not. Some countries permit a general dual citizenship while others permit dual citizenship but only of a limited number of countries.

A country that allows dual citizenship may still not recognize the other citizenship of its nationals within its own territory (for example, in relation to entry into the country, national service, duty to vote, etc.). Similarly, it may not permit consular access by another country for a person who is also its national. Some countries prohibit dual citizenship holders from serving in their armed forces or on police forces or holding certain public offices.[3]"
superstar architects,superstar architects
homogeneous communites,homogeneous communites
dronk strikes,"A drone strike is an air strike delivered by one or more unmanned combat aerial vehicles (UCAV) or weaponized commercial unmanned aerial vehicles (UAV). Only the United States, Israel, China, Iran, Italy, India, Pakistan, Russia, Turkey, and Poland[1][2] are known to have manufactured operational UCAVs as of 2019.[3]

Drone attacks can be conducted by commercial UCAVs dropping bombs, firing a missile, or crashing into a target.[4] Since the turn of the century, most drone strikes have been carried out by the US military in such countries as Afghanistan, Pakistan, Syria, Iraq, Somalia and Yemen using air-to-surface missiles,[5] but drone warfare has increasingly been deployed by Turkey and Azerbaijan.[6] Drones strikes are used for targeted killings by several countries.[7][8]

In 2020 a Turkish-made UAV loaded with explosives detected and attacked Haftar's forces in Libya with its artificial intelligence without command, according to a report from the UN Security Council’s Panel of Experts on Libya, published in March 2021. It was considered the first attack carried out by the UAVs on their own initiative.[9][10][11]"
weed legallzation,"In the United States, the use and possession of cannabis is illegal under federal law for any purpose pursuant to the Controlled Substances Act of 1970 (CSA). Under the CSA, cannabis is classified as a Schedule I substance, determined to have a high potential for abuse and no accepted medical use – thereby prohibiting even medical use of the drug.[1] However, at the state level policies regarding the medical and recreational use of cannabis vary greatly, and in many states conflict significantly with federal law.

The medical use of cannabis is legal with a doctor's recommendation in 36 states, four out of five permanently inhabited U.S. territories, and the District of Columbia.[2] Twelve other states have laws that limit THC content, for the purpose of allowing access to products that are rich in cannabidiol (CBD), a non-psychoactive component of cannabis.[2] Although cannabis remains a Schedule I drug, the Rohrabacher–Farr amendment prohibits federal prosecution of individuals complying with state medical cannabis laws.[3]

The recreational use of cannabis is legalized in 18 states,[a] the District of Columbia, the Northern Mariana Islands, and Guam. Another 13 states and the U.S. Virgin Islands have decriminalized its use.[4] Commercial distribution of cannabis has been legalized in all jurisdictions where possession has been legalized, except the District of Columbia. Prior to January 2018, the Cole Memorandum provided some protection against enforcement of federal law in states that have legalized cannabis, but the memorandum was rescinded by Attorney General Jeff Sessions.[5]

Although the use of cannabis remains federally illegal, some of its derivative compounds have been approved by the Food and Drug Administration for prescription use. Cannabinoid drugs which have received FDA approval are Marinol (THC), Syndros (THC), Cesamet (nabilone), and Epidiolex (cannabidiol). For non-prescription use, cannabidiol derived from industrial hemp is legal at the federal level, but legality and enforcement varies by state.[6][7]"
ms small,ms small
making public education more optimizable for students,"State schools (in England, Wales, and New Zealand) or public schools (Scottish English and North American English)[note 1] are generally primary or secondary schools that educate all children without charge. They are funded in whole or in part by taxation.[citation needed] State funded schools exist in virtually every country of the world, though there are significant variations in their structure and educational programmes. State education generally encompasses primary and secondary education (4 years old to 18 years old)."
highscool early college availability,"The Early College High School Initiative in the United States allows students to receive a high school diploma and an associate degree, or up to two years of college credit, by taking a mixture of high school and college classes. This differs from dual enrollment, where students are enrolled in a traditional high school and take college classes, whereas early college students take high school classes in preparation for full college workloads. At early colleges, students also have fewer high school classes because some of their college classes replace their high school classes. Early colleges differ from closely related middle colleges. ECHS students spend their school day at college, and go to their home school occasionally for events such as football games, homecoming, and prom.

The ECHS Initiative began in 2002 with funding from the Bill & Melinda Gates Foundation, among others. The first early college in the United States, Bard College at Simon's Rock, was founded in 1966.[1] Today, more than 230 early colleges across 28 states serve 50,000+ students.[2]"
powerful politicians,powerful politicians
advantages of a plant based diet,"Plant-based or plant-forward eating patterns focus on foods primarily from plants. This includes not only fruits and vegetables, but also nuts, seeds, oils, whole grains, legumes, and beans. It doesn’t mean that you are vegetarian or vegan and never eat meat or dairy. Rather, you are proportionately choosing more of your foods from plant sources.

Mediterranean and vegetarian diets

What is the evidence that plant-based eating patterns are healthy? Much nutrition research has examined plant-based eating patterns such as the Mediterranean diet and a vegetarian diet. The Mediterranean diet has a foundation of plant-based foods; it also includes fish, poultry, eggs, cheese, and yogurt a few times a week, with meats and sweets less often.

The Mediterranean diet has been shown in both large population studies and randomized clinical trials to reduce risk of heart disease, metabolic syndrome, diabetes, certain cancers (specifically colon, breast, and prostate cancer), depression, and in older adults, a decreased risk of frailty, along with better mental and physical function."
not consuming animal products,"An animal product is any material derived from the body of an animal.[1] Examples are fat, flesh, blood, milk, eggs, and lesser known products, such as isinglass and rennet.[2]

Animal by-products, as defined by the USDA, are products harvested or manufactured from livestock other than muscle meat.[3] In the EU, animal by-products (ABPs) are defined somewhat more broadly, as materials from animals that people do not consume.[4] Thus, chicken eggs for human consumption are considered by-products in the US but not France; whereas eggs destined for animal feed are classified as animal by-products in both countries. This does not in itself reflect on the condition, safety, or wholesomeness of the product.

Animal by-products are carcasses and parts of carcasses from slaughterhouses, animal shelters, zoos and veterinarians, and products of animal origin not intended for human consumption, including catering waste. These products may go through a process known as rendering to be made into human and non-human foodstuffs, fats, and other material that can be sold to make commercial products such as cosmetics, paint, cleaners, polishes, glue, soap and ink. The sale of animal by-products allows the meat industry to compete economically with industries selling sources of vegetable protein.[5]

The word animals includes all species in the biological kingdom animalia. For example, insects, shrimp, and oysters are animals.

Generally, products made from fossilized or decomposed animals, such as petroleum formed from the ancient remains of marine animals are not considered animal products. Crops grown in soil fertilized with animal remains are rarely characterized as animal products.

Several popular diet patterns prohibit the inclusion of some categories of animal products and may also limit the conditions of when other animal products may be permitted. This includes but not limited to secular diets; like, vegetarian, pescetarian, and paleolithic diets, as well as religious diets, such as kosher, halal, mahayana, macrobiotic, and sattvic diets. Other diets, such as vegan-vegetarian diets and all its subsets exclude any material of animal origin.[6] Scholarly, the term animal source foods (ASFs) has been used to refer to refer to these animal products and byproducts collectively.[7]

In international trade legislation, the terminology products of animal origin (POAO) is used for referring to foods and goods that are derived from animals or have close relation to them.[8]"
parent's deciding vaccination,parent's deciding vaccination
arm,arm
vfdd market,vfdd market
baby boomers,"Baby boomers (often shortened to boomers) are the demographic cohort following the Silent Generation and preceding Generation X. The generation is generally defined as people born from 1946 to 1964, during the post–World War II baby boom.[1] The term is also used outside the United States, but the dates, the demographic context, and the cultural identifiers may vary.[2][3][4][5] The baby boom has been described variously as a ""shockwave""[6] and as ""the pig in the python"".[7][8] Most baby boomers are children of either the Greatest Generation or the Silent Generation, and are often parents of late Gen Xers and Millennials. Late baby boomers can also be the parents of gen-zer’s.[9]

In the West, boomers' childhoods in the 1950s and 1960s had significant reforms in education, both as part of the ideological confrontation that was the Cold War,[10][11] and as a continuation of the interwar period.[12][13] In the 1960s and 1970s, as this relatively large number of young people entered their teens and young adulthood—the oldest turned 18 in 1964—they, and those around them, created a very specific rhetoric around their cohort,[14] and the social movements brought about by their size in numbers, such as the counterculture of the 1960s[15] and its backlash.[16]

In many countries, this period was one of deep political instability due to the postwar youth bulge.[16][17] In China, boomers lived through the Cultural Revolution and were subject to the one-child policy as adults.[18] These social changes and rhetoric had an important impact in the perceptions of the boomers, as well as society's increasingly common tendency to define the world in terms of generations, which was a relatively new phenomenon. That this group reached puberty and maximum height earlier than previous generations added to the tension between the generations.[19]

In Europe and North America, many boomers came of age in a time of increasing affluence and widespread government subsidies in postwar housing and education,[6] and grew up genuinely expecting the world to improve with time.[7] Those with higher standards of living and educational levels were often the most demanding of betterment.[16][20] In the early 21st century, baby boomers in developed countries, with a few exceptions, are the single biggest cohort in their societies due to subreplacement fertility and population aging.[21]"
generation,generation
rebuild gazap,"The Gaza Strip (/ˈɡɑːzə/;[3] Arabic: قِطَاعُ غَزَّةَ‎ Qiṭāʿu Ġazzah [qi.tˤaːʕ ɣaz.zah]), or simply Gaza, is a Palestinian enclave[4][5][6][7][8][9][10] on the eastern coast of the Mediterranean Sea. It borders Egypt on the southwest for 11 kilometers (6.8 mi) and Israel on the east and north along a 51 km (32 mi) border. Gaza and the West Bank are claimed by the de jure sovereign State of Palestine.

The territories of Gaza and the West Bank are separated from each other by Israeli territory. Both fell under the jurisdiction of the Palestinian Authority,[11] but the Strip has, since the Battle of Gaza in June 2007, been governed by Hamas, a militant, Palestinian, fundamentalist Islamic organization,[12] which came to power in the last-held elections in 2006. It has been placed under an Israeli and US-led international economic and political boycott from that time onwards.[13]

The territory is 41 kilometers (25 mi) long, from 6 to 12 kilometers (3.7 to 7.5 mi) wide, and has a total area of 365 square kilometers (141 sq mi).[14][15] With around 1.85 million Palestinians[16] on some 362 square kilometers, Gaza, if considered a top-level political unit, ranks as the 3rd most densely populated in the world.[17][18] An extensive Israeli buffer zone within the Strip renders much land off-limits to Gaza's Palestinians.[19] Gaza has an annual population growth rate of 2.91% (2014 est.), the 13th highest in the world, and is often referred to[by whom?] as overcrowded.[15][20] The population is expected to increase to 2.1 million in 2020.[clarification needed] In 2012, the United Nations Country Team (UNCT) in the occupied Palestinian territory warned that the Gaza Strip might not be a ""liveable place"" by 2020;[21] as of 2020, Gaza had suffered shortages of water, medicine and power, a situation exacerbated by the coronavirus crisis. According to Al Jazeera, ""19 human rights groups urged Israel to lift its siege on Gaza"". The UN has also urged the lifting of the blockade,[22][23][24] while a report by UNCTAD, prepared for the UN General Assembly and released on 25 November 2020, said that Gaza's economy was on the verge of collapse and that it was essential to lift the blockade.[25][26] Due to the Israeli and Egyptian border closures and the Israeli sea and air blockade, the population is not free to leave or enter the Gaza Strip, nor is it allowed to freely import or export goods. Sunni Muslims make up the predominant part of the Palestinian population in the Gaza Strip.

Despite the 2005 Israeli disengagement from Gaza,[27] the United Nations, international human rights organisations, and the majority of governments and legal commentators consider the territory to be still occupied by Israel, supported by additional restrictions placed on Gaza by Egypt. Israel maintains direct external control over Gaza and indirect control over life within Gaza: it controls Gaza's air and maritime space, as well as six of Gaza's seven land crossings. It reserves the right to enter Gaza at will with its military and maintains a no-go buffer zone within the Gaza territory. Gaza is dependent on Israel for water, electricity, telecommunications, and other utilities.[27] The system of control imposed by Israel is described[by whom?] as an ""indirect occupation"".[28] Some Israeli analysts have disputed the idea that Israel still occupies Gaza, and have depicted the territory as a de facto independent state.

When Hamas won a majority in the 2006 Palestinian legislative election, the opposing political party, Fatah, refused to join the proposed coalition, until a short-lived unity government agreement was brokered by Saudi Arabia. When this collapsed under pressure from Israel and the United States, the Palestinian Authority instituted a non-Hamas government in the West Bank while Hamas formed a government on its own in Gaza.[29] Further economic sanctions were imposed by Israel and the European Quartet against Hamas. A brief civil war between the two Palestinian groups had broken out in Gaza when, apparently under a US-backed plan, Fatah contested Hamas's administration. Hamas emerged the victor and expelled Fatah-allied officials and members of the PA's security apparatus from the strip,[30][31] and has remained the sole governing power in Gaza since that date.[29]"
civil war theorh,"civil war theory. A civil war, also known as an intrastate war in polemology,[1] is a war between organized groups within the same state (or country). The aim of one side may be to take control of the country or a region, to achieve independence for a region, or to change government policies.[2] The term is a calque of Latin bellum civile which was used to refer to the various civil wars of the Roman Republic in the 1st century BC.

Most modern civil wars involve intervention by outside powers. According to Patrick M. Regan in his book Civil Wars and Foreign Powers (2000) about two thirds of the 138 intrastate conflicts between the end of World War II and 2000 saw international intervention, with the United States intervening in 35 of these conflicts.[3]

A civil war is a high-intensity conflict, often involving regular armed forces, that is sustained, organized and large-scale. Civil wars may result in large numbers of casualties and the consumption of significant resources.[4]

Civil wars since the end of World War II have lasted on average just over four years, a dramatic rise from the one-and-a-half-year average of the 1900–1944 period. While the rate of emergence of new civil wars has been relatively steady since the mid-19th century, the increasing length of those wars has resulted in increasing numbers of wars ongoing at any one time. For example, there were no more than five civil wars underway simultaneously in the first half of the 20th century while there were over 20 concurrent civil wars close to the end of the Cold War. Since 1945, civil wars have resulted in the deaths of over 25 million people, as well as the forced displacement of millions more. Civil wars have further resulted in economic collapse; Somalia, Burma (Myanmar), Uganda and Angola are examples of nations that were considered to have had promising futures before being engulfed in civil wars.[5]"
sytian civil war,"The Syrian civil war (Arabic: الْحَرْبُ الْأَهْلِيَّةُ السُّورِيَّةُ‎, romanized: al-ḥarb al-ʾahlīyah as-sūrīyah) is an ongoing multi-sided civil war, fought in Syria, between the Syrian Arab Republic led by Syrian president Bashar al-Assad (along with domestic and foreign allies) and various domestic and foreign forces that oppose both the Syrian government and each other (in varying combinations).[123]

The unrest in Syria (which began on 15 March 2011 as part of the wider 2011 Arab Spring protests) grew out of discontent with the Syrian government and escalated to an armed conflict after protests calling for Assad's removal were violently suppressed.[124][125][126]

The war is currently being fought by several factions, including the Syrian Armed Forces and its domestic and international allies, a loose alliance of mostly Sunni opposition rebel groups (such as the Free Syrian Army), Salafi jihadist groups (including al-Nusra Front and Tahrir al-Sham), the mixed Kurdish-Arab Syrian Democratic Forces (SDF), and the Islamic State of Iraq and the Levant (ISIL).

A number of foreign countries, such as Iran, Russia, Turkey, and the United States, have either directly involved themselves in the conflict or provided support to one or another faction. Iran, Russia, and Hezbollah support the Syrian Arab Republic and the Syrian Armed Forces militarily, with Russia conducting airstrikes and other military operations since September 2015. The U.S.-led international coalition, established in 2014 with the declared purpose of countering ISIL, has conducted airstrikes primarily against ISIL as well as some against government and pro-government targets. They have also deployed special forces and artillery units to engage ISIL on the ground. Since 2015, the U.S. has supported the Autonomous Administration of North and East Syria and its armed wing, the Syrian Democratic Forces (SDF), materially, financially, and logistically. At different times, Turkish forces have fought the SDF, ISIL, and the Syrian government since 2016, but have also actively supported the Syrian opposition and occupied large swaths of northwestern Syria while engaging in significant ground combat. Between 2011 and 2017, fighting from the Syrian civil war spilled over into Lebanon as opponents and supporters of the Syrian government traveled to Lebanon to fight and attack each other on Lebanese soil, with ISIL and al-Nusra also engaging the Lebanese Army. Furthermore, while officially neutral, Israel has exchanged border fire and carried out repeated strikes against Hezbollah and Iranian forces, whose presence in southwestern Syria it views as a threat.[127][128]

International organizations have accused virtually all sides involved, including the Ba'athist Syrian government, ISIL, opposition rebel groups, Russia,[129] Turkey,[130] and the U.S.-led coalition[131] of severe human rights violations and massacres.[132] The conflict has caused a major refugee crisis. Over the course of the war, a number of peace initiatives have been launched, including the March 2017 Geneva peace talks on Syria led by the United Nations, but fighting has continued.[133]"
nyt,"The New York Times (N.Y.T. or N.Y. Times) is an American daily newspaper based in New York City with a worldwide readership.[7][8] Founded in 1851, the Times has since won 132 Pulitzer Prizes (the most of any newspaper),[9] and has long been regarded within the industry as a national ""newspaper of record"".[10] It is ranked 18th in the world by circulation and 3rd in the U.S.[11]

The paper is owned by The New York Times Company, which is publicly traded. It has been governed by the Sulzberger family since 1896, through a dual-class share structure after its shares became publicly traded.[12] A. G. Sulzberger and his father, Arthur Ochs Sulzberger Jr.—the paper's publisher and the company's chairman, respectively—are the fifth and fourth generation of the family to head the paper.[13]

Since the mid-1970s, The New York Times has expanded its layout and organization, adding special weekly sections on various topics supplementing the regular news, editorials, sports, and features. Since 2008,[14] the Times has been organized into the following sections: News, Editorials/Opinions-Columns/Op-Ed, New York (metropolitan), Business, Sports, Arts, Science, Styles, Home, Travel, and other features.[15] On Sundays, the Times is supplemented by the Sunday Review (formerly the Week in Review),[16] The New York Times Book Review,[17] The New York Times Magazine,[18] and T: The New York Times Style Magazine.[19]"
computer-generated imagery,computer-generated imagery
learning cursive in school,learning cursive in school
nyt comments,"The New York Times (N.Y.T. or N.Y. Times) is an American daily newspaper based in New York City with a worldwide readership.[7][8] Founded in 1851, the Times has since won 132 Pulitzer Prizes (the most of any newspaper),[9] and has long been regarded within the industry as a national ""newspaper of record"".[10] It is ranked 18th in the world by circulation and 3rd in the U.S.[11]"
salt preference,salt preference
seasoning food,seasoning food
organic farming,"Organic farming is an agricultural system that uses fertilizers of organic origin such as compost manure, green manure, and bone meal and places emphasis on techniques such as crop rotation and companion planting. It originated early in the 20th century in reaction to rapidly changing farming practices. Certified organic agriculture accounts for 70 million hectares globally, with over half of that total in Australia.[1] Organic farming continues to be developed by various organizations today. Biological pest control, mixed cropping and the fostering of insect predators are encouraged. Organic standards are designed to allow the use of naturally-occurring substances while prohibiting or strictly limiting synthetic substances.[2] For instance, naturally-occurring pesticides such as pyrethrin and rotenone are permitted, while synthetic fertilizers and pesticides are generally prohibited. Synthetic substances that are allowed include, for example, copper sulfate, elemental sulfur and Ivermectin. Genetically modified organisms, nanomaterials, human sewage sludge, plant growth regulators, hormones, and antibiotic use in livestock husbandry are prohibited.[3][4] Organic farming advocates claim advantages in sustainability,[5][6] openness, self-sufficiency, autonomy and independence,[6] health, food security, and food safety.

Organic agricultural methods are internationally regulated and legally enforced by many nations, based in large part on the standards set by the International Federation of Organic Agriculture Movements (IFOAM), an international umbrella organization for organic farming organizations established in 1972.[7] Organic agriculture can be defined as ""an integrated farming system that strives for sustainability, the enhancement of soil fertility and biological diversity while, with rare exceptions, prohibiting synthetic pesticides, antibiotics, synthetic fertilizers, genetically modified organisms, and growth hormones"".[8][9][10][11]

Since 1990, the market for organic food and other products has grown rapidly, reaching $63 billion worldwide in 2012.[12]: 25  This demand has driven a similar increase in organically-managed farmland that grew from 2001 to 2011 at a compounding rate of 8.9% per annum.[13]
As of 2019, approximately 72,300,000 hectares (179,000,000 acres) worldwide were farmed organically, representing approximately 1.5 percent of total world farmland.[14]"
belief systems,"A belief system can refer to a religion or a world view. A world view (or worldview) is a term calqued from the German word Weltanschauung (About this sound[ˈvɛlt.ʔanˌʃaʊ.ʊŋ] (help·info)) Welt is the German word for 'world,' and Anschauung is the German word for 'view' or 'outlook'. It is a concept fundamental to German philosophy and epistemology and refers to a wide world perception. Additionally, it refers to the framework of ideas and beliefs through which an individual interprets the world and interacts in it."
tina for greece,tina for greece
queen elizabeth ii,"Elizabeth II (Elizabeth Alexandra Mary; born 21 April 1926)[b] is Queen of the United Kingdom and 15 other Commonwealth realms.[c]

Elizabeth was born in Mayfair, London, as the first child of the Duke and Duchess of York (later King George VI and Queen Elizabeth). Her father ascended the throne in 1936 upon the abdication of his brother, King Edward VIII, making Elizabeth the heir presumptive. She was educated privately at home and began to undertake public duties during the Second World War, serving in the Auxiliary Territorial Service. In November 1947, she married Philip Mountbatten, a former prince of Greece and Denmark, with whom she had four children: Charles, Prince of Wales; Anne, Princess Royal; Prince Andrew, Duke of York; and Prince Edward, Earl of Wessex.

When her father died in February 1952, Elizabeth—then 25 years old—became queen regnant of seven independent Commonwealth countries: the United Kingdom, Canada, Australia, New Zealand, South Africa, Pakistan, and Ceylon, as well as Head of the Commonwealth. Elizabeth has reigned as a constitutional monarch through major political changes such as the Troubles in Northern Ireland, devolution in the United Kingdom, the accession of the United Kingdom to the European Communities, the United Kingdom's withdrawal from the European Union, Canadian patriation, and the decolonisation of Africa. Between 1956 and 1992, the number of her realms varied as territories gained independence, and as realms, including South Africa, Pakistan, and Ceylon (renamed Sri Lanka), became republics. Her many visits and meetings include a state visit to the Republic of Ireland and visits to or from five popes. Significant events have included her coronation in 1953 and the celebrations of her Silver, Golden, and Diamond Jubilees in 1977, 2002, and 2012 respectively. In 2017, she became the first British monarch to reach a Sapphire Jubilee. On 9 April 2021, after over 73 years of marriage, her husband, Prince Philip, died at the age of 99.

Elizabeth is the longest-lived and longest-reigning British monarch, the longest-serving female head of state in history, the oldest living and longest-reigning current monarch, and the oldest and longest-serving incumbent head of state. Throughout her reign, Elizabeth has faced republican sentiment and criticism of the royal family, particularly after the breakdown of her children's marriages, her annus horribilis in 1992, and the 1997 death of her former daughter-in-law Diana, Princess of Wales. However, support for the monarchy in the United Kingdom has been and remains consistently high, as does her personal popularity."
professional critics are sometimes a waste times,professional critics are sometimes a waste times
belief,belief
on-line courses,online courses
mr. stager,mr. stager
govennment salt guidelines,"The federal government has tried to limit consumers' salt intake by providing stricter dietary guidelines that recommend just a teaspoon a day per person, or about 2,300 milligrams. But people either don't pay attention or can't easily determine the salt content in the foods they eat."
co-op accomodations,co-op accomodations
teaching cursive writing,teaching cursive writing
cannibalism,"Cannibalism is the act of consuming another individual of the same species as food. Cannibalism is a common ecological interaction in the animal kingdom and has been recorded in more than 1,500 species.[1] Human cannibalism is well documented, both in ancient and in recent times.[2]

The rate of cannibalism increases in nutritionally poor environments as individuals turn to conspecifics as an additional food source.[3] Cannibalism regulates population numbers, whereby resources such as food, shelter and territory become more readily available with the decrease of potential competition. Although it may benefit the individual, it has been shown that the presence of cannibalism decreases the expected survival rate of the whole population and increases the risk of consuming a relative.[3] Other negative effects may include the increased risk of pathogen transmission as the encounter rate of hosts increases.[4] Cannibalism, however, does not—as once believed—occur only as a result of extreme food shortage or of artificial/unnatural conditions, but may also occur under natural conditions in a variety of species.[1][5][6]

Cannibalism is prevalent in aquatic ecosystems, in which up to approximately 90% of the organisms[vague] engage in cannibalistic activity at some point in their life-cycle.[vague][7] Cannibalism is not restricted to carnivorous species: it also occurs in herbivores and in detritivores.[vague][5] Sexual cannibalism normally involves the consumption of the male by the female individual before, during or after copulation.[3] Other forms of cannibalism include size-structured cannibalism and intrauterine cannibalism.

Behavioural, physiological and morphological adaptations have evolved to decrease the rate of cannibalism in individual species.[3]"
kindle,"Amazon Kindle is a series of e-readers designed and marketed by Amazon. Amazon Kindle devices enable users to browse, buy, download, and read e-books, newspapers, magazines and other digital media via wireless networking to the Kindle Store.[5] The hardware platform, which Amazon subsidiary Lab126 developed, began as a single device in 2007. Currently, it comprises a range of devices, including e-readers with E Ink electronic paper displays and Kindle applications on all major computing platforms. All Kindle devices integrate with Kindle Store content and, as of March 2018, the store had over six million e-books available in the United States.[6]"
access to better food,access to better food
religious beliefs in relation to general law,"Because of their belief in a separation of church and state, the framers of the Constitution favored a neutral posture toward religion. The members of the Constitutional Convention, the group charged with authoring the Constitution, believed that the government should have no power to influence its citizens toward or away from a religion. The principle of separating church from state was integral to the framers’ understanding of religious freedom. They believed that any governmental intervention in the religious affairs of citizens would necessarily infringe on their religious freedom. Thus, the Constitution maintains a general silence on the subject save for two instances. The first instance, in Article VI, is a proscription of any religious tests as a requisite qualification for public service. The second instance is in the First Amendment of the Bill of Rights.

The First Amendment contains two clauses that prescribe the government's relationship with religion. In the first instance, the Establishment Clause states that ""Congress shall make no law respecting an establishment of religion."" In the strictest reading, the Establishment Clause proscribes any adoption of an official religion by the federal government. More broadly, the phrase functions as a way of assuring that the federal government will not adopt any stance in favor of or against any religion. However, the Supreme Court has tolerated a certain degree of government involvement in religion. For instance, the Court has allowed government funding to go to private religious schools and prayers to begin certain legislative meetings, as in Town of Greece v. Galloway. In that case, the Court ruled that a town hall meeting that began with prayers, predominantly given by members of different denominations of Christianity, was not a violation of the Establishment Clause, in part because legislative prayers are for the legislators and not for the public.

The second clause of the First Amendment that deals with religion immediately follows the Establishment Clause: ""Congress shall make no law respecting an establishment of religion, or prohibiting the exercise thereof."" Where the first clause prohibits Congress from adopting any particular religion, the second clause prohibits Congress from interfering with an individual's exercise of religion. This second clause is called the Free Exercise Clause. The Free Exercise Clause protects an individual's right not only to believe what he or she would like but also to practice it. The clause protects individuals from laws that would expressly inhibit them from engaging in religious practices.

The Supreme Court has interpreted limits to the Free Exercise Clause and allowed the government to legislate against certain religious practices, such as bigamy and peyote use. In the last 30 years especially, the Court has generally adopted a more restrictive view of the protections of the Free Exercise Clause. Some commentators have suggested that the Free Exercise Clause is contradictory with the Establishment Clause because by protecting certain religious practices that the government would otherwise like to prohibit, the Constitution takes stance in favor of and not neutral to religion."
unrealistic student expectations,unrealistic student expectations
enviromental issues,"Environmental issues are harmful effects of human activity on the biophysical environment.[citation needed][dubious – discuss] Environmental protection is a practice of protecting the natural environment on the individual, organizational or governmental levels, for the benefit of both the environment and humans. Environmentalism, a social and environmental movement, addresses environmental issues through advocacy, education and activism.[1]

The carbon dioxide equivalent of greenhouse gases (GHG) in the atmosphere has already exceeded 400 parts per million (NOAA) (with total ""long-term"" GHG exceeding 455 parts per million) (Intergovernmental Panel on Climate Report). The amount of greenhouse gas in the atmosphere is possibly above the threshold that can potentially cause climate change. The UN Office for the Coordination of Humanitarian Affairs (OCHA) has stated ""Climate change is not just a distant future threat. It is the main driver behind rising humanitarian needs and we are seeing its impact. The number of people affected and the damages inflicted by extreme weather has been unprecedented.""[2] Further, OCHA has stated:

Climate disasters are on the rise. Around 70 percent of disasters are now climate-related – up from around 50 percent from two decades ago.
These disasters take a heavier human toll and come with a higher price tag. In the last decade, 2.4 billion people were affected by climate-related disasters, compared to 1.7 billion in the previous decade. The cost of responding to disasters has risen tenfold between 1992 and 2008.
Destructive sudden heavy rains, intense tropical storms, repeated flooding, and droughts are likely to increase, as will the vulnerability of local communities in the absence of strong concerted action.[3]
Environment destruction caused by humans is a global, ongoing problem. By the year 2050, the global human population is expected to grow by 2 billion people, thereby reaching a level of 9.6 billion people.[4] The human effects on Earth can be seen in many different ways. A main effect, is an increase in global temperature. According to the report ”Our Changing Climate”, the global warming that has been going on for the past 50 years is primarily due to human activities.[5] Since 1895, the average temperature in the United States has increased by between 0.7 °C and 1.1 °C (1.3 °F and 1.9 °F), with most of the increase taking place since around 1970.[5][6][7]"
seasoning of food,seasoning of food
sexist jokes,sexist
organic farms,"Organic farming is an agricultural system that uses fertilizers of organic origin such as compost manure, green manure, and bone meal and places emphasis on techniques such as crop rotation and companion planting. It originated early in the 20th century in reaction to rapidly changing farming practices. Certified organic agriculture accounts for 70 million hectares globally, with over half of that total in Australia.[1] Organic farming continues to be developed by various organizations today. Biological pest control, mixed cropping and the fostering of insect predators are encouraged. Organic standards are designed to allow the use of naturally-occurring substances while prohibiting or strictly limiting synthetic substances.[2] For instance, naturally-occurring pesticides such as pyrethrin and rotenone are permitted, while synthetic fertilizers and pesticides are generally prohibited. Synthetic substances that are allowed include, for example, copper sulfate, elemental sulfur and Ivermectin. Genetically modified organisms, nanomaterials, human sewage sludge, plant growth regulators, hormones, and antibiotic use in livestock husbandry are prohibited.[3][4] Organic farming advocates claim advantages in sustainability,[5][6] openness, self-sufficiency, autonomy and independence,[6] health, food security, and food safety.

Organic agricultural methods are internationally regulated and legally enforced by many nations, based in large part on the standards set by the International Federation of Organic Agriculture Movements (IFOAM), an international umbrella organization for organic farming organizations established in 1972.[7] Organic agriculture can be defined as ""an integrated farming system that strives for sustainability, the enhancement of soil fertility and biological diversity while, with rare exceptions, prohibiting synthetic pesticides, antibiotics, synthetic fertilizers, genetically modified organisms, and growth hormones"".[8][9][10][11]

Since 1990, the market for organic food and other products has grown rapidly, reaching $63 billion worldwide in 2012.[12]: 25  This demand has driven a similar increase in organically-managed farmland that grew from 2001 to 2011 at a compounding rate of 8.9% per annum.[13]
As of 2019, approximately 72,300,000 hectares (179,000,000 acres) worldwide were farmed organically, representing approximately 1.5 percent of total world farmland.[14]"
organic systems,"Organic farming is an agricultural system that uses fertilizers of organic origin such as compost manure, green manure, and bone meal and places emphasis on techniques such as crop rotation and companion planting. It originated early in the 20th century in reaction to rapidly changing farming practices. Certified organic agriculture accounts for 70 million hectares globally, with over half of that total in Australia.[1] Organic farming continues to be developed by various organizations today. Biological pest control, mixed cropping and the fostering of insect predators are encouraged. Organic standards are designed to allow the use of naturally-occurring substances while prohibiting or strictly limiting synthetic substances.[2] For instance, naturally-occurring pesticides such as pyrethrin and rotenone are permitted, while synthetic fertilizers and pesticides are generally prohibited. Synthetic substances that are allowed include, for example, copper sulfate, elemental sulfur and Ivermectin. Genetically modified organisms, nanomaterials, human sewage sludge, plant growth regulators, hormones, and antibiotic use in livestock husbandry are prohibited.[3][4] Organic farming advocates claim advantages in sustainability,[5][6] openness, self-sufficiency, autonomy and independence,[6] health, food security, and food safety.

Organic agricultural methods are internationally regulated and legally enforced by many nations, based in large part on the standards set by the International Federation of Organic Agriculture Movements (IFOAM), an international umbrella organization for organic farming organizations established in 1972.[7] Organic agriculture can be defined as ""an integrated farming system that strives for sustainability, the enhancement of soil fertility and biological diversity while, with rare exceptions, prohibiting synthetic pesticides, antibiotics, synthetic fertilizers, genetically modified organisms, and growth hormones"".[8][9][10][11]

Since 1990, the market for organic food and other products has grown rapidly, reaching $63 billion worldwide in 2012.[12]: 25  This demand has driven a similar increase in organically-managed farmland that grew from 2001 to 2011 at a compounding rate of 8.9% per annum.[13]
As of 2019, approximately 72,300,000 hectares (179,000,000 acres) worldwide were farmed organically, representing approximately 1.5 percent of total world farmland.[14]"
liberal religious beliefs,liberal religious beliefs
charles as king,"“Under common law, Prince Charles will automatically become King the moment the Queen dies. Prince William could only become King if Prince Charles chose to abdicate. That would require legislation, as happened with the Declaration of Abdication Act 1936."
germany leaving the euro union,"The European Union (EU) is a political and economic union of 27 member states that are located primarily in Europe.[8] The union has a total area of 4,233,255.3 km2 (1,634,469.0 sq mi) and an estimated total population of about 447 million. An internal single market has been established through a standardised system of laws that apply in all member states in those matters, and only those matters, where the states have agreed to act as one. EU policies aim to ensure the free movement of people, goods, services and capital within the internal market;[9] enact legislation in justice and home affairs; and maintain common policies on trade,[10] agriculture,[11] fisheries and regional development.[12] Passport controls have been abolished for travel within the Schengen Area.[13] A monetary union was established in 1999, coming into full force in 2002, and is composed of 19 member states which use the euro currency. The EU has often been described as a sui generis political entity (without precedent or comparison) with the characteristics of either a federation or confederation.[14][15]

The union and EU citizenship were established when the Maastricht Treaty came into force in 1993.[16] The EU traces its origins to the European Coal and Steel Community (ECSC) and the European Economic Community (EEC), established, respectively, by the 1951 Treaty of Paris and 1957 Treaty of Rome. The original member states of what came to be known as the European Communities were the Inner Six: Belgium, France, Italy, Luxembourg, the Netherlands, and West Germany. The communities and their successors have grown in size by the accession of 21 new member states and in power by the addition of policy areas to their remit. The United Kingdom became the one and only member state to leave the EU[17] on 31 January 2020. Before this, three territories of member states had left the EU or its forerunners. The latest major amendment to the constitutional basis of the EU, the Treaty of Lisbon, came into force in 2009.

Containing some 5.8 per cent of the world population in 2020,[c] the EU had generated a nominal gross domestic product (GDP) of around US$17.1 trillion in 2021,[5] constituting approximately 18 per cent of global nominal GDP.[19][better source needed] Additionally, all EU countries have a very high Human Development Index according to the United Nations Development Programme. In 2012, the EU was awarded the Nobel Peace Prize.[20] Through the Common Foreign and Security Policy, the union has developed a role in external relations and defence. It maintains permanent diplomatic missions throughout the world and represents itself at the United Nations, the World Trade Organization, the G7 and the G20. Due to its global influence, the European Union has been described by some scholars as an emerging superpower.[21][22][23]"
salt in diet,salt in diet
mark twain,"Samuel Langhorne Clemens (November 30, 1835 – April 21, 1910),[1] known by his pen name Mark Twain, was an American writer, humorist, entrepreneur, publisher, and lecturer. He was lauded as the ""greatest humorist the United States has produced,""[2] and William Faulkner called him ""the father of American literature"".[3] His novels include The Adventures of Tom Sawyer (1876) and its sequel, the Adventures of Huckleberry Finn (1884),[4] the latter often called ""The Great American Novel"".

Twain was raised in Hannibal, Missouri, which later provided the setting for Tom Sawyer and Huckleberry Finn. He served an apprenticeship with a printer and then worked as a typesetter, contributing articles to the newspaper of his older brother Orion Clemens. He later became a riverboat pilot on the Mississippi River before heading west to join Orion in Nevada. He referred humorously to his lack of success at mining, turning to journalism for the Virginia City Territorial Enterprise.[5] His humorous story ""The Celebrated Jumping Frog of Calaveras County"" was published in 1865, based on a story that he heard at Angels Hotel in Angels Camp, California, where he had spent some time as a miner. The short story brought international attention and was even translated into French.[6] His wit and satire, in prose and in speech, earned praise from critics and peers, and he was a friend to presidents, artists, industrialists, and European royalty.

Twain earned a great deal of money from his writings and lectures but invested in ventures that lost most of it—such as the Paige Compositor, a mechanical typesetter that failed because of its complexity and imprecision. He filed for bankruptcy in the wake of these financial setbacks, but in time overcame his financial troubles with the help of Henry Huttleston Rogers. He eventually paid all his creditors in full, even though his bankruptcy relieved him of having to do so. Twain was born shortly after an appearance of Halley's Comet, and he predicted that he would ""go out with it"" as well; he died the day after the comet made its closest approach to the Earth."
radical conservativism,"In United States politics, the radical right is a political preference that leans towards extreme conservatism, nationalist white supremacist ideologies and other right-wing beliefs in hierarchical structure.[1] The term was first used by social scientists in the 1950s regarding small groups such as the John Birch Society in the United States and since then it has been applied to similar groups worldwide.[2]

The term ""radical"" was applied to the groups because they sought to make fundamental (hence ""radical"") changes within institutions and remove from political life persons and institutions that threatened their values or economic interests.[3]"
stability,stability
forced imprisoned treatment,forced imprisoned treatment
tennis fans,tennis fans
bicycle,bicycle
appearance,appearance
medicaid and medicare,medicaid and medicare
cameras,cameras
bad,bad
thought,thought
uneven playing fields in life,"In commerce, a level playing field is a concept about fairness, not that each player has an equal chance to succeed, but that they all play by the same set of rules.[1]

In a game played on a playing field, such as rugby, one team would have an unfair advantage if the field had a slope. Since some real-life playing fields do in fact have slopes, it is customary for teams to swap ends of the playing field at half time.

A metaphorical playing field is said to be level if no external interference affects the ability of the players to compete fairly.

Some government regulations are intended to provide such fairness, since all participants must abide by the same rules. However, they can have the opposite effect, for example if larger firms find it easier to pay for fixed costs of regulation. It may be added that if the rules affect different participants differently, then they are not actually the same.[citation needed]

Handicapping might be thought of as the opposite concept, of unequal rules designed to make the outcome of play more equal."
3-d printing,"3D printing, or additive manufacturing, is the construction of a three-dimensional object from a CAD model or a digital 3D model.[1] The term ""3D printing"" can refer to a variety of processes in which material is deposited, joined or solidified under computer control to create a three-dimensional object,[2] with material being added together (such as plastics, liquids or powder grains being fused together), typically layer by layer.

In the 1980s, 3D printing techniques were considered suitable only for the production of functional or aesthetic prototypes, and a more appropriate term for it at the time was rapid prototyping.[3] As of 2019, the precision, repeatability, and material range of 3D printing have increased to the point that some 3D printing processes are considered viable as an industrial-production technology, whereby the term additive manufacturing can be used synonymously with 3D printing.[4] One of the key advantages of 3D printing is the ability to produce very complex shapes or geometries that would be otherwise impossible to construct by hand, including hollow parts or parts with internal truss structures to reduce weight. Fused deposition modeling (FDM), which uses a continuous filament of a thermoplastic material, is the most common 3D printing process in use as of 2020.[5]"
3d printing,"3D printing, or additive manufacturing, is the construction of a three-dimensional object from a CAD model or a digital 3D model.[1] The term ""3D printing"" can refer to a variety of processes in which material is deposited, joined or solidified under computer control to create a three-dimensional object,[2] with material being added together (such as plastics, liquids or powder grains being fused together), typically layer by layer.

In the 1980s, 3D printing techniques were considered suitable only for the production of functional or aesthetic prototypes, and a more appropriate term for it at the time was rapid prototyping.[3] As of 2019, the precision, repeatability, and material range of 3D printing have increased to the point that some 3D printing processes are considered viable as an industrial-production technology, whereby the term additive manufacturing can be used synonymously with 3D printing.[4] One of the key advantages of 3D printing is the ability to produce very complex shapes or geometries that would be otherwise impossible to construct by hand, including hollow parts or parts with internal truss structures to reduce weight. Fused deposition modeling (FDM), which uses a continuous filament of a thermoplastic material, is the most common 3D printing process in use as of 2020.[5]"
turkish lack of democracy,"The Peoples' Democratic Party (Turkish: Halkların Demokratik Partisi, HDP, Kurdish: Partiya Demokratîk a Gelan[22]), or Democratic Party of the Peoples, is a pro-minority political party in Turkey. Generally left-wing, the party places a strong emphasis on participatory and radical democracy, feminism, LGBT+ rights, minority rights, youth rights and egalitarianism. It is an associate member of the Party of European Socialists (PES)[23] a consultative member of the Socialist International[24] and a party within the Progressive Alliance (PA).[25]

Aspiring to fundamentally challenge the existing Turkish-Kurdish divide and other existing parameters in Turkish politics, the HDP was founded in 2012 as the political wing of the Peoples' Democratic Congress, a union of numerous left-wing movements that had previously fielded candidates as independents to bypass the 10% election threshold. The HDP is in an alliance with the Kurdish Democratic Regions Party (DBP), often described as the HDP's fraternal party. From 2013 to 2015, the HDP participated in peace negotiations between the Turkish government and the Kurdistan Workers' Party (PKK). The ruling AKP accuses the HDP of having direct links with the PKK.[26]

The party operates a co-presidential system of leadership, with one chairman and one chairwoman.[27] In the 2014 presidential election, the party put forward its chairman, Selahattin Demirtaş, who won 9.77% of the vote. Despite concerns that it could fall short of the 10% election threshold, the party put forward party-lists instead of running independent candidates in the subsequent June 2015 general election. Exceeding expectations, it polled at 13.12%, becoming the third largest parliamentary group. The party briefly participated in the interim election government formed by AKP Prime Minister Ahmet Davutoğlu on 28 August 2015, with HDP MPs Ali Haydar Konca and Müslüm Doğan becoming the Minister of European Union Affairs and the Minister of Development respectively.

Witnessing the 2016 Turkish coup attempt and pointing out previous repression of democratic forces by martial powers, the HDP strongly opposed the coup. The HDP was first ignored and put left out of the post-coup national truce while the Turkish purges targeted alleged members of the Gülen movement. From September 2016, the Turkish judiciary started to submit HDP elected officials to anti-terrorism accusations. As of March 2018, the MP status of seven HDP representatives had been revoked and six more representatives, including the ex-chairman Demirtaş, remained under arrest,[28] disturbing widely the HDP's ability to communicate and be active on the political scene. In December 2020 HDP co-deputy head for local governments Salim Kaplan said that ""since 2016, 20,000 of our members have been taken into custody and more than 10,000 of our members and executives have been sent to jail."" and 48 municipalities have been seized by the government.[29]"
turkey being a democratic country,"The Peoples' Democratic Party (Turkish: Halkların Demokratik Partisi, HDP, Kurdish: Partiya Demokratîk a Gelan[22]), or Democratic Party of the Peoples, is a pro-minority political party in Turkey. Generally left-wing, the party places a strong emphasis on participatory and radical democracy, feminism, LGBT+ rights, minority rights, youth rights and egalitarianism. It is an associate member of the Party of European Socialists (PES)[23] a consultative member of the Socialist International[24] and a party within the Progressive Alliance (PA).[25]

Aspiring to fundamentally challenge the existing Turkish-Kurdish divide and other existing parameters in Turkish politics, the HDP was founded in 2012 as the political wing of the Peoples' Democratic Congress, a union of numerous left-wing movements that had previously fielded candidates as independents to bypass the 10% election threshold. The HDP is in an alliance with the Kurdish Democratic Regions Party (DBP), often described as the HDP's fraternal party. From 2013 to 2015, the HDP participated in peace negotiations between the Turkish government and the Kurdistan Workers' Party (PKK). The ruling AKP accuses the HDP of having direct links with the PKK.[26]

The party operates a co-presidential system of leadership, with one chairman and one chairwoman.[27] In the 2014 presidential election, the party put forward its chairman, Selahattin Demirtaş, who won 9.77% of the vote. Despite concerns that it could fall short of the 10% election threshold, the party put forward party-lists instead of running independent candidates in the subsequent June 2015 general election. Exceeding expectations, it polled at 13.12%, becoming the third largest parliamentary group. The party briefly participated in the interim election government formed by AKP Prime Minister Ahmet Davutoğlu on 28 August 2015, with HDP MPs Ali Haydar Konca and Müslüm Doğan becoming the Minister of European Union Affairs and the Minister of Development respectively.

Witnessing the 2016 Turkish coup attempt and pointing out previous repression of democratic forces by martial powers, the HDP strongly opposed the coup. The HDP was first ignored and put left out of the post-coup national truce while the Turkish purges targeted alleged members of the Gülen movement. From September 2016, the Turkish judiciary started to submit HDP elected officials to anti-terrorism accusations. As of March 2018, the MP status of seven HDP representatives had been revoked and six more representatives, including the ex-chairman Demirtaş, remained under arrest,[28] disturbing widely the HDP's ability to communicate and be active on the political scene. In December 2020 HDP co-deputy head for local governments Salim Kaplan said that ""since 2016, 20,000 of our members have been taken into custody and more than 10,000 of our members and executives have been sent to jail."" and 48 municipalities have been seized by the government.[29]"
bike lanes,bike lanes
bike lane,bike lanes
tennis,tennis fans
the rrhfa,the rrhfa
rrhfa inductions,rrhfa inductions
crowd,crowd
camera,camera
zip cars,"Zipcar is an American car-sharing company and a subsidiary of Avis Budget Group. Zipcar provides automobile reservations to its members, billable by the minute, hour or day; members may have to pay a monthly or annual membership fee in addition to car reservations charges. Zipcar was founded in 2000 by Antje Danielson and Robin Chase.[5]

On March 14, 2013, Avis Budget Group acquired Zipcar for approximately US$500 million.[6] Scott Griffith, who had run the company for the previous 10 years, resigned the day after the acquisition closed and passed the reins to a new company president, Mark Norman.[7] In early 2014, Kaye Ceille took over as Zipcar's North American President. In the summer of 2016, Kaye Ceille became a managing director of Avis Budget Group International.

In September 2016, Zipcar announced that it had 1 million members across 500 cities in 9 countries, and offers nearly 10,000 vehicles.[8]

Members can reserve vehicles with Zipcar's mobile app, online, or in some places by phone at any time, either immediately or up to a year in advance. Zipcar members have automated access to the cars using an access card which unlocks the door; the keys are already located inside. Alternatively, members can use Zipcar's Android or iPhone app to locate a Zipcar by honking its horn as well as to unlock the doors.[9] Zipcar charges a one-time application fee, an annual fee, and a reservation charge. Fuel, parking, insurance, and maintenance are included in the price.[10]

In the third quarter of 2007, Zipcar merged with Seattle-based rival Flexcar to create a nationwide car rental company.[11] The company's IPO was in April 2011. Zipcar common stock traded on NASDAQ under the ticker symbol ""ZIP"" until it was acquired by Avis in 2013.[12][13]"
transportation system,transportation system
cycle,cycel
uneducated people,"English Literacy in the United States is 79% according to a 2019 report by the National Center for Education Statistics.[1] 21% of American adults are illiterate or functionally illiterate.[2] According to the U.S. Department of Education, 54% of adults in the United States have literacy below the 6th-grade level.[3]

Literacy has increased along with access to education and higher vocational standards. In many nations, the ability to read a simple sentence suffices as literacy, and was the previous standard for the U.S. The definition of literacy has changed greatly; the term is presently defined as the ability to use printed and written information to function in society, to achieve one's goals, and to develop one's knowledge and potential.[4]

The United States Department of Education assesses literacy in the general population through its National Assessment of Adult Literacy (NAAL).[5] The NAAL survey defines three types of literacy:[6]

prose literacy: the knowledge and skills needed to search, comprehend, and use continuous texts. Examples include editorials, news stories, brochures, and instructional materials.
document literacy: the knowledge and skills needed to search, comprehend, and use non-continuous texts in various formats. Examples include job applications, payroll forms, transportation schedules, maps, tables, and drug and food labels.
quantitative literacy: the knowledge and skills required to identify and perform computations, either alone or sequentially, using numbers embedded in printed materials. Examples include balancing a checkbook, figuring out tips, completing an order form, or determining an amount.
Modern jobs often demand a high level of literacy, and its lack in adults and adolescents has been studied extensively. The National Institute for Literacy estimates that 32 million American adults are unable to read, which can contribute to chronic unemployment, low self-esteem, and a lower quality of available work.[7]

According to a 1992 survey, about 40 million adults had Level 1 literary competency, the lowest level, comprising understanding only basic written instructions.[8] A number of reports and studies are published annually to monitor the nation's status, and initiatives to improve literacy rates are funded by government and external sources.[9]"
3d printers,"The Peoples' Democratic Party (Turkish: Halkların Demokratik Partisi, HDP, Kurdish: Partiya Demokratîk a Gelan[22]), or Democratic Party of the Peoples, is a pro-minority political party in Turkey. Generally left-wing, the party places a strong emphasis on participatory and radical democracy, feminism, LGBT+ rights, minority rights, youth rights and egalitarianism. It is an associate member of the Party of European Socialists (PES)[23] a consultative member of the Socialist International[24] and a party within the Progressive Alliance (PA).[25]

Aspiring to fundamentally challenge the existing Turkish-Kurdish divide and other existing parameters in Turkish politics, the HDP was founded in 2012 as the political wing of the Peoples' Democratic Congress, a union of numerous left-wing movements that had previously fielded candidates as independents to bypass the 10% election threshold. The HDP is in an alliance with the Kurdish Democratic Regions Party (DBP), often described as the HDP's fraternal party. From 2013 to 2015, the HDP participated in peace negotiations between the Turkish government and the Kurdistan Workers' Party (PKK). The ruling AKP accuses the HDP of having direct links with the PKK.[26]

The party operates a co-presidential system of leadership, with one chairman and one chairwoman.[27] In the 2014 presidential election, the party put forward its chairman, Selahattin Demirtaş, who won 9.77% of the vote. Despite concerns that it could fall short of the 10% election threshold, the party put forward party-lists instead of running independent candidates in the subsequent June 2015 general election. Exceeding expectations, it polled at 13.12%, becoming the third largest parliamentary group. The party briefly participated in the interim election government formed by AKP Prime Minister Ahmet Davutoğlu on 28 August 2015, with HDP MPs Ali Haydar Konca and Müslüm Doğan becoming the Minister of European Union Affairs and the Minister of Development respectively.

Witnessing the 2016 Turkish coup attempt and pointing out previous repression of democratic forces by martial powers, the HDP strongly opposed the coup. The HDP was first ignored and put left out of the post-coup national truce while the Turkish purges targeted alleged members of the Gülen movement. From September 2016, the Turkish judiciary started to submit HDP elected officials to anti-terrorism accusations. As of March 2018, the MP status of seven HDP representatives had been revoked and six more representatives, including the ex-chairman Demirtaş, remained under arrest,[28] disturbing widely the HDP's ability to communicate and be active on the political scene. In December 2020 HDP co-deputy head for local governments Salim Kaplan said that ""since 2016, 20,000 of our members have been taken into custody and more than 10,000 of our members and executives have been sent to jail."" and 48 municipalities have been seized by the government.[29]"
3-d printer,"The Peoples' Democratic Party (Turkish: Halkların Demokratik Partisi, HDP, Kurdish: Partiya Demokratîk a Gelan[22]), or Democratic Party of the Peoples, is a pro-minority political party in Turkey. Generally left-wing, the party places a strong emphasis on participatory and radical democracy, feminism, LGBT+ rights, minority rights, youth rights and egalitarianism. It is an associate member of the Party of European Socialists (PES)[23] a consultative member of the Socialist International[24] and a party within the Progressive Alliance (PA).[25]

Aspiring to fundamentally challenge the existing Turkish-Kurdish divide and other existing parameters in Turkish politics, the HDP was founded in 2012 as the political wing of the Peoples' Democratic Congress, a union of numerous left-wing movements that had previously fielded candidates as independents to bypass the 10% election threshold. The HDP is in an alliance with the Kurdish Democratic Regions Party (DBP), often described as the HDP's fraternal party. From 2013 to 2015, the HDP participated in peace negotiations between the Turkish government and the Kurdistan Workers' Party (PKK). The ruling AKP accuses the HDP of having direct links with the PKK.[26]

The party operates a co-presidential system of leadership, with one chairman and one chairwoman.[27] In the 2014 presidential election, the party put forward its chairman, Selahattin Demirtaş, who won 9.77% of the vote. Despite concerns that it could fall short of the 10% election threshold, the party put forward party-lists instead of running independent candidates in the subsequent June 2015 general election. Exceeding expectations, it polled at 13.12%, becoming the third largest parliamentary group. The party briefly participated in the interim election government formed by AKP Prime Minister Ahmet Davutoğlu on 28 August 2015, with HDP MPs Ali Haydar Konca and Müslüm Doğan becoming the Minister of European Union Affairs and the Minister of Development respectively.

Witnessing the 2016 Turkish coup attempt and pointing out previous repression of democratic forces by martial powers, the HDP strongly opposed the coup. The HDP was first ignored and put left out of the post-coup national truce while the Turkish purges targeted alleged members of the Gülen movement. From September 2016, the Turkish judiciary started to submit HDP elected officials to anti-terrorism accusations. As of March 2018, the MP status of seven HDP representatives had been revoked and six more representatives, including the ex-chairman Demirtaş, remained under arrest,[28] disturbing widely the HDP's ability to communicate and be active on the political scene. In December 2020 HDP co-deputy head for local governments Salim Kaplan said that ""since 2016, 20,000 of our members have been taken into custody and more than 10,000 of our members and executives have been sent to jail."" and 48 municipalities have been seized by the government.[29]"
fat people,fat people
north american city bicycles,north american city bicycles
bike,bike lanes
economics' math,"Mathematical economics is the application of mathematical methods to represent theories and analyze problems in economics. By convention, these applied methods are beyond simple geometry, such as differential and integral calculus, difference and differential equations, matrix algebra, mathematical programming, and other computational methods.[1][2] Proponents of this approach claim that it allows the formulation of theoretical relationships with rigor, generality, and simplicity.[3]

Mathematics allows economists to form meaningful, testable propositions about wide-ranging and complex subjects which could less easily be expressed informally. Further, the language of mathematics allows economists to make specific, positive claims about controversial or contentious subjects that would be impossible without mathematics.[4] Much of economic theory is currently presented in terms of mathematical economic models, a set of stylized and simplified mathematical relationships asserted to clarify assumptions and implications.[5]

Broad applications include:

optimization problems as to goal equilibrium, whether of a household, business firm, or policy maker
static (or equilibrium) analysis in which the economic unit (such as a household) or economic system (such as a market or the economy) is modeled as not changing
comparative statics as to a change from one equilibrium to another induced by a change in one or more factors
dynamic analysis, tracing changes in an economic system over time, for example from economic growth.[2][6][7]
Formal economic modeling began in the 19th century with the use of differential calculus to represent and explain economic behavior, such as utility maximization, an early economic application of mathematical optimization. Economics became more mathematical as a discipline throughout the first half of the 20th century, but introduction of new and generalized techniques in the period around the Second World War, as in game theory, would greatly broaden the use of mathematical formulations in economics.[8][7]

This rapid systematizing of economics alarmed critics of the discipline as well as some noted economists. John Maynard Keynes, Robert Heilbroner, Friedrich Hayek and others have criticized the broad use of mathematical models for human behavior, arguing that some human choices are irreducible to mathematics."
fame matters,fame matters
it doesn't mention college,it doesn't mention college
mandaory voting,mandaory voting
new bike lanes,new bike lanes
mr. daly,mr. daly
takes no position,"Compulsory voting, also called mandatory voting, is the requirement in some countries that eligible citizens register and vote in elections. Penalties might be imposed on those who fail to do so without a valid reason. According to the CIA World Factbook, 21 countries, including 10 Latin American countries, officially had compulsory voting as of December 2017,[1] with a number of those countries not enforcing it.

During the first two decades of the 21st century, compulsory voting was introduced in Samoa and Bulgaria,[2] while Chile, Cyprus, the Dominican Republic, Fiji and Paraguay have repealed it during that same period."
government overspending on infrastructure,government overspending on infrastructure
syriaallies,"Ensuring national security, increasing influence among its Arab neighbours and securing the return of the Golan Heights, have been the primary goals of the Syrian Arab Republic's foreign policy. At many points in its history, Syria has seen virulent tension with its geographically cultural neighbours, such as Turkey, Israel, Iraq, and Lebanon. Syria enjoyed an improvement in relations with several of the states in its region in the 21st century, prior to the Arab Spring and the Syrian Civil War.

Since the ongoing civil war, the Syrian Arab Republic’s government has been increasingly isolated from the countries in the region, and the wider international community. Diplomatic relations were severed with several countries including: Turkey, Saudi Arabia, Canada, France, Italy, Australia, New Zealand, South Korea, Switzerland, Sweden, Denmark, the Netherlands, Germany, United States, United Kingdom, Belgium, Spain, Japan and Qatar.[1] Syria was suspended from the Arab League in 2011 and the Organisation of Islamic Cooperation in 2012. Syria continues to foster good relations with its traditional allies, Iran and Russia.

Other countries that presently maintain good relations with Syria include China, North Korea, Vietnam,[2] Cuba,[3][4] Venezuela,[5][6] Bolivia,[7] Nicaragua,[8][9] Guyana,[10] India,[11][12][13] South Africa,[14][15] Tanzania,[16] Pakistan,[17] Armenia,[18] Belarus,[19] South Ossetia,[20] Indonesia,[21] From among the Arab League states, Syria continues to have good relations with Iraq, Egypt (after 3 July 2013), Algeria,[22] Oman,[23][24][25][26] Sudan,[27] and Palestine. Syria does not maintain diplomatic relations with Israel, but has diplomatic relations with Abkhazia and South Ossetia.

In December 2018, after American president Donald Trump announced the partial withdrawal of U.S. troops from Syria, some countries initiated reopening of their diplomatic relations with Syria. Following the visit of Sudanese President Omar al-Bashir, the Arab League initiated the process of readmission of the Syrian Arab Republic to the organization,[28] while the United Arab Emirates reopened their embassy in Syria on 27 December,[29] and Bahrain[30] and Italy[31] announced their intentions to reopen their embassies."
lords coe,"Sebastian Newbold Coe, Baron Coe, CH, KBE, Hon FRIBA (born 29 September 1956), often referred to as Seb Coe or Lord Coe,[3][4] is a British politician and former track and field athlete. As a middle-distance runner, Coe won four Olympic medals, including the 1500 metres gold medal at the Olympic Games in 1980 and 1984. He set nine outdoor and three indoor world records in middle-distance track events – including, in 1979, setting three world records in the space of 41 days – and the world record he set in the 800 metres in 1981 remained unbroken until 1997. Coe's rivalries with fellow Britons Steve Ovett and Steve Cram dominated middle-distance racing for much of the 1980s.[5]

Following Coe's retirement from athletics, he was a Conservative member of parliament from 1992 to 1997 for Falmouth in Cornwall, and became a Life Peer on 16 May 2000.

He headed the successful London bid to host the 2012 Summer Olympics and became chairman of the London Organising Committee for the Olympic Games. In 2007, he was elected a vice-president of the International Association of Athletics Federations (IAAF), and re-elected for another four-year term in 2011.[6] In August 2015, he was elected president of the IAAF.[7]

In 2012, Coe was appointed Pro-Chancellor at Loughborough University where he had been an undergraduate, and is also a member of the University's governing body. He was of one of 24 athletes inducted as inaugural members of the IAAF Hall of Fame.[8] In November 2012, he was appointed chairman of the British Olympic Association. Coe was presented with the Lifetime Achievement award at the BBC Sports Personality of the Year in December 2012.[9]"
baskeyball player height,baskeyball player height
nba regulations,"The rules of basketball are the rules and regulations that govern the play, officiating, equipment and procedures of basketball. While many of the basic rules are uniform throughout the world, variations do exist. Most leagues or governing bodies in North America, the most important of which are the National Basketball Association and NCAA, formulate their own rules. In addition, the Technical Commission of the International Basketball Federation (FIBA) determines rules for international play; most leagues outside North America use the complete FIBA ruleset."
forcing participation is not democracy,forcing participation is not democracy
bikecyclists,bikecyclists
placing blame,"Blame is placed only if a capable addressee of norms is present (personhood), if this person was in the time of offending competent enough to question a norm (competence), and if this person actually communicated a negation of this norm (message)."
sending people to the site of an emergency,sending people to the site of an emergency
obese employees,obese employees
ukraine nuclear concessions in exchange for terrirorial integrity,"Ukraine is heavily dependent on nuclear energy – it has 15 reactors generating about half of its electricity.
Ukraine receives most of its nuclear services and nuclear fuel from Russia, but is reducing this dependence by buying fuel from Westinghouse.
In 2004 Ukraine commissioned two large new reactors. The government plans to maintain the nuclear share in electricity production to 2030, which will involve substantial new build.
In 2021 Westinghouse was contracted to finish building a new reactor at Khmelnitsky using AP1000 components from an aborted US project.
The government is looking to the West for both technology and investment in its nuclear plants. Westinghouse has an agreement to build four AP1000 reactors at established sites."
graduate voters,graduate voters
childish behavior by older americans,childish behavior by older americans
millenials being childish,"Millennials, also known as Generation Y or Gen Y, are the demographic cohort following Generation X and preceding Generation Z. Researchers and popular media use the early 1980s as starting birth years and the mid-1990s to early 2000s as ending birth years, with the generation typically being defined as people born from 1981 to 1996.[1] Most millennials are the children of baby boomers and early Gen Xers;[2] Millennials are often the parents of Generation Alpha.[3]

Across the globe, young people have postponed marriage.[4] Millennials were born at a time of declining fertility rates around the world,[5] and are having fewer children than their predecessors.[6][7][8][9] Those in developing nations will continue to constitute the bulk of global population growth.[10] In the developed world, young people of the 2010s were less inclined to have sexual intercourse compared to their predecessors when they were at the same age.[11] In the West, they are less likely to be religious than their predecessors,[5][12] but they may identify as spiritual.[13]

Millennials have been described as the first global generation and the first generation that grew up in the Internet age.[14] The generation is generally marked by elevated usage of and familiarity with the Internet, mobile devices, and social media,[15] which is why they are sometimes termed digital natives.[16] Between the 1990s and the 2010s, people from the developing world became increasingly well educated, a factor that boosted economic growth in these countries.[17] Millennials across the world have suffered significant economic disruption since starting their working lives; many faced high levels of youth unemployment during their early years in the labour market in the wake of the Great Recession, and suffered another recession a decade later due to the COVID-19 pandemic.[18][19]"
6000 rio olympic games,
conservative christians,"The Christian right, or the religious right, are Christian political factions that are characterized by their strong support of socially conservative policies. Christian conservatives seek to influence politics and public policy with their interpretation of the teachings of Christianity.[1]

In the United States, the Christian right is an informal coalition formed around a core of largely white conservative evangelical Protestants and Roman Catholics.[2][3][4] The Christian right draws additional support from politically conservative mainline Protestants and members of the Church of Jesus Christ of Latter-Day Saints.[2][5] The movement has its roots in American politics going back as far as the 1940s; it has been especially influential since the 1970s.[6][7] Its influence draws from grassroots activism as well as from focus on social issues and the ability to motivate the electorate around those issues.[8]

The Christian right is notable for advancing socially conservative positions on issues including school prayer, intelligent design, embryonic stem cell research,[9] homosexuality,[10] temperance,[11] euthanasia, contraception, Christian nationalism,[12] Sunday Sabbatarianism,[13] sex education, abortion,[14] and pornography.[15] Although the term Christian right is most commonly associated with politics in the United States, similar Christian conservative groups can be found in the political cultures of other Christian-majority nations."